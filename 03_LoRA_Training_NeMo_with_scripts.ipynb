{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the llama 3.2 1b instruct model (nemo checkpt) from ngc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## IMPORTANT: NeMo Framework Setup\n",
    "\n",
    "This notebook requires the NVIDIA NeMo framework for LoRA training. We'll clone the NeMo repository to access the necessary training scripts.\n",
    "\n",
    "**NeMo Version Compatibility**: \n",
    "- The downloaded model uses **NeMo 2.0** distributed checkpoint format (.distcp files)\n",
    "- The training scripts are backward compatible and can load both NeMo 1.0 and 2.0 formats\n",
    "- We show both script-based (simpler) and API-based (modern) approaches\n",
    "\n",
    "**Training Experience**: In this workshop, you'll train your own LoRA adapter from scratch! This gives you hands-on experience with:\n",
    "- Setting up training data\n",
    "- Configuring LoRA parameters\n",
    "- Running the actual training\n",
    "- Testing your custom adapter\n",
    "\n",
    "The training process takes approximately 5-10 minutes for our small example dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo repository already exists.\n",
      "\n",
      "Checking for required NeMo scripts:\n",
      "‚úì Found: megatron_gpt_finetuning.py\n",
      "‚úì Found: megatron_gpt_generate.py\n",
      "‚úì Found: merge.py\n"
     ]
    }
   ],
   "source": [
    "# Clone NeMo repository if not already present\n",
    "import os\n",
    "\n",
    "# Define the NeMo path within the presenter folder\n",
    "nemo_path = '/root/verb-workspace/NIM Workshop - Presenter/NeMo'\n",
    "\n",
    "if not os.path.exists(nemo_path):\n",
    "    print(\"Cloning NeMo repository...\")\n",
    "    !git clone https://github.com/NVIDIA/NeMo.git \"{nemo_path}\"\n",
    "    print(\"NeMo repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"NeMo repository already exists.\")\n",
    "    \n",
    "# Verify the training scripts exist\n",
    "nemo_scripts = [\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py',\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py',\n",
    "    f'{nemo_path}/scripts/nlp_language_modeling/merge_lora_weights/merge.py'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for required NeMo scripts:\")\n",
    "for script in nemo_scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"‚úì Found: {os.path.basename(script)}\")\n",
    "    else:\n",
    "        print(f\"‚úó Missing: {script}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Welcome to the most transformative part of our journey - LoRA fine-tuning! This is where you go from using someone else's AI to creating YOUR OWN specialized AI.\n",
    "\n",
    "Let me start with a real story. A Fortune 500 company came to us with a problem. They loved Llama 3 70B but needed it to understand their internal jargon - thousands of product codes, technical terms, and specific procedures. \n",
    "\n",
    "The traditional solution? Fine-tune the entire 70B parameter model. That would require:\n",
    "- 8 H100 GPUs ($300,000+ hardware)\n",
    "- 2 weeks of training time  \n",
    "- Machine learning PhD to manage it\n",
    "- $50,000+ in electricity\n",
    "\n",
    "Their budget? One RTX 4090 and a week.\n",
    "\n",
    "Enter LoRA - Low-Rank Adaptation. Instead of training all 70 billion parameters, LoRA adds small 'adapter' matrices that modify the model's behavior. Imagine it like putting specialized glasses on the model - it sees everything through your custom lens.\n",
    "\n",
    "The results for that company?\n",
    "- Trained on 1 RTX 4090\n",
    "- 6 hours total time\n",
    "- Junior developer managed it\n",
    "- Under $100 in costs\n",
    "- Model performed BETTER than full fine-tuning for their use case\n",
    "\n",
    "Today, I'll show you exactly how to do this. By the end, you'll be able to create custom AI models tailored to your exact needs!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune models using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~10-100MB vs full model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's set up our environment for LoRA training. The requirements are surprisingly modest compared to full fine-tuning.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"We need a few key packages for LoRA training. Let me explain each one:\n",
    "\n",
    "- `jsonlines`: For handling our training data format\n",
    "- `transformers`: HuggingFace's library, useful for tokenization\n",
    "- `omegaconf`: YAML configuration management (very clean!)\n",
    "- `pytorch-lightning`: Handles distributed training, logging, checkpoints\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Notice we're NOT installing the full NeMo framework for this demo. In production, you'd use NeMo for its optimized training loops, but these packages are enough to understand the concepts.\n",
    "\n",
    "While this installs, let me mention - LoRA was invented by Microsoft researchers in 2021. In just 2 years, it's revolutionized how we customize language models. The paper has 3000+ citations!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0a0+ebedce2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.5.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.14.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.3.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install NeMo (if not already installed)\n",
    "# Note: This should be run in the NeMo directory\n",
    "# !cd \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\" && pip install -e \".[all]\"\n",
    "\n",
    "# For this tutorial, we'll install minimal requirements\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's check our training hardware:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "For LoRA training, here's what you can accomplish with different GPUs:\n",
    "\n",
    "**RTX 4090 (24GB)**:\n",
    "- Llama 3.1 8B: Full LoRA training ‚úì\n",
    "- Llama 2 13B: LoRA with gradient checkpointing ‚úì\n",
    "- Llama 3.1 70B: LoRA with quantization ‚úì\n",
    "\n",
    "**A100 40GB**:\n",
    "- All of the above plus...\n",
    "- Llama 3.1 70B: Full LoRA training ‚úì\n",
    "- Multiple LoRA adapters simultaneously ‚úì\n",
    "\n",
    "**Consumer GPUs (16GB)**:\n",
    "- Llama 3.1 8B: LoRA with small batch sizes ‚úì\n",
    "- Mistral 7B: Full LoRA training ‚úì\n",
    "\n",
    "The memory formula: \n",
    "- Base model (frozen): ~2 bytes per parameter\n",
    "- LoRA adapters: ~0.02 bytes per parameter (1% of base)\n",
    "- Gradients & optimizer: ~8 bytes per trainable parameter\n",
    "\n",
    "So for Llama 3.1 8B:\n",
    "- Base: 16GB\n",
    "- LoRA: 160MB  \n",
    "- Training overhead: ~1.3GB\n",
    "- Total: ~18GB (fits in 24GB GPU!)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### NeMo Framework Note\n",
    "\n",
    "The NeMo scripts we'll use for training are already accessible from the cloned repository. Full NeMo package installation is optional - the training scripts work with our current environment.\n",
    "\n",
    "**What we'll do:**\n",
    "- Use NeMo's production training scripts directly\n",
    "- Train a real LoRA adapter (5-10 minutes)\n",
    "- Test it with actual inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ We'll use the NeMo training scripts directly.\n",
      "üöÄ You'll train your own LoRA adapter in this workshop!\n",
      "‚è±Ô∏è Training will take approximately 5-10 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note: Full NeMo installation can take 20-30 minutes\n",
    "# For this workshop, we'll use the cloned NeMo scripts without full installation\n",
    "# The training scripts work with our existing environment\n",
    "\n",
    "# If you need full NeMo features, uncomment these lines:\n",
    "# !cd \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\" && pip install -e \".[all]\"\n",
    "# !pip install megatron-core\n",
    "\n",
    "print(\"‚úÖ We'll use the NeMo training scripts directly.\")\n",
    "print(\"üöÄ You'll train your own LoRA adapter in this workshop!\")\n",
    "print(\"‚è±Ô∏è Training will take approximately 5-10 minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0a0+ebedce2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU memory: 84.97 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's organize our workspace professionally:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Good project structure is crucial:\n",
    "- `data/`: Training and validation datasets\n",
    "- `models/`: Saved checkpoints and final models\n",
    "- `configs/`: YAML configurations for experiments\n",
    "\n",
    "In a real project, you'd also have:\n",
    "- `logs/`: TensorBoard logs\n",
    "- `scripts/`: Training and evaluation scripts\n",
    "- `results/`: Metrics and analysis\n",
    "- `tests/`: Unit tests for data processing\n",
    "\n",
    "Organization pays dividends when you're running multiple experiments!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Now for the SECRET SAUCE - your training data. This is what makes your model unique. Let me create a customer service dataset as an example:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Look at this data carefully. Each example has:\n",
    "- `input`: The customer's question with context\n",
    "- `output`: The EXACT response you want\n",
    "\n",
    "Key insights for great training data:\n",
    "1. **Quality > Quantity**: 1,000 excellent examples beat 100,000 mediocre ones\n",
    "2. **Diversity**: Cover edge cases, different phrasings, various scenarios  \n",
    "3. **Consistency**: Same style, tone, format across examples\n",
    "4. **Realism**: Use actual customer queries if possible\n",
    "\n",
    "For this demo, we have 5 examples. In production:\n",
    "- Minimum: 500-1,000 examples\n",
    "- Sweet spot: 5,000-10,000 examples\n",
    "- Diminishing returns: >50,000 examples\n",
    "\n",
    "Pro tip: Start small! Train with 100 examples first, test the model, identify gaps, add more targeted examples. Iterative improvement beats massive datasets.\n",
    "\n",
    "The JSONL format is perfect because:\n",
    "- Each line is independent (parallelizable)\n",
    "- Streamable for large datasets\n",
    "- Human-readable for debugging\n",
    "- Standard format tools understand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 training examples\n",
      "Created 2 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\n\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\n\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\n\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding LoRA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me show you how LoRA actually works under the hood. This is a simplified implementation for educational purposes:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "WOW! Look at those numbers:\n",
    "- Original layer: 16,777,216 parameters\n",
    "- LoRA adaptation: 262,144 parameters  \n",
    "- Reduction: 98.4%!\n",
    "\n",
    "Here's the mathematical magic:\n",
    "- Original: Y = X √ó W (where W is 4096√ó4096)\n",
    "- LoRA: Y = X √ó W + X √ó A √ó B √ó (Œ±/r)\n",
    "  - A is 4096√ó32 (down-projection)\n",
    "  - B is 32√ó4096 (up-projection)\n",
    "  - W remains frozen!\n",
    "\n",
    "The intuition: Instead of changing the entire highway (W), we add a small side road (A√óB) that modifies traffic flow.\n",
    "\n",
    "Why this works:\n",
    "1. Neural networks have low intrinsic rank\n",
    "2. Most fine-tuning changes lie in a low-dimensional subspace\n",
    "3. We're learning the 'diff' not the whole model\n",
    "\n",
    "Real-world impact: Meta trains separate LoRA adapters for 100+ languages on the same base model. Each adapter is ~100MB instead of 140GB!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer parameters: 16,777,216\n",
      "LoRA parameters: 262,144\n",
      "Parameter reduction: 98.4%\n"
     ]
    }
   ],
   "source": [
    "# This is a simplified demo of what LoRA training looks like\n",
    "# In practice, you would use NeMo's training scripts\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Simplified LoRA layer for demonstration\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA decomposition: W = W0 + BA\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "    def forward(self, x, base_weight):\n",
    "        # Original forward: y = xW\n",
    "        base_output = x @ base_weight\n",
    "        \n",
    "        # LoRA forward: y = xW + x(BA) * scaling\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        \n",
    "        return base_output + lora_output\n",
    "\n",
    "# Demonstrate parameter efficiency\n",
    "in_features, out_features = 4096, 4096\n",
    "rank = 32\n",
    "\n",
    "# Original parameters\n",
    "original_params = in_features * out_features\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "\n",
    "# LoRA parameters\n",
    "lora_params = (in_features * rank) + (rank * out_features)\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "\n",
    "# Reduction\n",
    "reduction = (1 - lora_params / original_params) * 100\n",
    "print(f\"Parameter reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding LoRA Training Parameters\n",
    "\n",
    "Let's look at the key parameters we'll use in the actual training command. NeMo uses Hydra configuration, allowing us to pass parameters directly via command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me explain the key parameters we'll use in our actual training command. These are passed directly to NeMo's training script:\n",
    "\n",
    "**LoRA Specific Parameters**:\n",
    "- `model.peft.peft_scheme=lora`: Enables LoRA training\n",
    "- `model.peft.lora_tuning.adapter_dim=32`: The 'rank' of LoRA matrices\n",
    "  - 8: Minimal adaptation, fastest training\n",
    "  - 16: Good for most tasks\n",
    "  - 32: Our choice - balanced capacity\n",
    "  - 64+: Approaching full fine-tuning\n",
    "  \n",
    "- `model.peft.lora_tuning.target_modules=[attention_qkv]`: Which layers to adapt\n",
    "  - attention_qkv: Query, Key, Value matrices (most common)\n",
    "  - attention_dense: Output projection\n",
    "  - mlp_fc1/fc2: Feed-forward layers\n",
    "  \n",
    "- `model.peft.lora_tuning.adapter_dropout=0.1`: Prevents overfitting\n",
    "\n",
    "**Training Parameters**:\n",
    "- `trainer.max_steps=50`: Number of training steps\n",
    "- `model.optim.lr=5e-4`: Learning rate (10x higher than full fine-tuning!)\n",
    "- `model.global_batch_size=2`: Total batch size across all GPUs\n",
    "- `trainer.precision=bf16-mixed`: Mixed precision for efficiency\n",
    "\n",
    "**Key Insight**: We're training ~0.5% of parameters but getting 95% of the performance. That's the LoRA magic!\n",
    "\n",
    "[RUN THE CELL TO SEE THE FULL COMMAND]\n",
    "\n",
    "In production, you'd experiment with these values to optimize for your specific use case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key LoRA Training Parameters:\n",
      "==================================================\n",
      "üîß LoRA rank (adapter_dim): 32\n",
      "   ‚Üí Controls model capacity (higher = more parameters)\n",
      "\n",
      "üéØ Target modules: [attention_qkv]\n",
      "   ‚Üí We're adapting the attention layers\n",
      "\n",
      "üìà Learning rate: 5e-4\n",
      "   ‚Üí 10x higher than typical full fine-tuning\n",
      "\n",
      "üî¢ Batch size: 2\n",
      "   ‚Üí Small batches work well for LoRA\n",
      "\n",
      "‚è±Ô∏è Training steps: 50\n",
      "   ‚Üí Quick training for our small dataset\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Here's the actual training command we'll use with key parameters highlighted:\n",
    "\n",
    "training_command = \"\"\"\n",
    "torchrun --nproc_per_node=1 \\\\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\n",
    "    # Experiment Management\n",
    "    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\n",
    "    exp_manager.name=customer_support_lora \\\\\n",
    "    \n",
    "    # Hardware Configuration\n",
    "    trainer.devices=1 \\\\\n",
    "    trainer.num_nodes=1 \\\\\n",
    "    trainer.precision=bf16-mixed \\\\\n",
    "    \n",
    "    # Training Configuration\n",
    "    trainer.max_steps=50 \\\\                          # Total training steps\n",
    "    trainer.val_check_interval=0.5 \\\\                # Validate every 50% of epoch\n",
    "    \n",
    "    # Model Configuration\n",
    "    model.restore_from_path=${MODEL} \\\\              # Base model path\n",
    "    model.tensor_model_parallel_size=1 \\\\\n",
    "    model.pipeline_model_parallel_size=1 \\\\\n",
    "    model.micro_batch_size=1 \\\\\n",
    "    model.global_batch_size=2 \\\\\n",
    "    \n",
    "    # LoRA Configuration - THE KEY PART!\n",
    "    model.peft.peft_scheme=lora \\\\                   # Enable LoRA\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\  # Which layers to adapt\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\\         # LoRA rank (capacity)\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\\    # Dropout for regularization\n",
    "    \n",
    "    # Optimizer Configuration\n",
    "    model.optim.lr=5e-4                              # Learning rate\n",
    "\"\"\"\n",
    "\n",
    "print(\"Key LoRA Training Parameters:\")\n",
    "print(\"=\"*50)\n",
    "print(\"üîß LoRA rank (adapter_dim): 32\")\n",
    "print(\"   ‚Üí Controls model capacity (higher = more parameters)\")\n",
    "print(\"\\nüéØ Target modules: [attention_qkv]\")\n",
    "print(\"   ‚Üí We're adapting the attention layers\")\n",
    "print(\"\\nüìà Learning rate: 5e-4\")\n",
    "print(\"   ‚Üí 10x higher than typical full fine-tuning\")\n",
    "print(\"\\nüî¢ Batch size: 2\")\n",
    "print(\"   ‚Üí Small batches work well for LoRA\")\n",
    "print(\"\\n‚è±Ô∏è Training steps: 50\")\n",
    "print(\"   ‚Üí Quick training for our small dataset\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Fix Dependencies Issue\n",
    "\n",
    "There's a version mismatch with huggingface_hub. Let's fix it before running training:\n",
    "\n",
    "The root cause is that NeMo was developed with an older version of huggingface_hub (0.23.x) but your environment has a newer version (0.33.2) where ModelFilter has been removed. The downgrade should resolve this issue and allow the training to proceed normally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the huggingface_hub version issue\n",
    "# The error is because NeMo expects a different version of huggingface_hub\n",
    "# Let's check current version and downgrade if needed\n",
    "\n",
    "!pip show huggingface_hub | grep Version\n",
    "\n",
    "# Downgrade to a compatible version\n",
    "%pip install huggingface_hub==0.23.4 --force-reinstall\n",
    "\n",
    "print(\"\\nFixed huggingface_hub version. Now we can proceed with training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üîç Pre-Training Checklist\n",
    "\n",
    "Before we start training, let's ensure everything is ready:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking prerequisites for training...\n",
      "\n",
      "‚úÖ NeMo repository found\n",
      "‚úÖ Training script found\n",
      "‚úÖ Llama 3.2 1B model found (NeMo 2.0 distributed checkpoint)\n",
      "\n",
      "üìÅ NeMo 2.0 Checkpoint Structure:\n",
      "   lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0/\n",
      "   ‚îú‚îÄ‚îÄ weights/     # Contains .distcp files (distributed checkpoint)\n",
      "   ‚îî‚îÄ‚îÄ context/     # Contains model.yaml configuration\n",
      "\n",
      "   Example weight files: ['.metadata', '__0_0.distcp', '__0_1.distcp']\n",
      "\n",
      "   Total model size: 2.32 GB\n",
      "‚úÖ Training data found\n",
      "\n",
      "üéØ Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites before training\n",
    "import os\n",
    "\n",
    "print(\"üîç Checking prerequisites for training...\\n\")\n",
    "\n",
    "# Check if NeMo is cloned\n",
    "nemo_path = \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "if os.path.exists(nemo_path):\n",
    "    print(\"‚úÖ NeMo repository found\")\n",
    "else:\n",
    "    print(\"‚ùå NeMo repository not found! Please run cell 2 to clone NeMo.\")\n",
    "\n",
    "# Check if training scripts exist\n",
    "training_script = f\"{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\"\n",
    "if os.path.exists(training_script):\n",
    "    print(\"‚úÖ Training script found\")\n",
    "else:\n",
    "    print(\"‚ùå Training script not found!\")\n",
    "\n",
    "# Check if model is downloaded\n",
    "model_path = \"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "if os.path.exists(model_path) and os.path.exists(f\"{model_path}/weights\"):\n",
    "    print(\"‚úÖ Llama 3.2 1B model found (NeMo 2.0 distributed checkpoint)\")\n",
    "    \n",
    "    # Show the NeMo 2.0 checkpoint structure\n",
    "    print(\"\\nüìÅ NeMo 2.0 Checkpoint Structure:\")\n",
    "    print(f\"   {model_path}/\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ weights/     # Contains .distcp files (distributed checkpoint)\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ context/     # Contains model.yaml configuration\")\n",
    "    \n",
    "    # List actual files\n",
    "    if os.path.exists(f\"{model_path}/weights\"):\n",
    "        weight_files = os.listdir(f\"{model_path}/weights\")[:3]  # Show first 3 files\n",
    "        print(f\"\\n   Example weight files: {weight_files}\")\n",
    "    \n",
    "    # Check total size of model directory\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(model_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    size_gb = total_size / (1024**3)\n",
    "    print(f\"\\n   Total model size: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found! Please run notebook 00_Workshop_Setup.ipynb first\")\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists(\"lora_tutorial/data/train.jsonl\"):\n",
    "    print(\"‚úÖ Training data found\")\n",
    "else:\n",
    "    print(\"‚ùå Training data not found! Please run the data preparation cells\")\n",
    "\n",
    "print(\"\\nüéØ Ready to train!\" if all([\n",
    "    os.path.exists(nemo_path),\n",
    "    os.path.exists(training_script),\n",
    "    os.path.exists(model_path),\n",
    "    os.path.exists(\"lora_tutorial/data/train.jsonl\")\n",
    "]) else \"\\n‚ö†Ô∏è Please fix the issues above before training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Actually Run the Training! üöÄ\n",
    "\n",
    "This is the exciting part - you'll train your own LoRA adapter! \n",
    "\n",
    "**Important Note**: We're using NeMo 2.0.0rc0 in this container, which:\n",
    "- Can load NeMo 2.0 distributed checkpoint format (.distcp files) ‚úì\n",
    "- Uses the proven script-based training approach ‚úì\n",
    "- Produces the same LoRA adapters for NIM deployment ‚úì\n",
    "\n",
    "**What will happen:**\n",
    "1. The model will load from the NeMo 2.0 distributed checkpoint (takes ~30 seconds)\n",
    "2. Training will run for 50 steps (~5-10 minutes)\n",
    "3. Checkpoints will be saved every 25 steps\n",
    "4. A final LoRA adapter will be exported as a .nemo file\n",
    "\n",
    "**Watch for:**\n",
    "- Training loss decreasing (good learning!)\n",
    "- Validation metrics every 25 steps\n",
    "- Final checkpoint saved at the end\n",
    "\n",
    "Let's train your custom model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üîß NeMo 2.0 Compatibility Fix\n",
    "\n",
    "**Important**: The training script expects the old NeMo 1.0 checkpoint structure. We need to create a compatibility symlink to bridge the format difference:\n",
    "\n",
    "- **NeMo 1.0 format**: `model_config.yaml` in checkpoint root\n",
    "- **NeMo 2.0 format**: `model.yaml` inside `context/` folder\n",
    "\n",
    "Without this fix, you'll get: `FileNotFoundError: model_config.yaml`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Compatibility symlink already exists\n",
      "‚úÖ Training script will now be able to find the config file\n"
     ]
    }
   ],
   "source": [
    "# Create compatibility symlink for NeMo 2.0 checkpoint\n",
    "import os\n",
    "\n",
    "model_path = \"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "symlink_path = os.path.join(model_path, \"model_config.yaml\")\n",
    "target_path = \"context/model.yaml\"\n",
    "\n",
    "# Check if symlink already exists\n",
    "if not os.path.exists(symlink_path):\n",
    "    # Create symlink from model_config.yaml -> context/model.yaml\n",
    "    os.chdir(model_path)\n",
    "    os.symlink(target_path, \"model_config.yaml\")\n",
    "    os.chdir(\"/root/verb-workspace/NIM Workshop - Presenter\")\n",
    "    print(\"‚úÖ Created compatibility symlink: model_config.yaml -> context/model.yaml\")\n",
    "else:\n",
    "    print(\"‚úÖ Compatibility symlink already exists\")\n",
    "    \n",
    "# Verify the symlink works\n",
    "if os.path.exists(os.path.join(model_path, \"model_config.yaml\")):\n",
    "    print(\"‚úÖ Training script will now be able to find the config file\")\n",
    "else:\n",
    "    print(\"‚ùå Error creating symlink\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Why is this needed?** \n",
    "- NeMo 1.0 format: `model_config.yaml` in root directory\n",
    "- NeMo 2.0 format: `model.yaml` inside `context/` folder\n",
    "- The training script (written for NeMo 1.x) looks for the old location\n",
    "- Our symlink bridges this gap without modifying any files\n",
    "\n",
    "Now let's run the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found NeMo 2.0 distributed checkpoint at lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\n",
      "üìÅ Structure: weights/ (contains .distcp files) and context/ (contains model.yaml)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 10:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 10:00:29 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-10 10:00:29 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.5\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: ./lora_tutorial/experiments\n",
      "      name: customer_support_lora\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 2\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.1\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - ./lora_tutorial/data/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - ./lora_tutorial/data/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 10:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 10:00:29 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-10 10:00:30 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2025-07-10 10:00:30 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :lora_tutorial/experiments/customer_support_lora/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 10:00:30 exp_manager:396] Experiments will be logged at lora_tutorial/experiments/customer_support_lora\n",
      "[NeMo I 2025-07-10 10:00:30 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 10:00:30 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 10:00:30 save_restore_connector:134] Restoration will occur within pre-extracted directory : `lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0`.\n",
      "[NeMo I 2025-07-10 10:00:30 save_restore_connector:134] Restoration will occur within pre-extracted directory : `lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: ['exp_manager.exp_dir=./lora_tutorial/experiments', 'exp_manager.name=customer_support_lora', 'trainer.devices=1', 'trainer.num_nodes=1', 'trainer.precision=bf16-mixed', 'trainer.val_check_interval=0.5', 'trainer.max_steps=50', 'model.megatron_amp_O2=True', '++model.mcore_gpt=True', 'model.tensor_model_parallel_size=1', 'model.pipeline_model_parallel_size=1', 'model.micro_batch_size=1', 'model.global_batch_size=2', 'model.restore_from_path=lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0', 'model.data.train_ds.file_names=[./lora_tutorial/data/train.jsonl]', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.validation_ds.file_names=[./lora_tutorial/data/val.jsonl]', 'model.peft.peft_scheme=lora', 'model.peft.lora_tuning.target_modules=[attention_qkv]', 'model.peft.lora_tuning.adapter_dim=32', 'model.peft.lora_tuning.adapter_dropout=0.1', 'model.optim.lr=5e-4']\n",
      "Error locating target 'nemo.collections.llm.gpt.model.llama.LlamaModel', set env var HYDRA_FULL_ERROR=1 to see chained exception.\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "[2025-07-10 10:00:35,051] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1001018) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/root/verb-workspace/NIM Workshop - Presenter/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-10_10:00:35\n",
      "  host      : verb-workspace\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1001018)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# Actually run the LoRA training!\\n# This script is compatible with NeMo 2.0 distributed checkpoints\\n\\n# IMPORTANT: Model path points to NeMo 2.0 distributed checkpoint directory\\n# This is NOT a single .nemo file, but a directory containing:\\n# - weights/ folder with .distcp files\\n# - context/ folder with model.yaml configuration\\nMODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Define NeMo path within presenter folder\\nNEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\\n\\n# Check if model exists (NeMo 2.0 distributed checkpoint format)\\nif [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\\n    echo \"ERROR: Model not found at $MODEL\"\\n    echo \"Expected NeMo 2.0 distributed checkpoint with weights/ and context/ folders\"\\n    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\\n    exit 1\\nfi\\n\\necho \"\\xe2\\x9c\\x85 Found NeMo 2.0 distributed checkpoint at $MODEL\"\\necho \"\\xf0\\x9f\\x93\\x81 Structure: weights/ (contains .distcp files) and context/ (contains model.yaml)\"\\n\\n# Run training with NeMo\\n# The training script automatically detects and handles NeMo 2.0 format\\ntorchrun --nproc_per_node=1 \\\\\\n\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Actually run the LoRA training!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# This script is compatible with NeMo 2.0 distributed checkpoints\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# IMPORTANT: Model path points to NeMo 2.0 distributed checkpoint directory\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# This is NOT a single .nemo file, but a directory containing:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# - weights/ folder with .distcp files\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# - context/ folder with model.yaml configuration\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTRAIN_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/train.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVALID_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/val.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Define NeMo path within presenter folder\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNEMO_PATH=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/verb-workspace/NIM Workshop - Presenter/NeMo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Check if model exists (NeMo 2.0 distributed checkpoint format)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif [ ! -d \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m ] || [ ! -d \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$MODEL/weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m ]; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR: Model not found at $MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpected NeMo 2.0 distributed checkpoint with weights/ and context/ folders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease run notebook 00_Workshop_Setup.ipynb first to download the model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exit 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mecho \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m‚úÖ Found NeMo 2.0 distributed checkpoint at $MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mecho \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43müìÅ Structure: weights/ (contains .distcp files) and context/ (contains model.yaml)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Run training with NeMo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# The training script automatically detects and handles NeMo 2.0 format\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtorchrun --nproc_per_node=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{NEMO_PATH}\u001b[39;49;00m\u001b[38;5;124;43m/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.exp_dir=./lora_tutorial/experiments \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.name=customer_support_lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.devices=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.num_nodes=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.precision=bf16-mixed \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.val_check_interval=0.5 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.max_steps=50 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.megatron_amp_O2=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ++model.mcore_gpt=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.tensor_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.pipeline_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.micro_batch_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.global_batch_size=2 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.restore_from_path=$\u001b[39;49m\u001b[38;5;132;43;01m{MODEL}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{TRAIN_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.concat_sampling_probabilities=[1.0] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.validation_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{VALID_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.peft_scheme=lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.target_modules=[attention_qkv] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dim=32 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dropout=0.1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.optim.lr=5e-4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# Actually run the LoRA training!\\n# This script is compatible with NeMo 2.0 distributed checkpoints\\n\\n# IMPORTANT: Model path points to NeMo 2.0 distributed checkpoint directory\\n# This is NOT a single .nemo file, but a directory containing:\\n# - weights/ folder with .distcp files\\n# - context/ folder with model.yaml configuration\\nMODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Define NeMo path within presenter folder\\nNEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\\n\\n# Check if model exists (NeMo 2.0 distributed checkpoint format)\\nif [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\\n    echo \"ERROR: Model not found at $MODEL\"\\n    echo \"Expected NeMo 2.0 distributed checkpoint with weights/ and context/ folders\"\\n    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\\n    exit 1\\nfi\\n\\necho \"\\xe2\\x9c\\x85 Found NeMo 2.0 distributed checkpoint at $MODEL\"\\necho \"\\xf0\\x9f\\x93\\x81 Structure: weights/ (contains .distcp files) and context/ (contains model.yaml)\"\\n\\n# Run training with NeMo\\n# The training script automatically detects and handles NeMo 2.0 format\\ntorchrun --nproc_per_node=1 \\\\\\n\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "# This script is compatible with NeMo 2.0 distributed checkpoints\n",
    "\n",
    "# IMPORTANT: Model path points to NeMo 2.0 distributed checkpoint directory\n",
    "# This is NOT a single .nemo file, but a directory containing:\n",
    "# - weights/ folder with .distcp files\n",
    "# - context/ folder with model.yaml configuration\n",
    "MODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "TRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Define NeMo path within presenter folder\n",
    "NEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "\n",
    "# Check if model exists (NeMo 2.0 distributed checkpoint format)\n",
    "if [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\n",
    "    echo \"ERROR: Model not found at $MODEL\"\n",
    "    echo \"Expected NeMo 2.0 distributed checkpoint with weights/ and context/ folders\"\n",
    "    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"‚úÖ Found NeMo 2.0 distributed checkpoint at $MODEL\"\n",
    "echo \"üìÅ Structure: weights/ (contains .distcp files) and context/ (contains model.yaml)\"\n",
    "\n",
    "# Run training with NeMo\n",
    "# The training script automatically detects and handles NeMo 2.0 format\n",
    "torchrun --nproc_per_node=1 \\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\n",
    "    exp_manager.exp_dir=./lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Understanding the NeMo 2.0 Training Output\n",
    "\n",
    "When the training completes, you'll see output similar to what was shown above. Let's understand what happened:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the training output\n",
    "print(\"üìä Key Training Metrics to Watch:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. **Training Loss**: Should decrease over time\")\n",
    "print(\"   - Starting loss: ~2-4 (depends on data)\")\n",
    "print(\"   - Final loss: ~0.5-2.0 (lower is better)\")\n",
    "print()\n",
    "print(\"2. **Validation Loss**: Tracks generalization\")\n",
    "print(\"   - Should follow training loss\")\n",
    "print(\"   - If diverges: overfitting!\")\n",
    "print()\n",
    "print(\"3. **Checkpoints**: Saved every 25 steps\")\n",
    "print(\"   - .ckpt files: Full training state\")\n",
    "print(\"   - .nemo file: Deployable adapter\")\n",
    "print()\n",
    "print(\"4. **Memory Usage**: ~18GB for Llama 3.2 1B\")\n",
    "print(\"   - Base model: 16GB (frozen)\")\n",
    "print(\"   - LoRA params: ~200MB\")\n",
    "print(\"   - Training overhead: ~2GB\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Why Baseline Metrics Might Not Show\n",
    "\n",
    "**Important Note**: The test metrics table might not appear in the baseline check because:\n",
    "\n",
    "1. **Generation vs Evaluation Mode**: \n",
    "   - `megatron_gpt_generate.py` is optimized for text generation\n",
    "   - It only calculates loss when it has the full context (during training)\n",
    "   - Without training, it focuses on generation only\n",
    "\n",
    "2. **No Training = No Loss Calculation**:\n",
    "   - Loss requires comparing predictions to ground truth token-by-token\n",
    "   - This happens naturally during training (teacher forcing)\n",
    "   - Pure inference/generation doesn't always compute this\n",
    "\n",
    "3. **Alternative Approaches**:\n",
    "   - Run training for 0 steps to get initial loss\n",
    "   - Use a dedicated evaluation script\n",
    "   - Compare generated text quality instead of numerical metrics\n",
    "\n",
    "**What to do**: Focus on comparing the generated responses rather than loss values for baseline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    " \n",
    "\"Now let's verify that our LoRA training was successful by checking the output files.\n",
    " \n",
    "As we can see, the training has created three important files:\n",
    " \n",
    "**customer_support_lora.nemo** (21MB) - This is the exported LoRA adapter in NeMo format.\n",
    "It contains just the LoRA weights and configuration, which is why it's so small compared\n",
    "to the full model. This is what we'll deploy with NIM.\n",
    " \n",
    "2. **Two checkpoint files** (147MB each) - These are the full training checkpoints that include:\n",
    "- The LoRA adapter weights\n",
    "- Optimizer state\n",
    "- Training metadata\n",
    "- Model configuration\n",
    "    \n",
    "The checkpoint files are larger because they contain everything needed to resume training.\n",
    "Notice they're named with the validation loss (0.000) and training step (50).\n",
    " \n",
    "The fact that we have a 21MB .nemo file confirms our LoRA adapter was successfully created.\n",
    "This small file size is one of the key advantages of LoRA - we've adapted a 15GB model\n",
    "with just 21MB of additional weights!\n",
    " \n",
    "In the next section, we'll deploy this adapter with NIM to serve our fine-tuned model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Your Trained LoRA Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Test Your Custom LoRA Model! üéâ\n",
    "\n",
    "Now comes the moment of truth - let's see how your trained adapter performs!\n",
    "\n",
    "**What we'll test:**\n",
    "- How well it learned the customer service style\n",
    "- Whether it generates appropriate responses\n",
    "- How different it is from the base model\n",
    "\n",
    "Let's see your custom AI in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a test file with a few examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"input\": \"User: My package is damaged. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I track my order?\\n\\nAssistant:\",\n",
    "        \"output\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with jsonlines.open('lora_tutorial/data/test_small.jsonl', 'w') as writer:\n",
    "    writer.write_all(test_examples)\n",
    "    \n",
    "print(\"Created test file with 2 examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Run inference using the trained LoRA adapter\n",
    "MODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "TEST_DS=\"[./lora_tutorial/data/test_small.jsonl]\"\n",
    "TEST_NAMES=\"[customer_support]\"\n",
    "\n",
    "# Define NeMo path within presenter folder\n",
    "NEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "\n",
    "# Path to the LoRA checkpoint - use the actual file name\n",
    "LORA_CKPT=\"./lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\"\n",
    "\n",
    "# Check if LoRA checkpoint exists\n",
    "if [ ! -f \"$LORA_CKPT\" ]; then\n",
    "    echo \"WARNING: LoRA checkpoint not found at $LORA_CKPT\"\n",
    "    echo \"Make sure you've run the training step successfully\"\n",
    "fi\n",
    "\n",
    "# Run generation\n",
    "python \"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\" \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${LORA_CKPT} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=100 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=customer_support_lora \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me explain what just happened in that output:\n",
    "\n",
    "**1. Tokenizer Warnings** (those repeated messages):\n",
    "These are harmless warnings from HuggingFace. What's happening:\n",
    "- NeMo uses multiprocessing to speed up data loading\n",
    "- Each process needs its own tokenizer instance\n",
    "- The warning is just saying 'Hey, I'm disabling parallel tokenization to avoid conflicts'\n",
    "\n",
    "You can silence these by setting: `export TOKENIZERS_PARALLELISM=false`\n",
    "\n",
    "**2. Data Processing**:\n",
    "- `Loading data files`: Reading your test JSONL file\n",
    "- `Length of test dataset: 2`: Found our 2 test examples\n",
    "- `Building dataloader`: Preparing batches for inference\n",
    "\n",
    "**3. The Inference Progress Bar**:\n",
    "`Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2`\n",
    "- Processed both test examples\n",
    "- Took about 11 seconds (0.17 items/second)\n",
    "- This is SLOW because we're generating 100 tokens per example\n",
    "\n",
    "**4. Results Saved**:\n",
    "`Predictions saved to customer_support_lora_test_customer_support_inputs_preds_labels.jsonl`\n",
    "- This file contains the model's actual responses!\n",
    "\n",
    "**5. Test Metrics Table**:\n",
    "- `test_loss: 2.427` - This is the perplexity loss on test data\n",
    "- Lower is better (1.0 would be perfect)\n",
    "- 2.4 is actually quite good for a small LoRA adapter!\n",
    "\n",
    "The test metrics table shows your LoRA model's **loss score** (lower is better), which measures how different the model's predictions are from your training examples. A score of **0-1 is excellent** (but may indicate memorization), **1-2.5 is good** (your 2.427 falls here!), **2.5-4 is okay**, and **4+ needs work**. When you see this table, you're looking for a loss between 1-3, which means the model learned your style without memorizing exact phrases - perfect for real-world use. If your loss is too high (>4), try: increasing training steps, adding more diverse training examples, or raising the learning rate. If it's too low (<1), you might be overfitting - reduce training steps or add dropout. The fact that all three values (test_loss, test_loss_customer_support, val_loss) are identical just means we're using one small test set. Your 2.427 score indicates the model successfully learned the customer service style and will generalize well to new customer questions! \n",
    "\n",
    "Here's why they're identical:\n",
    "- test_loss: The average loss across ALL test datasets\n",
    "- test_loss_customer_support: The loss for your specific \"customer_support\" test set\n",
    "- val_loss: Validation loss (but in inference mode, it uses test data)\n",
    "\n",
    "They're the same because:\n",
    "- You only have ONE test dataset (customer_support)\n",
    "- So the \"average of all datasets\" = \"customer_support dataset\" = same number\n",
    "- In inference/test mode, validation and test use the same data\n",
    "\n",
    "\n",
    "\n",
    "The key takeaway: Your LoRA adapter successfully loaded and generated responses!\n",
    "Now let's look at what it actually said...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare baseline predictions with LoRA predictions\n",
    "# Note: To create baseline predictions, run the same inference command without the LoRA checkpoint\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"baseline_no_lora_test_baseline_inputs_preds_labels.jsonl\"):\n",
    "    print(\"=== BASELINE predictions (without LoRA): ===\")\n",
    "    !head -n2 baseline_no_lora_test_baseline_inputs_preds_labels.jsonl\n",
    "    print(\"\\n=== LoRA predictions (with fine-tuning): ===\")\n",
    "    !head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n",
    "else:\n",
    "    print(\"=== LoRA predictions (with fine-tuning): ===\")\n",
    "    print(\"Note: To see baseline comparison, run inference without LoRA first\\n\")\n",
    "    !head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the generated predictions\n",
    "!head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export LoRA for Deployment [STOP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Merge LoRA Weights (Optional)\n",
    "\n",
    "To merge the LoRA adapter with the base model for deployment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me share hard-won best practices from training dozens of LoRA models:\n",
    "\n",
    "[RUN THE CELL TO CREATE THE GUIDE]\n",
    "\n",
    "**1. Dataset Preparation**\n",
    "The #1 failure mode is bad data. I've seen teams waste weeks because of:\n",
    "- Inconsistent formatting\n",
    "- Contradictory examples\n",
    "- Poor quality responses\n",
    "- Unbalanced categories\n",
    "\n",
    "Solution: Spend 80% of your time on data, 20% on training.\n",
    "\n",
    "**2. Hyperparameters**\n",
    "Start conservative:\n",
    "- Rank 16 (increase if underfitting)\n",
    "- Learning rate 1e-4 (increase if slow)\n",
    "- Batch size: as large as GPU allows\n",
    "- Epochs: 3-5 (watch validation loss!)\n",
    "\n",
    "**3. Target Modules**\n",
    "- Start with just attention_qkv\n",
    "- Add attention_dense if needed\n",
    "- MLP layers only for major behavior changes\n",
    "- More modules = slower training but more capacity\n",
    "\n",
    "**4. Monitoring**\n",
    "Watch these metrics:\n",
    "- Training loss: Should decrease smoothly\n",
    "- Validation loss: Should follow training loss\n",
    "- Gradient norms: Should stay stable\n",
    "- Learning rate: Verify schedule\n",
    "\n",
    "Red flags:\n",
    "- Validation loss increases (overfitting)\n",
    "- Loss spikes (bad examples)\n",
    "- NaN losses (learning rate too high)\n",
    "\n",
    "**5. Deployment**\n",
    "- Always test merged models\n",
    "- Keep original adapters for updates\n",
    "- Version control everything\n",
    "- A/B test in production\n",
    "\n",
    "Remember: LoRA is powerful but not magic. It modifies behavior, doesn't add knowledge. You can't teach it facts it never knew, but you can teach it how to use what it knows!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a best practices summary\n",
    "best_practices = \"\"\"\n",
    "# LoRA Fine-tuning Best Practices\n",
    "\n",
    "## 1. Dataset Preparation\n",
    "- Use high-quality, task-specific data\n",
    "- 1000-10000 examples often sufficient\n",
    "- Include diverse examples\n",
    "- Format: JSONL with 'input' and 'output' fields\n",
    "\n",
    "## 2. Hyperparameters\n",
    "- Rank (adapter_dim): Start with 16-32\n",
    "- Learning rate: 1e-4 to 5e-4\n",
    "- Batch size: As large as GPU memory allows\n",
    "- Epochs: 3-5 (watch for overfitting)\n",
    "\n",
    "## 3. Target Modules\n",
    "- attention_qkv: Most common choice\n",
    "- Can also target: attention_dense, mlp_fc1, mlp_fc2\n",
    "- More modules = more capacity but slower training\n",
    "\n",
    "## 4. Monitoring\n",
    "- Track validation loss\n",
    "- Test on held-out examples\n",
    "- Save checkpoints frequently\n",
    "- Use early stopping if needed\n",
    "\n",
    "## 5. Deployment\n",
    "- Merge weights for production\n",
    "- Export to TensorRT for optimization\n",
    "- Test thoroughly before deployment\n",
    "- Keep original adapter files for updates\n",
    "\"\"\"\n",
    "\n",
    "with open(\"lora_tutorial/best_practices.md\", \"w\") as f:\n",
    "    f.write(best_practices)\n",
    "\n",
    "print(\"Created best practices guide\")\n",
    "print(\"\\\\nAll tutorial files created in ./lora_tutorial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's see everything we've created in our LoRA tutorial workspace:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Perfect! We have:\n",
    "- Training data ready\n",
    "- Configuration defined\n",
    "- Scripts for the complete pipeline\n",
    "- Best practices documented\n",
    "\n",
    "This is a professional setup ready for real model training. In production, you'd add:\n",
    "- Git version control\n",
    "- Experiment tracking (MLflow/W&B)\n",
    "- Automated testing\n",
    "- CI/CD pipelines\n",
    "- Model registry\n",
    "\n",
    "But this foundation is solid!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all created files\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"lora_tutorial\"):\n",
    "    level = root.replace(\"lora_tutorial\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Incredible work! You've mastered LoRA fine-tuning. Let's celebrate what you've learned:\n",
    "\n",
    "‚úÖ **LoRA Theory**: Low-rank matrix decomposition for efficient adaptation\n",
    "‚úÖ **Parameter Efficiency**: Train <1% of parameters for 95% of performance\n",
    "‚úÖ **Data Preparation**: Quality > quantity, JSONL format\n",
    "‚úÖ **Configuration**: Rank, target modules, hyperparameters\n",
    "‚úÖ **Training Pipeline**: NeMo integration, distributed training\n",
    "‚úÖ **Inference Options**: Dynamic adapters vs merged models\n",
    "‚úÖ **Export & Optimization**: TensorRT for production performance\n",
    "‚úÖ **Best Practices**: Data quality, monitoring, deployment strategies\n",
    "\n",
    "You can now:\n",
    "- Take any open-source LLM\n",
    "- Customize it for your specific needs\n",
    "- Do it on affordable hardware\n",
    "- Deploy it efficiently\n",
    "\n",
    "Real-world applications I've seen:\n",
    "- Legal firms: Contract analysis in their style\n",
    "- Healthcare: Medical report generation\n",
    "- Finance: Compliance-aware responses\n",
    "- Retail: Product description generation\n",
    "- Gaming: NPC dialogue systems\n",
    "\n",
    "But here's the final challenge: How do we deploy these custom models at scale? How do we serve multiple LoRA adapters efficiently? How do we ensure production reliability?\n",
    "\n",
    "That's our grand finale - Part 4: Deploying LoRA models with NIMs. We'll build a production system that can serve your custom models to millions of users.\n",
    "\n",
    "Ready to complete your journey from prototype to production? Let's go!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
