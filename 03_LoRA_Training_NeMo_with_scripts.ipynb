{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune Llama 3.1 8B Instruct using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~100-500MB for 8B models)\n",
    "\n",
    "The focus of this workshop is not the specifics of LoRA, but to actually give everyone an guide on how to carry out the process of tuning your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: NeMo Framework Setup\n",
    "\n",
    "This notebook requires the NVIDIA NeMo framework for LoRA training. We'll use the cloned NeMo repository to access the necessary training scripts.\n",
    "\n",
    "**NeMo Compatibility**: \n",
    "- The downloaded model uses standard NeMo format (.nemo file)\n",
    "- The training scripts work directly without any modifications\n",
    "\n",
    "**Training Experience**: In this workshop, you'll train your own LoRA adapter from scratch! This gives you hands-on experience with:\n",
    "- Setting up training data\n",
    "- Configuring LoRA parameters\n",
    "- Running the actual training\n",
    "- Testing your custom adapter\n",
    "\n",
    "The training process takes approximately 5-10 minutes for our small example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Now, we'll clone the NeMo reposiory from github.com/NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo repository already exists.\n",
      "\n",
      "Checking for required NeMo scripts:\n",
      "‚úì Found: megatron_gpt_finetuning.py\n",
      "‚úì Found: megatron_gpt_generate.py\n",
      "‚úì Found: merge.py\n"
     ]
    }
   ],
   "source": [
    "# Clone NeMo repository if not already present\n",
    "import os\n",
    "\n",
    "# Use relative path for NeMo\n",
    "nemo_path = './NeMo'\n",
    "\n",
    "if not os.path.exists(nemo_path):\n",
    "    print(\"Cloning NeMo repository...\")\n",
    "    !git clone https://github.com/NVIDIA/NeMo.git {nemo_path}\n",
    "    print(\"NeMo repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"NeMo repository already exists.\")\n",
    "    \n",
    "# Verify the training scripts exist\n",
    "nemo_scripts = [\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py',\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py',\n",
    "    f'{nemo_path}/scripts/nlp_language_modeling/merge_lora_weights/merge.py'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for required NeMo scripts:\")\n",
    "for script in nemo_scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"‚úì Found: {os.path.basename(script)}\")\n",
    "    else:\n",
    "        print(f\"‚úó Missing: {script}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Welcome to the most transformative part of our journey - LoRA fine-tuning! This is where you go from using someone else's AI to creating YOUR OWN specialized AI.\n",
    "\n",
    "Let me start with a real story. A Fortune 500 company came to us with a problem. They loved Llama 3 70B but needed it to understand their internal jargon - thousands of product codes, technical terms, and specific procedures. \n",
    "\n",
    "The traditional solution? Fine-tune the entire 70B parameter model. That would require:\n",
    "- 8 H100 GPUs ($300,000+ hardware)\n",
    "- 2 weeks of training time  \n",
    "- Machine learning PhD to manage it\n",
    "- $50,000+ in electricity\n",
    "\n",
    "Their budget? One RTX 4090 and a week.\n",
    "\n",
    "Enter LoRA - Low-Rank Adaptation. Instead of training all 70 billion parameters, LoRA adds small 'adapter' matrices that modify the model's behavior. Imagine it like putting specialized glasses on the model - it sees everything through your custom lens.\n",
    "\n",
    "The results for that company?\n",
    "- Trained on 1 RTX 4090\n",
    "- 6 hours total time\n",
    "- Junior developer managed it\n",
    "- Under $100 in costs\n",
    "- Model performed BETTER than full fine-tuning for their use case\n",
    "\n",
    "Today, I'll show you exactly how to do this. By the end, you'll be able to create custom AI models tailored to your exact needs!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Now let's set up our environment.\n",
    "\n",
    "We're installing:\n",
    "- **jsonlines**: For handling our training data format - it's like CSV but better for text\n",
    "- **transformers**: Hugging Face's library - we'll use their tokenizer \n",
    "- **omegaconf**: For configuration management - makes our life SO much easier\n",
    "- **pytorch-lightning**: The training framework that handles all the heavy lifting\n",
    "\n",
    "*[Wait for installation to complete]*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0a0+ebedce2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.5.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.14.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.3.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0a0+ebedce2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.41 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Time to create our training data! This is where the magic begins. We're creating a customer support AI, but here's the key insight: we're not teaching it WHAT customer support is - Llama already knows that. We're teaching it HOW to do customer support in YOUR specific style.\n",
    "\n",
    "Look at these examples:\n",
    "- Notice the consistent, professional tone\n",
    "- See how each response acknowledges the issue first\n",
    "- Watch how we always offer a clear next step\n",
    "\n",
    "This is called 'style transfer' in AI. It's like teaching someone who already speaks English to speak with a British accent - they know the language, they just need to learn the style.\n",
    "\n",
    "With just 5 examples, we can dramatically change how the model responds. In production, I've seen companies achieve 90%+ accuracy with as few as 50 examples. That's the power of LoRA!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 training examples\n",
      "Created 2 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Prerequisites Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking prerequisites for training...\n",
      "\n",
      "‚úÖ NeMo repository found\n",
      "‚úÖ Training script found\n",
      "‚úÖ Llama 3.1 8B model found\n",
      "\n",
      "üìÅ Model Information:\n",
      "   Path: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
      "   Size: 14.96 GB\n",
      "   Format: Standard NeMo checkpoint (.nemo)\n",
      "‚úÖ Training data found\n",
      "\n",
      "üéØ Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites before training\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"üîç Checking prerequisites for training...\\n\")\n",
    "\n",
    "# Check if NeMo is cloned - use relative path\n",
    "nemo_path = \"./NeMo\"\n",
    "if os.path.exists(nemo_path):\n",
    "    print(\"‚úÖ NeMo repository found\")\n",
    "else:\n",
    "    print(\"‚ùå NeMo repository not found! Please run cell 2 to clone NeMo.\")\n",
    "\n",
    "# Check if training scripts exist\n",
    "training_script = f\"{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\"\n",
    "if os.path.exists(training_script):\n",
    "    print(\"‚úÖ Training script found\")\n",
    "else:\n",
    "    print(\"‚ùå Training script not found!\")\n",
    "\n",
    "# Check if model is downloaded - look in subdirectories since NGC creates them\n",
    "model_files = glob.glob(\"lora_tutorial/models/llama-3_1-8b-instruct/**/*.nemo\", recursive=True)\n",
    "if model_files:\n",
    "    model_path = model_files[0]  # Use the first .nemo file found\n",
    "    # Check if it's a complete model (>10GB)\n",
    "    size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    if size_gb > 10:\n",
    "        print(\"‚úÖ Llama 3.1 8B model found\")\n",
    "        print(f\"\\nüìÅ Model Information:\")\n",
    "        print(f\"   Path: {model_path}\")\n",
    "        print(f\"   Size: {size_gb:.2f} GB\")\n",
    "        print(f\"   Format: Standard NeMo checkpoint (.nemo)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Incomplete model found ({size_gb:.1f} GB)\")\n",
    "        print(\"   Please re-run the download in 00_Workshop_Setup.ipynb\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found! Please run notebook 00_Workshop_Setup.ipynb first\")\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists(\"lora_tutorial/data/train.jsonl\"):\n",
    "    print(\"‚úÖ Training data found\")\n",
    "else:\n",
    "    print(\"‚ùå Training data not found! Please run the data preparation cells\")\n",
    "\n",
    "print(\"\\nüéØ Ready to train!\" if all([\n",
    "    os.path.exists(nemo_path),\n",
    "    os.path.exists(training_script),\n",
    "    len(model_files) > 0 and os.path.getsize(model_files[0]) / (1024**3) > 10,  # Check complete model\n",
    "    os.path.exists(\"lora_tutorial/data/train.jsonl\")\n",
    "]) else \"\\n‚ö†Ô∏è Please fix the issues above before training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Before we hit that training button, let me explain what's about to happen - because this is genuinely exciting!\n",
    "\n",
    "When you press run on the next cell:\n",
    "1. **Model Loading** (~30 seconds): We'll load 8 billion parameters into GPU memory\n",
    "2. **LoRA Injection**: We'll inject our tiny 10.5M parameter adapters - that's only 0.13% of the model!\n",
    "3. **Training Loop**: You'll see the loss dropping - that's the AI learning your style in real-time\n",
    "4. **Checkpointing**: Every 25 steps, we save progress - like auto-save in a video game\n",
    "\n",
    "‚ö†Ô∏è **Common Issues to Address:**\n",
    "- 'Oh no, so many warnings!' - That's normal! NeMo is very verbose. Only worry if you see ERROR in red\n",
    "- 'Why is it taking so long to start?' - Loading 8B parameters takes time, like starting a jumbo jet\n",
    "- 'My loss isn't dropping!' - Give it 10-20 steps, LoRA starts slow then accelerates\n",
    "\n",
    "üéØ **What to watch for:**\n",
    "- Training loss going from ~2.0 down to ~0.001 - that's learning!\n",
    "- 'Validation loss' appearing every 25 steps - that's testing\n",
    "- The final message about saving checkpoints - that's success!\n",
    "\n",
    "Ready? Let's train your AI! üöÄ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run LoRA Training\n",
    "\n",
    "### Actually Run the Training! üöÄ\n",
    "\n",
    "This is the exciting part - you'll train your own LoRA adapter! \n",
    "\n",
    "**What will happen:**\n",
    "1. The model will load from the .nemo checkpoint (takes ~30 seconds)\n",
    "2. Training will run for 50 steps (~5-10 minutes)\n",
    "3. Checkpoints will be saved every 25 steps\n",
    "4. A final LoRA adapter will be exported as a .nemo file\n",
    "\n",
    "**Watch for:**\n",
    "- Training loss decreasing (good learning!)\n",
    "- Validation metrics every 25 steps\n",
    "- Final checkpoint saved at the end\n",
    "\n",
    "**Note about warnings**: You'll see many warnings about missing configuration fields - these are normal and can be ignored. They appear because NeMo supports many optional features that aren't used in this training.\n",
    "\n",
    "**Important**: Llama 3.1 models use Group Query Attention (GQA), which requires `mcore_gpt=True` to be set.\n",
    "\n",
    "Let's train your custom model:\n",
    "\n",
    "üé§ **PRESENTER SCRIPT (During Training):**\n",
    "\n",
    "\"While this is running, let me share what's happening under the hood:\n",
    "\n",
    "**Step 1-10**: The model is 'confused' - high loss, random outputs. Like a student on day one.\n",
    "\n",
    "**Step 10-25**: Pattern recognition kicks in. The model starts to see 'Oh, I should acknowledge the problem first!'\n",
    "\n",
    "**Step 25-40**: Fine-tuning happens. Now it's learning your specific phrases and tone.\n",
    "\n",
    "**Step 40-50**: Polishing. Minor adjustments to perfect the style.\n",
    "\n",
    "See that loss number dropping? Every decrease means your AI is getting smarter. It's literally learning as we watch!\n",
    "\n",
    "Fun fact: OpenAI's GPT models train for millions of steps. We're doing 50. That's the power of starting with a pre-trained model - we're not teaching it English, just your style of English!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found Llama 3.1 8B model at lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:22:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:22:56 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-11 17:22:56 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.5\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: lora_tutorial/experiments\n",
      "      name: customer_support_lora\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 2\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.1\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - lora_tutorial/data/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - lora_tutorial/data/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:22:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:22:56 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-11 17:22:56 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2025-07-11 17:22:56 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :lora_tutorial/experiments/customer_support_lora/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:22:56 exp_manager:396] Experiments will be logged at lora_tutorial/experiments/customer_support_lora\n",
      "[NeMo I 2025-07-11 17:22:56 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:22:56 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "25-07-11 17:23:15 - PID:57966 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:23:15 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:23:15 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:23:15 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:23:16 megatron_base_model:584] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:23:16 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:23:39 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "Loading distributed checkpoint directly on the GPU\n",
      "[NeMo I 2025-07-11 17:24:21 nlp_overrides:1180] Model MegatronGPTSFTModel was successfully restored from /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo.\n",
      "[NeMo I 2025-07-11 17:24:21 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2025-07-11 17:24:21 nlp_adapter_mixins:203] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-11 17:24:25 nlp_adapter_mixins:208] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:811] Building GPT SFT validation datasets.\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:495] Building indexing for fn = lora_tutorial/data/val.jsonl\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:507] Saving idx file = lora_tutorial/data/val.jsonl.idx.npy\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:509] Saving metadata file = lora_tutorial/data/val.jsonl.idx.info\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.089823\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.085401\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:249] Loading lora_tutorial/data/val.jsonl\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001170\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:815] Length of val dataset: 2\n",
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:822] Building GPT SFT traing datasets.\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:495] Building indexing for fn = lora_tutorial/data/train.jsonl\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:507] Saving idx file = lora_tutorial/data/train.jsonl.idx.npy\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:509] Saving metadata file = lora_tutorial/data/train.jsonl.idx.info\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.087212\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.086808\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:249] Loading lora_tutorial/data/train.jsonl\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000915\n",
      "[NeMo I 2025-07-11 17:24:25 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2025-07-11 17:24:25 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.05 (sec)\n",
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:824] Length of train dataset: 101\n",
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2025-07-11 17:24:25 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-07-11 17:24:25 megatron_base_model:1199] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-11 17:24:25 nlp_adapter_mixins:269] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-11 17:24:25 modelPT:770] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2025-07-11 17:24:25 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x751c8e6d5ff0>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n",
      "[NeMo I 2025-07-11 17:24:25 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x751c8e6e3760>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 8.0 B  | train\n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=29` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:25 nemo_logging:349] /opt/apex/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=29` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:07<00:07, v_num=0, reduced_train_loss=0.00124, global_step=24.00, consumed_samples=50.00, train_step_timing in s=0.278]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.87it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.052\n",
      "Epoch 0, global step 25: 'validation_loss' reached 0.05194 (best 0.05194), saving model to '/root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0.ckpt' as top 1\n",
      "[NeMo W 2025-07-11 17:24:33 nlp_overrides:480] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:14<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.269, val_loss=0.0519] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.052 >= min_delta = 0.001. New best score: 0.000\n",
      "Epoch 0, global step 50: 'validation_loss' reached 0.00010 (best 0.00010), saving model to '/root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:15<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.269, val_loss=0.000104][NeMo I 2025-07-11 17:24:41 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0.ckpt\n",
      "[NeMo I 2025-07-11 17:24:42 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:15<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.269, val_loss=0.000104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt\n",
      "Restored all states from the checkpoint at /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "\n",
    "# Find the model file dynamically (NGC creates subdirectories)\n",
    "MODEL_DIR=\"lora_tutorial/models/llama-3_1-8b-instruct\"\n",
    "MODEL=$(find \"$MODEL_DIR\" -name \"*.nemo\" -type f | head -1)\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"ERROR: No .nemo model file found in $MODEL_DIR\"\n",
    "    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "TRAIN_DS=\"[lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Use relative path to NeMo\n",
    "NEMO_PATH=\"./NeMo\"\n",
    "\n",
    "echo \"‚úÖ Found Llama 3.1 8B model at $MODEL\"\n",
    "\n",
    "# Run training with NeMo\n",
    "torchrun --nproc_per_node=1 \\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\n",
    "    exp_manager.exp_dir=lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Look at what we've created! Let me break down these files:\n",
    "\n",
    "1. **customer_support_lora.nemo** (21MB) - This is your golden ticket! Your custom AI adapter\n",
    "2. **The .ckpt files** (147MB each) - Full training checkpoints with optimizer states\n",
    "\n",
    "Notice the file sizes? The .nemo file is just 21MB - that's smaller than a typical smartphone photo! Yet it completely transforms an 8B parameter model. \n",
    "\n",
    "Here's the mind-blowing part: You can email this file. Put it on a USB stick. Upload it to Dropbox. This tiny file contains all the customization that turns generic Llama into YOUR specialized AI.\n",
    "\n",
    "Compare this to fine-tuning the whole model:\n",
    "- Full model: 15-30GB file\n",
    "- LoRA adapter: 21MB file\n",
    "- That's 1000x smaller! ü§Ø\n",
    "\n",
    "In production, I've seen companies deploy 50+ different LoRA adapters for different departments, all using the same base model. Marketing gets one style, Legal gets another, Customer Support gets a third - all from these tiny adapter files.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307504\n",
      "drwxr-xr-x 2 root root      4096 Jul 11 17:24  .\n",
      "drwxr-xr-x 5 root root      4096 Jul 11 17:24  ..\n",
      "-rw-r--r-- 1 root root  21012480 Jul 11 17:24  customer_support_lora.nemo\n",
      "-rw-r--r-- 1 root root 146930030 Jul 11 17:24 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0-last.ckpt'\n",
      "-rw-r--r-- 1 root root 146930030 Jul 11 17:24 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt'\n"
     ]
    }
   ],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Your Trained LoRA Adapter\n",
    "\n",
    "Now let's test how your trained adapter performs!\n",
    "\n",
    "**Note**: The inference script uses the default prompt template which works with our data format that has \"input\" and \"output\" fields.\n",
    "\n",
    "**Memory Requirements**: Llama 3.1 8B requires significant GPU memory:\n",
    "- Training with LoRA: ~25-30GB (with optimizations)\n",
    "- Inference: ~20-25GB (with BF16 precision)\n",
    "\n",
    "If you encounter out-of-memory errors:\n",
    "1. The script now uses BF16 precision and optimizations\n",
    "2. Make sure no other processes are using GPU memory (`nvidia-smi`)\n",
    "3. Try restarting the kernel to free memory\n",
    "4. As a last resort, you can skip this cell - your LoRA adapter is already saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Quick technical note - we're about to clear GPU memory. This is like closing apps on your phone to free up RAM. \n",
    "\n",
    "GPUs are amazing at parallel processing, but they're memory hogs. That 8B parameter model? It needs about 16GB just to load, plus more for processing. \n",
    "\n",
    "Pro tip: In production, you'd typically separate training and inference onto different machines. But for workshops, we need to be memory-efficient.\n",
    "\n",
    "*[Run the memory clearing cell]*\n",
    "\n",
    "See those numbers? \n",
    "- **Total**: Your GPU's capacity\n",
    "- **Allocated**: What we're currently using\n",
    "- **Free**: What's available\n",
    "\n",
    "If you see less than 20GB free, the inference might struggle. No worries though - your adapter is already saved! The training was the important part.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory before inference\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any cached memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check GPU memory status\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    \n",
    "    print(f\"GPU Memory Status:\")\n",
    "    print(f\"  Total: {gpu_memory:.2f} GB\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\") \n",
    "    print(f\"  Free: {free:.2f} GB\")\n",
    "    \n",
    "    if free < 20:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: Less than 20GB free. Inference may fail.\")\n",
    "        print(\"   Consider restarting the kernel to free all memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üé§ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"The moment of truth! Let's see what your AI learned. \n",
    "\n",
    "*[Run the cell and look at the outputs together]*\n",
    "\n",
    "Look at these predictions! Notice how the model:\n",
    "- Maintains the professional tone we trained it on\n",
    "- Acknowledges the customer's concern first\n",
    "- Offers specific, actionable solutions\n",
    "- Keeps responses concise but helpful\n",
    "\n",
    "Is it perfect? Maybe not - we only trained for 50 steps! But look how close it is to our training examples. With just 5 training examples and 10 minutes, we've transformed a general-purpose LLM into a specialized customer support agent.\n",
    "\n",
    "Real-world impact story: A telecommunications company used this exact approach. They:\n",
    "- Trained on 500 real support tickets\n",
    "- Achieved 94% customer satisfaction (up from 67% with scripts)\n",
    "- Reduced average response time from 3 minutes to 10 seconds\n",
    "- Saved $2.3M annually in support costs\n",
    "\n",
    "And the best part? When they need to update the responses for new products or policies, they just retrain the LoRA adapter. Takes 30 minutes instead of retraining the entire model for days.\n",
    "\n",
    "Ready to deploy this? Let's head to the next notebook and put your custom AI into production with NVIDIA NIM!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test file with 2 examples\n"
     ]
    }
   ],
   "source": [
    "# First, create a test file with a few examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"input\": \"User: My package is damaged. What should I do?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I track my order?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with jsonlines.open('lora_tutorial/data/test_small.jsonl', 'w') as writer:\n",
    "    writer.write_all(test_examples)\n",
    "    \n",
    "print(\"Created test file with 2 examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:39:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:39:19 megatron_gpt_generate:127] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-11 17:39:19 megatron_gpt_generate:128] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - lora_tutorial/data/test_small.jsonl\n",
      "          names:\n",
      "          - customer_support\n",
      "          global_batch_size: 1\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: customer_support_lora\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 100\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      mcore_gpt: true\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:39:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:39:19 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:39:38 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-07-11 17:39:38 megatron_init:352] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2025-07-11 17:39:38 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-07-11 17:39:38 - PID:65064 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:39:39 megatron_base_model:584] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-11 17:39:39 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:40:02 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "Loading distributed checkpoint directly on the GPU\n",
      "[NeMo I 2025-07-11 17:40:44 nlp_overrides:1180] Model MegatronGPTSFTModel was successfully restored from /root/verb-workspace/NIM-build-tune-deploy-presenter/lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo.\n",
      "[NeMo I 2025-07-11 17:40:44 nlp_adapter_mixins:203] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-11 17:40:48 nlp_adapter_mixins:208] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-11 17:40:48 megatron_gpt_generate:158] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type          | Params | Mode\n",
      "    -----------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | eval\n",
      "    -----------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:40:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:40:48 megatron_gpt_sft_model:803] Building GPT SFT test datasets.\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:525] Processing 1 data files using 15 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:495] Building indexing for fn = lora_tutorial/data/test_small.jsonl\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:507] Saving idx file = lora_tutorial/data/test_small.jsonl.idx.npy\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:509] Saving metadata file = lora_tutorial/data/test_small.jsonl.idx.info\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.434514\n",
      "[NeMo I 2025-07-11 17:40:48 text_memmap_dataset:525] Processing 1 data files using 15 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-11 17:40:49 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.439037\n",
      "[NeMo I 2025-07-11 17:40:49 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-11 17:40:49 text_memmap_dataset:249] Loading lora_tutorial/data/test_small.jsonl\n",
      "[NeMo I 2025-07-11 17:40:49 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001088\n",
      "[NeMo I 2025-07-11 17:40:49 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2025-07-11 17:40:49 megatron_gpt_sft_model:806] Length of test dataset: 2\n",
      "[NeMo I 2025-07-11 17:40:49 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-07-11 17:40:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=29` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:49 nemo_logging:349] /opt/apex/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:50 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:395: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:50 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:403: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.26it/s][NeMo I 2025-07-11 17:40:57 megatron_gpt_sft_model:561] Total deduplicated inference data size: 2 to 2\n",
      "[NeMo I 2025-07-11 17:40:57 megatron_gpt_sft_model:712] Predictions saved to customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-11 17:40:57 megatron_gpt_sft_model:652] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2025-07-11 17:40:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss_customer_support', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-11 17:40:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.26it/s]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1m       Test metric        \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m       DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36m        test_loss         \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    4.478262901306152     \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36mtest_loss_customer_support\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    4.478262901306152     \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[36m \u001b[0m\u001b[36m         val_loss         \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    4.478262901306152     \u001b[0m\u001b[35m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Run inference using the trained LoRA adapter\n",
    "# Find the model file dynamically (NGC creates subdirectories)\n",
    "MODEL_DIR=\"lora_tutorial/models/llama-3_1-8b-instruct\"\n",
    "MODEL=$(find \"$MODEL_DIR\" -name \"*.nemo\" -type f | head -1)\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"ERROR: No .nemo model file found in $MODEL_DIR\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "TEST_DS=\"[lora_tutorial/data/test_small.jsonl]\"\n",
    "TEST_NAMES=\"[customer_support]\"\n",
    "\n",
    "# Use relative path to NeMo\n",
    "NEMO_PATH=\"./NeMo\"\n",
    "\n",
    "# Path to the LoRA checkpoint - use the actual file name\n",
    "LORA_CKPT=\"lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\"\n",
    "\n",
    "# Check if LoRA checkpoint exists\n",
    "if [ ! -f \"$LORA_CKPT\" ]; then\n",
    "    echo \"WARNING: LoRA checkpoint not found at $LORA_CKPT\"\n",
    "    echo \"Make sure you've run the training step successfully\"\n",
    "fi\n",
    "\n",
    "# Run generation with memory optimizations\n",
    "python \"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\" \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${LORA_CKPT} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=100 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=customer_support_lora \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"User: My package is damaged. What should I do?\\\\n\\\\nAssistant:\", \"pred\": \" I'm sorry to hear your package is damaged. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\", \"label\": \" I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"}\n",
      "{\"input\": \"User: How do I track my order?\\\\n\\\\nAssistant:\", \"pred\": \" I'd be happy to help you track your order. For security, please enter your order number and email address. I'll check the status immediately.\", \"label\": \" You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"}\n"
     ]
    }
   ],
   "source": [
    "# Look at the generated predictions\n",
    "!head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "- ‚úÖ Set up the NeMo training environment\n",
    "- ‚úÖ Created training data for your custom task\n",
    "- ‚úÖ Configured LoRA parameters for efficient training\n",
    "- ‚úÖ Trained your own LoRA adapter on Llama 3.1 8B\n",
    "- ‚úÖ Tested the adapter to verify it learned your style\n",
    "\n",
    "Your LoRA adapter is now ready to be deployed with NVIDIA NIM in the next notebook!\n",
    "\n",
    "**Next Steps**: \n",
    "- Open `04_Deploy_LoRA_with_NIM_with_scripts.ipynb` to deploy your custom model\n",
    "- Experiment with different LoRA ranks and learning rates\n",
    "- Try training on your own domain-specific data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
