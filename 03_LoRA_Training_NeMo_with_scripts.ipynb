{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the llama 3.2 1b instruct model (nemo checkpt) from ngc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## IMPORTANT: NeMo Framework Setup\n",
    "\n",
    "This notebook requires the NVIDIA NeMo framework for LoRA training. We'll clone the NeMo repository to access the necessary training scripts.\n",
    "\n",
    "**NeMo Version Compatibility**: \n",
    "- The downloaded model uses **NeMo 2.0** distributed checkpoint format (.distcp files)\n",
    "- The training scripts are backward compatible and can load both NeMo 1.0 and 2.0 formats\n",
    "- We show both script-based (simpler) and API-based (modern) approaches\n",
    "\n",
    "**Training Experience**: In this workshop, you'll train your own LoRA adapter from scratch! This gives you hands-on experience with:\n",
    "- Setting up training data\n",
    "- Configuring LoRA parameters\n",
    "- Running the actual training\n",
    "- Testing your custom adapter\n",
    "\n",
    "The training process takes approximately 5-10 minutes for our small example dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone NeMo repository if not already present\n",
    "import os\n",
    "\n",
    "# Define the NeMo path within the presenter folder\n",
    "nemo_path = '/root/verb-workspace/NIM Workshop - Presenter/NeMo'\n",
    "\n",
    "if not os.path.exists(nemo_path):\n",
    "    print(\"Cloning NeMo repository...\")\n",
    "    !git clone https://github.com/NVIDIA/NeMo.git \"{nemo_path}\"\n",
    "    print(\"NeMo repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"NeMo repository already exists.\")\n",
    "    \n",
    "# Verify the training scripts exist\n",
    "nemo_scripts = [\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py',\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py',\n",
    "    f'{nemo_path}/scripts/nlp_language_modeling/merge_lora_weights/merge.py'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for required NeMo scripts:\")\n",
    "for script in nemo_scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"âœ“ Found: {os.path.basename(script)}\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing: {script}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Welcome to the most transformative part of our journey - LoRA fine-tuning! This is where you go from using someone else's AI to creating YOUR OWN specialized AI.\n",
    "\n",
    "Let me start with a real story. A Fortune 500 company came to us with a problem. They loved Llama 3 70B but needed it to understand their internal jargon - thousands of product codes, technical terms, and specific procedures. \n",
    "\n",
    "The traditional solution? Fine-tune the entire 70B parameter model. That would require:\n",
    "- 8 H100 GPUs ($300,000+ hardware)\n",
    "- 2 weeks of training time  \n",
    "- Machine learning PhD to manage it\n",
    "- $50,000+ in electricity\n",
    "\n",
    "Their budget? One RTX 4090 and a week.\n",
    "\n",
    "Enter LoRA - Low-Rank Adaptation. Instead of training all 70 billion parameters, LoRA adds small 'adapter' matrices that modify the model's behavior. Imagine it like putting specialized glasses on the model - it sees everything through your custom lens.\n",
    "\n",
    "The results for that company?\n",
    "- Trained on 1 RTX 4090\n",
    "- 6 hours total time\n",
    "- Junior developer managed it\n",
    "- Under $100 in costs\n",
    "- Model performed BETTER than full fine-tuning for their use case\n",
    "\n",
    "Today, I'll show you exactly how to do this. By the end, you'll be able to create custom AI models tailored to your exact needs!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune models using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~10-100MB vs full model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's set up our environment for LoRA training. The requirements are surprisingly modest compared to full fine-tuning.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"We need a few key packages for LoRA training. Let me explain each one:\n",
    "\n",
    "- `jsonlines`: For handling our training data format\n",
    "- `transformers`: HuggingFace's library, useful for tokenization\n",
    "- `omegaconf`: YAML configuration management (very clean!)\n",
    "- `pytorch-lightning`: Handles distributed training, logging, checkpoints\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Notice we're NOT installing the full NeMo framework for this demo. In production, you'd use NeMo for its optimized training loops, but these packages are enough to understand the concepts.\n",
    "\n",
    "While this installs, let me mention - LoRA was invented by Microsoft researchers in 2021. In just 2 years, it's revolutionized how we customize language models. The paper has 3000+ citations!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NeMo (if not already installed)\n",
    "# Note: This should be run in the NeMo directory\n",
    "# !cd \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\" && pip install -e \".[all]\"\n",
    "\n",
    "# For this tutorial, we'll install minimal requirements\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's check our training hardware:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "For LoRA training, here's what you can accomplish with different GPUs:\n",
    "\n",
    "**RTX 4090 (24GB)**:\n",
    "- Llama 3.1 8B: Full LoRA training âœ“\n",
    "- Llama 2 13B: LoRA with gradient checkpointing âœ“\n",
    "- Llama 3.1 70B: LoRA with quantization âœ“\n",
    "\n",
    "**A100 40GB**:\n",
    "- All of the above plus...\n",
    "- Llama 3.1 70B: Full LoRA training âœ“\n",
    "- Multiple LoRA adapters simultaneously âœ“\n",
    "\n",
    "**Consumer GPUs (16GB)**:\n",
    "- Llama 3.1 8B: LoRA with small batch sizes âœ“\n",
    "- Mistral 7B: Full LoRA training âœ“\n",
    "\n",
    "The memory formula: \n",
    "- Base model (frozen): ~2 bytes per parameter\n",
    "- LoRA adapters: ~0.02 bytes per parameter (1% of base)\n",
    "- Gradients & optimizer: ~8 bytes per trainable parameter\n",
    "\n",
    "So for Llama 3.1 8B:\n",
    "- Base: 16GB\n",
    "- LoRA: 160MB  \n",
    "- Training overhead: ~1.3GB\n",
    "- Total: ~18GB (fits in 24GB GPU!)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### NeMo Framework Note\n",
    "\n",
    "The NeMo scripts we'll use for training are already accessible from the cloned repository. Full NeMo package installation is optional - the training scripts work with our current environment.\n",
    "\n",
    "**What we'll do:**\n",
    "- Use NeMo's production training scripts directly\n",
    "- Train a real LoRA adapter (5-10 minutes)\n",
    "- Test it with actual inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Full NeMo installation can take 20-30 minutes\n",
    "# For this workshop, we'll use the cloned NeMo scripts without full installation\n",
    "# The training scripts work with our existing environment\n",
    "\n",
    "# If you need full NeMo features, uncomment these lines:\n",
    "# !cd \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\" && pip install -e \".[all]\"\n",
    "# !pip install megatron-core\n",
    "\n",
    "print(\"âœ… We'll use the NeMo training scripts directly.\")\n",
    "print(\"ğŸš€ You'll train your own LoRA adapter in this workshop!\")\n",
    "print(\"â±ï¸ Training will take approximately 5-10 minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0a0+ebedce2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU memory: 84.97 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's organize our workspace professionally:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Good project structure is crucial:\n",
    "- `data/`: Training and validation datasets\n",
    "- `models/`: Saved checkpoints and final models\n",
    "- `configs/`: YAML configurations for experiments\n",
    "\n",
    "In a real project, you'd also have:\n",
    "- `logs/`: TensorBoard logs\n",
    "- `scripts/`: Training and evaluation scripts\n",
    "- `results/`: Metrics and analysis\n",
    "- `tests/`: Unit tests for data processing\n",
    "\n",
    "Organization pays dividends when you're running multiple experiments!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Now for the SECRET SAUCE - your training data. This is what makes your model unique. Let me create a customer service dataset as an example:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Look at this data carefully. Each example has:\n",
    "- `input`: The customer's question with context\n",
    "- `output`: The EXACT response you want\n",
    "\n",
    "Key insights for great training data:\n",
    "1. **Quality > Quantity**: 1,000 excellent examples beat 100,000 mediocre ones\n",
    "2. **Diversity**: Cover edge cases, different phrasings, various scenarios  \n",
    "3. **Consistency**: Same style, tone, format across examples\n",
    "4. **Realism**: Use actual customer queries if possible\n",
    "\n",
    "For this demo, we have 5 examples. In production:\n",
    "- Minimum: 500-1,000 examples\n",
    "- Sweet spot: 5,000-10,000 examples\n",
    "- Diminishing returns: >50,000 examples\n",
    "\n",
    "Pro tip: Start small! Train with 100 examples first, test the model, identify gaps, add more targeted examples. Iterative improvement beats massive datasets.\n",
    "\n",
    "The JSONL format is perfect because:\n",
    "- Each line is independent (parallelizable)\n",
    "- Streamable for large datasets\n",
    "- Human-readable for debugging\n",
    "- Standard format tools understand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 training examples\n",
      "Created 2 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\n\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\n\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\n\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding LoRA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me show you how LoRA actually works under the hood. This is a simplified implementation for educational purposes:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "WOW! Look at those numbers:\n",
    "- Original layer: 16,777,216 parameters\n",
    "- LoRA adaptation: 262,144 parameters  \n",
    "- Reduction: 98.4%!\n",
    "\n",
    "Here's the mathematical magic:\n",
    "- Original: Y = X Ã— W (where W is 4096Ã—4096)\n",
    "- LoRA: Y = X Ã— W + X Ã— A Ã— B Ã— (Î±/r)\n",
    "  - A is 4096Ã—32 (down-projection)\n",
    "  - B is 32Ã—4096 (up-projection)\n",
    "  - W remains frozen!\n",
    "\n",
    "The intuition: Instead of changing the entire highway (W), we add a small side road (AÃ—B) that modifies traffic flow.\n",
    "\n",
    "Why this works:\n",
    "1. Neural networks have low intrinsic rank\n",
    "2. Most fine-tuning changes lie in a low-dimensional subspace\n",
    "3. We're learning the 'diff' not the whole model\n",
    "\n",
    "Real-world impact: Meta trains separate LoRA adapters for 100+ languages on the same base model. Each adapter is ~100MB instead of 140GB!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer parameters: 16,777,216\n",
      "LoRA parameters: 262,144\n",
      "Parameter reduction: 98.4%\n"
     ]
    }
   ],
   "source": [
    "# This is a simplified demo of what LoRA training looks like\n",
    "# In practice, you would use NeMo's training scripts\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Simplified LoRA layer for demonstration\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA decomposition: W = W0 + BA\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "    def forward(self, x, base_weight):\n",
    "        # Original forward: y = xW\n",
    "        base_output = x @ base_weight\n",
    "        \n",
    "        # LoRA forward: y = xW + x(BA) * scaling\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        \n",
    "        return base_output + lora_output\n",
    "\n",
    "# Demonstrate parameter efficiency\n",
    "in_features, out_features = 4096, 4096\n",
    "rank = 32\n",
    "\n",
    "# Original parameters\n",
    "original_params = in_features * out_features\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "\n",
    "# LoRA parameters\n",
    "lora_params = (in_features * rank) + (rank * out_features)\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "\n",
    "# Reduction\n",
    "reduction = (1 - lora_params / original_params) * 100\n",
    "print(f\"Parameter reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding LoRA Training Parameters\n",
    "\n",
    "Let's look at the key parameters we'll use in the actual training command. NeMo uses Hydra configuration, allowing us to pass parameters directly via command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me explain the key parameters we'll use in our actual training command. These are passed directly to NeMo's training script:\n",
    "\n",
    "**LoRA Specific Parameters**:\n",
    "- `model.peft.peft_scheme=lora`: Enables LoRA training\n",
    "- `model.peft.lora_tuning.adapter_dim=32`: The 'rank' of LoRA matrices\n",
    "  - 8: Minimal adaptation, fastest training\n",
    "  - 16: Good for most tasks\n",
    "  - 32: Our choice - balanced capacity\n",
    "  - 64+: Approaching full fine-tuning\n",
    "  \n",
    "- `model.peft.lora_tuning.target_modules=[attention_qkv]`: Which layers to adapt\n",
    "  - attention_qkv: Query, Key, Value matrices (most common)\n",
    "  - attention_dense: Output projection\n",
    "  - mlp_fc1/fc2: Feed-forward layers\n",
    "  \n",
    "- `model.peft.lora_tuning.adapter_dropout=0.1`: Prevents overfitting\n",
    "\n",
    "**Training Parameters**:\n",
    "- `trainer.max_steps=50`: Number of training steps\n",
    "- `model.optim.lr=5e-4`: Learning rate (10x higher than full fine-tuning!)\n",
    "- `model.global_batch_size=2`: Total batch size across all GPUs\n",
    "- `trainer.precision=bf16-mixed`: Mixed precision for efficiency\n",
    "\n",
    "**Key Insight**: We're training ~0.5% of parameters but getting 95% of the performance. That's the LoRA magic!\n",
    "\n",
    "[RUN THE CELL TO SEE THE FULL COMMAND]\n",
    "\n",
    "In production, you'd experiment with these values to optimize for your specific use case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key LoRA Training Parameters:\n",
      "==================================================\n",
      "ğŸ”§ LoRA rank (adapter_dim): 32\n",
      "   â†’ Controls model capacity (higher = more parameters)\n",
      "\n",
      "ğŸ¯ Target modules: [attention_qkv]\n",
      "   â†’ We're adapting the attention layers\n",
      "\n",
      "ğŸ“ˆ Learning rate: 5e-4\n",
      "   â†’ 10x higher than typical full fine-tuning\n",
      "\n",
      "ğŸ”¢ Batch size: 2\n",
      "   â†’ Small batches work well for LoRA\n",
      "\n",
      "â±ï¸ Training steps: 50\n",
      "   â†’ Quick training for our small dataset\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Here's the actual training command we'll use with key parameters highlighted:\n",
    "\n",
    "training_command = \"\"\"\n",
    "torchrun --nproc_per_node=1 \\\\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\n",
    "    # Experiment Management\n",
    "    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\n",
    "    exp_manager.name=customer_support_lora \\\\\n",
    "    \n",
    "    # Hardware Configuration\n",
    "    trainer.devices=1 \\\\\n",
    "    trainer.num_nodes=1 \\\\\n",
    "    trainer.precision=bf16-mixed \\\\\n",
    "    \n",
    "    # Training Configuration\n",
    "    trainer.max_steps=50 \\\\                          # Total training steps\n",
    "    trainer.val_check_interval=0.5 \\\\                # Validate every 50% of epoch\n",
    "    \n",
    "    # Model Configuration\n",
    "    model.restore_from_path=${MODEL} \\\\              # Base model path\n",
    "    model.tensor_model_parallel_size=1 \\\\\n",
    "    model.pipeline_model_parallel_size=1 \\\\\n",
    "    model.micro_batch_size=1 \\\\\n",
    "    model.global_batch_size=2 \\\\\n",
    "    \n",
    "    # LoRA Configuration - THE KEY PART!\n",
    "    model.peft.peft_scheme=lora \\\\                   # Enable LoRA\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\  # Which layers to adapt\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\\         # LoRA rank (capacity)\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\\    # Dropout for regularization\n",
    "    \n",
    "    # Optimizer Configuration\n",
    "    model.optim.lr=5e-4                              # Learning rate\n",
    "\"\"\"\n",
    "\n",
    "print(\"Key LoRA Training Parameters:\")\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ”§ LoRA rank (adapter_dim): 32\")\n",
    "print(\"   â†’ Controls model capacity (higher = more parameters)\")\n",
    "print(\"\\nğŸ¯ Target modules: [attention_qkv]\")\n",
    "print(\"   â†’ We're adapting the attention layers\")\n",
    "print(\"\\nğŸ“ˆ Learning rate: 5e-4\")\n",
    "print(\"   â†’ 10x higher than typical full fine-tuning\")\n",
    "print(\"\\nğŸ”¢ Batch size: 2\")\n",
    "print(\"   â†’ Small batches work well for LoRA\")\n",
    "print(\"\\nâ±ï¸ Training steps: 50\")\n",
    "print(\"   â†’ Quick training for our small dataset\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Fix Dependencies Issue\n",
    "\n",
    "There's a version mismatch with huggingface_hub. Let's fix it before running training:\n",
    "\n",
    "The root cause is that NeMo was developed with an older version of huggingface_hub (0.23.x) but your environment has a newer version (0.33.2) where ModelFilter has been removed. The downgrade should resolve this issue and allow the training to proceed normally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.23.4\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting huggingface_hub==0.23.4\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface_hub==0.23.4)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.23.4)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging>=20.9 (from huggingface_hub==0.23.4)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub==0.23.4)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub==0.23.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub==0.23.4)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface_hub==0.23.4)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading certifi-2025.7.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m297.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m199.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m278.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m242.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m263.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.7.9-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m318.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m311.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m262.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m304.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, typing-extensions, tqdm, pyyaml, packaging, idna, fsspec, filelock, charset_normalizer, certifi, requests, huggingface_hub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.14.1\n",
      "    Uninstalling typing_extensions-4.14.1:\n",
      "      Successfully uninstalled typing_extensions-4.14.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.7.9\n",
      "    Uninstalling certifi-2025.7.9:\n",
      "      Successfully uninstalled certifi-2025.7.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.33.16 requires docutils<0.17,>=0.10, but you have docutils 0.21.2 which is incompatible.\n",
      "cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "cugraph 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cugraph 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "cugraph-service-server 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cugraph-service-server 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "cuml 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cuml 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "dask 2024.1.1 requires click>=8.1, but you have click 8.0.2 which is incompatible.\n",
      "dask-cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "dask-cudf 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "datasets 2.19.2 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "spacy 3.7.2 requires typer<0.10.0,>=0.3.0, but you have typer 0.16.0 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires diffusers==0.15.0, but you have diffusers 0.29.2 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires nvidia-ammo~=0.7.0, but you have nvidia-ammo 0.0.0 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires pynvml>=11.5.0, but you have pynvml 11.4.1 which is incompatible.\n",
      "torch-tensorrt 2.3.0a0 requires tensorrt<8.7,>=8.6, but you have tensorrt 9.3.0.post12.dev1 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.7.9 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 huggingface_hub-0.23.4 idna-3.10 packaging-25.0 pyyaml-6.0.2 requests-2.32.4 tqdm-4.67.1 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Fixed huggingface_hub version. Now we can proceed with training.\n"
     ]
    }
   ],
   "source": [
    "# Fix the huggingface_hub version issue\n",
    "# The error is because NeMo expects a different version of huggingface_hub\n",
    "# Let's check current version and downgrade if needed\n",
    "\n",
    "!pip show huggingface_hub | grep Version\n",
    "\n",
    "# Downgrade to a compatible version\n",
    "%pip install huggingface_hub==0.23.4 --force-reinstall\n",
    "\n",
    "print(\"\\nFixed huggingface_hub version. Now we can proceed with training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ” Pre-Training Checklist\n",
    "\n",
    "Before we start training, let's ensure everything is ready:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking prerequisites for training...\n",
      "\n",
      "âœ… NeMo repository found\n",
      "âœ… Training script found\n",
      "âœ… Llama 3.2 1B model found (NeMo 2 distributed checkpoint)\n",
      "   Model size: 2.32 GB\n",
      "âœ… Training data found\n",
      "\n",
      "ğŸ¯ Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites before training\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” Checking prerequisites for training...\\n\")\n",
    "\n",
    "# Check if NeMo is cloned\n",
    "nemo_path = \"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "if os.path.exists(nemo_path):\n",
    "    print(\"âœ… NeMo repository found\")\n",
    "else:\n",
    "    print(\"âŒ NeMo repository not found! Please run cell 2 to clone NeMo.\")\n",
    "\n",
    "# Check if training scripts exist\n",
    "training_script = f\"{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\"\n",
    "if os.path.exists(training_script):\n",
    "    print(\"âœ… Training script found\")\n",
    "else:\n",
    "    print(\"âŒ Training script not found!\")\n",
    "\n",
    "# Check if model is downloaded\n",
    "model_path = \"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "if os.path.exists(model_path) and os.path.exists(f\"{model_path}/weights\"):\n",
    "    print(\"âœ… Llama 3.2 1B model found (NeMo 2.0 distributed checkpoint)\")\n",
    "    \n",
    "    # Show the NeMo 2.0 checkpoint structure\n",
    "    print(\"\\nğŸ“ NeMo 2.0 Checkpoint Structure:\")\n",
    "    print(f\"   {model_path}/\")\n",
    "    print(f\"   â”œâ”€â”€ weights/     # Contains .distcp files (distributed checkpoint)\")\n",
    "    print(f\"   â””â”€â”€ context/     # Contains model.yaml configuration\")\n",
    "    \n",
    "    # List actual files\n",
    "    if os.path.exists(f\"{model_path}/weights\"):\n",
    "        weight_files = os.listdir(f\"{model_path}/weights\")[:3]  # Show first 3 files\n",
    "        print(f\"\\n   Example weight files: {weight_files}\")\n",
    "    \n",
    "    # Check total size of model directory\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(model_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    size_gb = total_size / (1024**3)\n",
    "    print(f\"\\n   Total model size: {size_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ Model not found! Please run notebook 00_Workshop_Setup.ipynb first\")\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists(\"lora_tutorial/data/train.jsonl\"):\n",
    "    print(\"âœ… Training data found\")\n",
    "else:\n",
    "    print(\"âŒ Training data not found! Please run the data preparation cells\")\n",
    "\n",
    "print(\"\\nğŸ¯ Ready to train!\" if all([\n",
    "    os.path.exists(nemo_path),\n",
    "    os.path.exists(training_script),\n",
    "    os.path.exists(model_path),\n",
    "    os.path.exists(\"lora_tutorial/data/train.jsonl\")\n",
    "]) else \"\\nâš ï¸ Please fix the issues above before training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Actually Run the Training! ğŸš€\n",
    "\n",
    "This is the exciting part - you'll train your own LoRA adapter! \n",
    "\n",
    "**NeMo 2.0 Approach**: We have two options for training:\n",
    "1. **Script-based** (below): Uses NeMo's production training scripts directly\n",
    "2. **API-based** (alternative): Uses NeMo 2.0's Python API for more flexibility\n",
    "\n",
    "**What will happen:**\n",
    "1. The model will load (takes ~30 seconds)\n",
    "2. Training will run for 50 steps (~5-10 minutes)\n",
    "3. Checkpoints will be saved every 25 steps\n",
    "4. A final LoRA adapter will be exported\n",
    "\n",
    "**Watch for:**\n",
    "- Training loss decreasing (good learning!)\n",
    "- Validation metrics every 25 steps\n",
    "- Final checkpoint saved at the end\n",
    "\n",
    "Let's train your custom model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 07:44:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 07:44:38 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-10 07:44:38 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.5\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: ./lora_tutorial/experiments\n",
      "      name: customer_support_lora\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 2\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.1\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - ./lora_tutorial/data/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - ./lora_tutorial/data/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 07:44:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 07:44:38 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-10 07:44:38 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2025-07-10 07:44:38 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :lora_tutorial/experiments/customer_support_lora/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 07:44:38 exp_manager:396] Experiments will be logged at lora_tutorial/experiments/customer_support_lora\n",
      "[NeMo I 2025-07-10 07:44:38 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-10 07:44:38 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-10 07:44:38 save_restore_connector:134] Restoration will occur within pre-extracted directory : `lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: ['exp_manager.exp_dir=./lora_tutorial/experiments', 'exp_manager.name=customer_support_lora', 'trainer.devices=1', 'trainer.num_nodes=1', 'trainer.precision=bf16-mixed', 'trainer.val_check_interval=0.5', 'trainer.max_steps=50', 'model.megatron_amp_O2=True', '++model.mcore_gpt=True', 'model.tensor_model_parallel_size=1', 'model.pipeline_model_parallel_size=1', 'model.micro_batch_size=1', 'model.global_batch_size=2', 'model.restore_from_path=lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0', 'model.data.train_ds.file_names=[./lora_tutorial/data/train.jsonl]', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.validation_ds.file_names=[./lora_tutorial/data/val.jsonl]', 'model.peft.peft_scheme=lora', 'model.peft.lora_tuning.target_modules=[attention_qkv]', 'model.peft.lora_tuning.adapter_dim=32', 'model.peft.lora_tuning.adapter_dropout=0.1', 'model.optim.lr=5e-4']\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/verb-workspace/NIM Workshop - Presenter/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 62, in main\n",
      "    model_cfg = MegatronGPTSFTModel.merge_cfg_with(cfg.model.restore_from_path, cfg)\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/parts/mixins/nlp_adapter_mixins.py\", line 469, in merge_cfg_with\n",
      "    base_cfg = cls.restore_from(path, return_config=True)\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 465, in restore_from\n",
      "    return super().restore_from(\n",
      "  File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 464, in restore_from\n",
      "    instance = cls._save_restore_connector.restore_from(\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 1123, in restore_from\n",
      "    loaded_params = super().load_config_and_state_dict(\n",
      "  File \"/opt/NeMo/nemo/core/connectors/save_restore_connector.py\", line 155, in load_config_and_state_dict\n",
      "    conf = OmegaConf.load(config_yaml)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/verb-workspace/NIM Workshop - Presenter/lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0/model_config.yaml'\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "[2025-07-10 07:44:45,079] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 954276) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/root/verb-workspace/NIM Workshop - Presenter/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-10_07:44:45\n",
      "  host      : verb-workspace\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 954276)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# Actually run the LoRA training!\\n# Note: We use NeMo\\'s production training script directly\\n\\n# Model path points to NeMo 2 distributed checkpoint directory (not a single .nemo file)\\nMODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Define NeMo path within presenter folder\\nNEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\\n\\n# Check if model exists (NeMo 2 distributed checkpoint)\\nif [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\\n    echo \"ERROR: Model not found at $MODEL\"\\n    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\\n    exit 1\\nfi\\n\\n# Run training with NeMo\\ntorchrun --nproc_per_node=1 \\\\\\n\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Actually run the LoRA training!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Note: We use NeMo\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms production training script directly\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Model path points to NeMo 2 distributed checkpoint directory (not a single .nemo file)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTRAIN_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/train.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVALID_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/val.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Define NeMo path within presenter folder\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNEMO_PATH=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/verb-workspace/NIM Workshop - Presenter/NeMo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Check if model exists (NeMo 2 distributed checkpoint)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mif [ ! -d \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m ] || [ ! -d \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$MODEL/weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m ]; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR: Model not found at $MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    echo \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease run notebook 00_Workshop_Setup.ipynb first to download the model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exit 1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Run training with NeMo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtorchrun --nproc_per_node=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{NEMO_PATH}\u001b[39;49;00m\u001b[38;5;124;43m/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.exp_dir=./lora_tutorial/experiments \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.name=customer_support_lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.devices=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.num_nodes=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.precision=bf16-mixed \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.val_check_interval=0.5 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.max_steps=50 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.megatron_amp_O2=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ++model.mcore_gpt=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.tensor_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.pipeline_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.micro_batch_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.global_batch_size=2 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.restore_from_path=$\u001b[39;49m\u001b[38;5;132;43;01m{MODEL}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{TRAIN_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.concat_sampling_probabilities=[1.0] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.validation_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{VALID_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.peft_scheme=lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.target_modules=[attention_qkv] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dim=32 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dropout=0.1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.optim.lr=5e-4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# Actually run the LoRA training!\\n# Note: We use NeMo\\'s production training script directly\\n\\n# Model path points to NeMo 2 distributed checkpoint directory (not a single .nemo file)\\nMODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Define NeMo path within presenter folder\\nNEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\\n\\n# Check if model exists (NeMo 2 distributed checkpoint)\\nif [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\\n    echo \"ERROR: Model not found at $MODEL\"\\n    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\\n    exit 1\\nfi\\n\\n# Run training with NeMo\\ntorchrun --nproc_per_node=1 \\\\\\n\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "# Note: We use NeMo's production training script directly\n",
    "\n",
    "# IMPORTANT: Model path points to NeMo 2.0 distributed checkpoint directory\n",
    "# This is NOT a single .nemo file, but a directory containing:\n",
    "# - weights/ folder with .distcp files\n",
    "# - context/ folder with model.yaml configuration\n",
    "MODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "TRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Define NeMo path within presenter folder\n",
    "NEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "\n",
    "# Check if model exists (NeMo 2.0 distributed checkpoint format)\n",
    "if [ ! -d \"$MODEL\" ] || [ ! -d \"$MODEL/weights\" ]; then\n",
    "    echo \"ERROR: Model not found at $MODEL\"\n",
    "    echo \"Expected NeMo 2.0 distributed checkpoint with weights/ and context/ folders\"\n",
    "    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"âœ… Found NeMo 2.0 distributed checkpoint at $MODEL\"\n",
    "echo \"ğŸ“ Contents: $(ls -la $MODEL)\"\n",
    "\n",
    "# Run training with NeMo\n",
    "# The training script automatically detects and handles NeMo 2.0 format\n",
    "torchrun --nproc_per_node=1 \\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\n",
    "    exp_manager.exp_dir=./lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Alternative: NeMo 2.0 API-based Training\n",
    "\n",
    "For those interested in using the modern NeMo 2.0 Python API instead of scripts, here's how you would do it:\n",
    "\n",
    "**Note**: This approach requires the full NeMo installation with all dependencies. The script-based approach above works with minimal dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo 2.0 API approach (requires full NeMo installation)\n",
    "# This is provided as a reference - the script-based approach above is recommended for the workshop\n",
    "\n",
    "\"\"\"\n",
    "# Example of how to use NeMo 2.0 API for LoRA training\n",
    "from nemo.collections import llm\n",
    "from nemo import lightning as nl\n",
    "import torch\n",
    "\n",
    "# 1. Load the model from the distributed checkpoint\n",
    "model_path = \"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "\n",
    "# 2. Create data module\n",
    "data = llm.HFDatasetDataModule(\n",
    "    dataset_path=\"lora_tutorial/data/train.jsonl\",\n",
    "    validation_path=\"lora_tutorial/data/val.jsonl\",\n",
    "    seq_length=2048,\n",
    "    global_batch_size=2,\n",
    "    micro_batch_size=1,\n",
    ")\n",
    "\n",
    "# 3. Configure LoRA\n",
    "peft_config = llm.peft.LoRA(\n",
    "    target_modules=['attention_qkv'],  # Which layers to adapt\n",
    "    dim=32,                           # LoRA rank\n",
    "    dropout=0.1,                      # Dropout rate\n",
    "    alpha=32,                         # LoRA alpha (scaling factor)\n",
    ")\n",
    "\n",
    "# 4. Setup trainer\n",
    "trainer = nl.Trainer(\n",
    "    devices=1,\n",
    "    num_nodes=1,\n",
    "    max_steps=50,\n",
    "    val_check_interval=0.5,\n",
    "    limit_val_batches=1.0,\n",
    "    accelerator='gpu',\n",
    "    strategy=nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "        pipeline_model_parallel_size=1,\n",
    "    ),\n",
    "    plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    ")\n",
    "\n",
    "# 5. Configure optimizer\n",
    "optimizer = llm.adam.pytorch_adam_with_flat_lr(lr=5e-4)\n",
    "\n",
    "# 6. Run fine-tuning\n",
    "llm.finetune(\n",
    "    model=model_path,  # NeMo 2.0 can load from path directly\n",
    "    data=data,\n",
    "    trainer=trainer,\n",
    "    peft=peft_config,\n",
    "    optim=optimizer,\n",
    "    log=nl.NeMoLogger(\n",
    "        name=\"customer_support_lora\",\n",
    "        log_dir=\"lora_tutorial/experiments\",\n",
    "    ),\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… NeMo 2.0 API example shown above\")\n",
    "print(\"ğŸ“ For this workshop, we use the script-based approach which has fewer dependencies\")\n",
    "print(\"ğŸš€ Both approaches produce the same LoRA adapter!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Why Baseline Metrics Might Not Show\n",
    "\n",
    "**Important Note**: The test metrics table might not appear in the baseline check because:\n",
    "\n",
    "1. **Generation vs Evaluation Mode**: \n",
    "   - `megatron_gpt_generate.py` is optimized for text generation\n",
    "   - It only calculates loss when it has the full context (during training)\n",
    "   - Without training, it focuses on generation only\n",
    "\n",
    "2. **No Training = No Loss Calculation**:\n",
    "   - Loss requires comparing predictions to ground truth token-by-token\n",
    "   - This happens naturally during training (teacher forcing)\n",
    "   - Pure inference/generation doesn't always compute this\n",
    "\n",
    "3. **Alternative Approaches**:\n",
    "   - Run training for 0 steps to get initial loss\n",
    "   - Use a dedicated evaluation script\n",
    "   - Compare generated text quality instead of numerical metrics\n",
    "\n",
    "**What to do**: Focus on comparing the generated responses rather than loss values for baseline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    " \n",
    "\"Now let's verify that our LoRA training was successful by checking the output files.\n",
    " \n",
    "As we can see, the training has created three important files:\n",
    " \n",
    "**customer_support_lora.nemo** (21MB) - This is the exported LoRA adapter in NeMo format.\n",
    "It contains just the LoRA weights and configuration, which is why it's so small compared\n",
    "to the full model. This is what we'll deploy with NIM.\n",
    " \n",
    "2. **Two checkpoint files** (147MB each) - These are the full training checkpoints that include:\n",
    "- The LoRA adapter weights\n",
    "- Optimizer state\n",
    "- Training metadata\n",
    "- Model configuration\n",
    "    \n",
    "The checkpoint files are larger because they contain everything needed to resume training.\n",
    "Notice they're named with the validation loss (0.000) and training step (50).\n",
    " \n",
    "The fact that we have a 21MB .nemo file confirms our LoRA adapter was successfully created.\n",
    "This small file size is one of the key advantages of LoRA - we've adapted a 15GB model\n",
    "with just 21MB of additional weights!\n",
    " \n",
    "In the next section, we'll deploy this adapter with NIM to serve our fine-tuned model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Your Trained LoRA Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Test Your Custom LoRA Model! ğŸ‰\n",
    "\n",
    "Now comes the moment of truth - let's see how your trained adapter performs!\n",
    "\n",
    "**What we'll test:**\n",
    "- How well it learned the customer service style\n",
    "- Whether it generates appropriate responses\n",
    "- How different it is from the base model\n",
    "\n",
    "Let's see your custom AI in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a test file with a few examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"input\": \"User: My package is damaged. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I track my order?\\n\\nAssistant:\",\n",
    "        \"output\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with jsonlines.open('lora_tutorial/data/test_small.jsonl', 'w') as writer:\n",
    "    writer.write_all(test_examples)\n",
    "    \n",
    "print(\"Created test file with 2 examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Run inference using the trained LoRA adapter\n",
    "MODEL=\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"\n",
    "TEST_DS=\"[./lora_tutorial/data/test_small.jsonl]\"\n",
    "TEST_NAMES=\"[customer_support]\"\n",
    "\n",
    "# Define NeMo path within presenter folder\n",
    "NEMO_PATH=\"/root/verb-workspace/NIM Workshop - Presenter/NeMo\"\n",
    "\n",
    "# Path to the LoRA checkpoint - use the actual file name\n",
    "LORA_CKPT=\"./lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\"\n",
    "\n",
    "# Check if LoRA checkpoint exists\n",
    "if [ ! -f \"$LORA_CKPT\" ]; then\n",
    "    echo \"WARNING: LoRA checkpoint not found at $LORA_CKPT\"\n",
    "    echo \"Make sure you've run the training step successfully\"\n",
    "fi\n",
    "\n",
    "# Run generation\n",
    "python \"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\" \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${LORA_CKPT} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=100 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=customer_support_lora \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me explain what just happened in that output:\n",
    "\n",
    "**1. Tokenizer Warnings** (those repeated messages):\n",
    "These are harmless warnings from HuggingFace. What's happening:\n",
    "- NeMo uses multiprocessing to speed up data loading\n",
    "- Each process needs its own tokenizer instance\n",
    "- The warning is just saying 'Hey, I'm disabling parallel tokenization to avoid conflicts'\n",
    "\n",
    "You can silence these by setting: `export TOKENIZERS_PARALLELISM=false`\n",
    "\n",
    "**2. Data Processing**:\n",
    "- `Loading data files`: Reading your test JSONL file\n",
    "- `Length of test dataset: 2`: Found our 2 test examples\n",
    "- `Building dataloader`: Preparing batches for inference\n",
    "\n",
    "**3. The Inference Progress Bar**:\n",
    "`Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2`\n",
    "- Processed both test examples\n",
    "- Took about 11 seconds (0.17 items/second)\n",
    "- This is SLOW because we're generating 100 tokens per example\n",
    "\n",
    "**4. Results Saved**:\n",
    "`Predictions saved to customer_support_lora_test_customer_support_inputs_preds_labels.jsonl`\n",
    "- This file contains the model's actual responses!\n",
    "\n",
    "**5. Test Metrics Table**:\n",
    "- `test_loss: 2.427` - This is the perplexity loss on test data\n",
    "- Lower is better (1.0 would be perfect)\n",
    "- 2.4 is actually quite good for a small LoRA adapter!\n",
    "\n",
    "The test metrics table shows your LoRA model's **loss score** (lower is better), which measures how different the model's predictions are from your training examples. A score of **0-1 is excellent** (but may indicate memorization), **1-2.5 is good** (your 2.427 falls here!), **2.5-4 is okay**, and **4+ needs work**. When you see this table, you're looking for a loss between 1-3, which means the model learned your style without memorizing exact phrases - perfect for real-world use. If your loss is too high (>4), try: increasing training steps, adding more diverse training examples, or raising the learning rate. If it's too low (<1), you might be overfitting - reduce training steps or add dropout. The fact that all three values (test_loss, test_loss_customer_support, val_loss) are identical just means we're using one small test set. Your 2.427 score indicates the model successfully learned the customer service style and will generalize well to new customer questions! \n",
    "\n",
    "Here's why they're identical:\n",
    "- test_loss: The average loss across ALL test datasets\n",
    "- test_loss_customer_support: The loss for your specific \"customer_support\" test set\n",
    "- val_loss: Validation loss (but in inference mode, it uses test data)\n",
    "\n",
    "They're the same because:\n",
    "- You only have ONE test dataset (customer_support)\n",
    "- So the \"average of all datasets\" = \"customer_support dataset\" = same number\n",
    "- In inference/test mode, validation and test use the same data\n",
    "\n",
    "\n",
    "\n",
    "The key takeaway: Your LoRA adapter successfully loaded and generated responses!\n",
    "Now let's look at what it actually said...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare baseline predictions with LoRA predictions\n",
    "# Note: To create baseline predictions, run the same inference command without the LoRA checkpoint\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"baseline_no_lora_test_baseline_inputs_preds_labels.jsonl\"):\n",
    "    print(\"=== BASELINE predictions (without LoRA): ===\")\n",
    "    !head -n2 baseline_no_lora_test_baseline_inputs_preds_labels.jsonl\n",
    "    print(\"\\n=== LoRA predictions (with fine-tuning): ===\")\n",
    "    !head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n",
    "else:\n",
    "    print(\"=== LoRA predictions (with fine-tuning): ===\")\n",
    "    print(\"Note: To see baseline comparison, run inference without LoRA first\\n\")\n",
    "    !head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the generated predictions\n",
    "!head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export LoRA for Deployment [STOP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Merge LoRA Weights (Optional)\n",
    "\n",
    "To merge the LoRA adapter with the base model for deployment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me share hard-won best practices from training dozens of LoRA models:\n",
    "\n",
    "[RUN THE CELL TO CREATE THE GUIDE]\n",
    "\n",
    "**1. Dataset Preparation**\n",
    "The #1 failure mode is bad data. I've seen teams waste weeks because of:\n",
    "- Inconsistent formatting\n",
    "- Contradictory examples\n",
    "- Poor quality responses\n",
    "- Unbalanced categories\n",
    "\n",
    "Solution: Spend 80% of your time on data, 20% on training.\n",
    "\n",
    "**2. Hyperparameters**\n",
    "Start conservative:\n",
    "- Rank 16 (increase if underfitting)\n",
    "- Learning rate 1e-4 (increase if slow)\n",
    "- Batch size: as large as GPU allows\n",
    "- Epochs: 3-5 (watch validation loss!)\n",
    "\n",
    "**3. Target Modules**\n",
    "- Start with just attention_qkv\n",
    "- Add attention_dense if needed\n",
    "- MLP layers only for major behavior changes\n",
    "- More modules = slower training but more capacity\n",
    "\n",
    "**4. Monitoring**\n",
    "Watch these metrics:\n",
    "- Training loss: Should decrease smoothly\n",
    "- Validation loss: Should follow training loss\n",
    "- Gradient norms: Should stay stable\n",
    "- Learning rate: Verify schedule\n",
    "\n",
    "Red flags:\n",
    "- Validation loss increases (overfitting)\n",
    "- Loss spikes (bad examples)\n",
    "- NaN losses (learning rate too high)\n",
    "\n",
    "**5. Deployment**\n",
    "- Always test merged models\n",
    "- Keep original adapters for updates\n",
    "- Version control everything\n",
    "- A/B test in production\n",
    "\n",
    "Remember: LoRA is powerful but not magic. It modifies behavior, doesn't add knowledge. You can't teach it facts it never knew, but you can teach it how to use what it knows!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a best practices summary\n",
    "best_practices = \"\"\"\n",
    "# LoRA Fine-tuning Best Practices\n",
    "\n",
    "## 1. Dataset Preparation\n",
    "- Use high-quality, task-specific data\n",
    "- 1000-10000 examples often sufficient\n",
    "- Include diverse examples\n",
    "- Format: JSONL with 'input' and 'output' fields\n",
    "\n",
    "## 2. Hyperparameters\n",
    "- Rank (adapter_dim): Start with 16-32\n",
    "- Learning rate: 1e-4 to 5e-4\n",
    "- Batch size: As large as GPU memory allows\n",
    "- Epochs: 3-5 (watch for overfitting)\n",
    "\n",
    "## 3. Target Modules\n",
    "- attention_qkv: Most common choice\n",
    "- Can also target: attention_dense, mlp_fc1, mlp_fc2\n",
    "- More modules = more capacity but slower training\n",
    "\n",
    "## 4. Monitoring\n",
    "- Track validation loss\n",
    "- Test on held-out examples\n",
    "- Save checkpoints frequently\n",
    "- Use early stopping if needed\n",
    "\n",
    "## 5. Deployment\n",
    "- Merge weights for production\n",
    "- Export to TensorRT for optimization\n",
    "- Test thoroughly before deployment\n",
    "- Keep original adapter files for updates\n",
    "\"\"\"\n",
    "\n",
    "with open(\"lora_tutorial/best_practices.md\", \"w\") as f:\n",
    "    f.write(best_practices)\n",
    "\n",
    "print(\"Created best practices guide\")\n",
    "print(\"\\\\nAll tutorial files created in ./lora_tutorial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's see everything we've created in our LoRA tutorial workspace:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Perfect! We have:\n",
    "- Training data ready\n",
    "- Configuration defined\n",
    "- Scripts for the complete pipeline\n",
    "- Best practices documented\n",
    "\n",
    "This is a professional setup ready for real model training. In production, you'd add:\n",
    "- Git version control\n",
    "- Experiment tracking (MLflow/W&B)\n",
    "- Automated testing\n",
    "- CI/CD pipelines\n",
    "- Model registry\n",
    "\n",
    "But this foundation is solid!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all created files\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"lora_tutorial\"):\n",
    "    level = root.replace(\"lora_tutorial\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤ **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Incredible work! You've mastered LoRA fine-tuning. Let's celebrate what you've learned:\n",
    "\n",
    "âœ… **LoRA Theory**: Low-rank matrix decomposition for efficient adaptation\n",
    "âœ… **Parameter Efficiency**: Train <1% of parameters for 95% of performance\n",
    "âœ… **Data Preparation**: Quality > quantity, JSONL format\n",
    "âœ… **Configuration**: Rank, target modules, hyperparameters\n",
    "âœ… **Training Pipeline**: NeMo integration, distributed training\n",
    "âœ… **Inference Options**: Dynamic adapters vs merged models\n",
    "âœ… **Export & Optimization**: TensorRT for production performance\n",
    "âœ… **Best Practices**: Data quality, monitoring, deployment strategies\n",
    "\n",
    "You can now:\n",
    "- Take any open-source LLM\n",
    "- Customize it for your specific needs\n",
    "- Do it on affordable hardware\n",
    "- Deploy it efficiently\n",
    "\n",
    "Real-world applications I've seen:\n",
    "- Legal firms: Contract analysis in their style\n",
    "- Healthcare: Medical report generation\n",
    "- Finance: Compliance-aware responses\n",
    "- Retail: Product description generation\n",
    "- Gaming: NPC dialogue systems\n",
    "\n",
    "But here's the final challenge: How do we deploy these custom models at scale? How do we serve multiple LoRA adapters efficiently? How do we ensure production reliability?\n",
    "\n",
    "That's our grand finale - Part 4: Deploying LoRA models with NIMs. We'll build a production system that can serve your custom models to millions of users.\n",
    "\n",
    "Ready to complete your journey from prototype to production? Let's go!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
