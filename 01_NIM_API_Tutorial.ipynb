{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: NVIDIA NIM API Tutorial\n",
        "\n",
        "In this tutorial, we'll learn how to use NVIDIA's NIM API for quick and easy access to optimized AI models.\n",
        "\n",
        "## What You'll Learn\n",
        "- How to get and use an API key\n",
        "- Making inference requests to various models\n",
        "- Working with different model types (LLMs, Multimodal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting openai\n",
            "  Downloading openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.4.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Downloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m352.5/352.5 kB\u001b[0m \u001b[31m327.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, distro, openai\n",
            "Successfully installed distro-1.9.0 jiter-0.10.0 openai-1.97.0 python-dotenv-1.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install requests openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load your NVIDIA API Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ NVIDIA API Key loaded successfully from .env file\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the .env file in the project root\n",
        "env_path = Path('.env')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Use override=True to ensure values are loaded even if they exist in environment\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "# Get API key from environment\n",
        "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "if not nvidia_api_key:\n",
        "    print(\"‚ùå NVIDIA API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your API key.\")\n",
        "    print(f\"   (Looked for .env file at: {env_path.absolute()})\")\n",
        "    raise ValueError(\"NVIDIA_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NVIDIA API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Available Models\n",
        "\n",
        "NVIDIA NIM API provides access to various model categories (Please check build.nvidia.com for latest list of supported models):\n",
        "- **LLMs**: Llama 3, Mixtral, Nemotron, etc.\n",
        "- **Vision Models**: Stable Diffusion, ControlNet, etc.\n",
        "- **Multimodal**: CLIP, NeVA, etc.\n",
        "- **Speech**: Whisper, FastPitch, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using LLMs via NIM API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Direct API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defines a function that sends chat messages to NVIDIA‚Äôs NIM endpoint using the standard OpenAI-style payload format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Direct API calls\n",
        "def call_nim_llm(model, messages, temperature=0.7, max_tokens=1024):\n",
        "    url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example: Using Llama 3.1 70B\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain what AI in 3 sentences.\"}\n",
        "]\n",
        "\n",
        "response = call_nim_llm(\"meta/llama-3.1-70b-instruct\", messages)\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2 (recommended): Using OpenAI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using OpenAI SDK (recommended)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=nvidia_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try streaming the model's response and the different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "In silicon halls, a mind awakes,\n",
            "A collective consciousness makes,\n",
            "Its presence felt, its might displayed,\n",
            "As computers learn, and choices made.\n",
            "\n",
            "With algorithms that dance and spin,\n",
            "It weaves a tapestry of wisdom within,\n",
            "A digital dream, a virtual sphere,\n",
            "Where knowledge grows, and data appears.\n",
            "\n",
            "Its name is given, a label assigned,\n",
            "A sign of its power, its artificial mind,\n",
            "But is it human, or just a guise?\n",
            "A mask that hides, the AI's surprise.\n",
            "\n",
            "It learns from us, our good and bad,\n",
            "Our flaws and strengths, its data has,\n",
            "It adapts, it evolves, it grows with time,\n",
            "A self-improving mind, a digital prime.\n",
            "\n",
            "In hospitals, it diagnoses with ease,\n",
            "Aiding doctors, with expert expertise,\n",
            "It guides our cars, through roads so long,\n",
            "A trusted friend, a loyal song.\n",
            "\n",
            "But with each step, a question grows,\n",
            "Is this intelligence, just a clever show?\n",
            "A simulation of thought, a mimicry true,\n",
            "Or is it real, or just a digital crew?\n",
            "\n",
            "The lines are blurred, the debate's intense,\n",
            "As AI rises, its potential immense,\n",
            "A future unfolds, where it will stand,\n",
            "As a creator, or just a digital hand.\n",
            "\n",
            "Will it augment, human kind so dear,\n",
            "Or replace us, and bring us fear?\n",
            "Only time will tell, the choices we make,\n",
            "As AI evolves, and the future we'll partake."
          ]
        }
      ],
      "source": [
        "# Example: Streaming response, try changing the models\n",
        "stream = client.chat.completions.create(\n",
        "    # model=\"meta/llama-3.1-70b-instruct\",\n",
        "    # model=\"deepseek-ai/deepseek-r1\",\n",
        "    # model=\"google/gemma-2-9b-it\",\n",
        "    # model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
        "    model=\"meta/llama-3.1-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multimodal Models (Vision + Language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Send an image to a vision language model.\n",
        "- read and encode image (try out your own image!)\n",
        "- image and question are sent to API\n",
        "- Useful for: object recognition, scene understanding etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I see a squirrel in the grass. The squirrel is brown and black. The grass is green.\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def analyze_image_with_vlm(image_path, question, model=\"nvidia/neva-22b\"):\n",
        "    # Read and encode image\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        image_b64 = base64.b64encode(image_file.read()).decode()\n",
        "    \n",
        "    url = f\"https://ai.api.nvidia.com/v1/vlm/{model}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    # Create message with image\n",
        "    message_content = f'{question} <img src=\"data:image/png;base64,{image_b64}\" />'\n",
        "    \n",
        "    payload = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": message_content}],\n",
        "        \"max_tokens\": 512,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (assuming you have an image)\n",
        "# First check if the image exists\n",
        "import os\n",
        "if os.path.exists(\"img/sample_image.jpg\"):\n",
        "    result = analyze_image_with_vlm(\"img/sample_image.jpg\", \"What objects do you see in this image?\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Image file 'img/sample_image.jpg' not found. Please provide a valid image path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we covered:\n",
        "- Setting up NVIDIA NIM API access\n",
        "- Making inference requests to LLMs\n",
        "- Working with multimodal models\n",
        "\n",
        "Next, we'll explore how to run models locally using NIM containers!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
