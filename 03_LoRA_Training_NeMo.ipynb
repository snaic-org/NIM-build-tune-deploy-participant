{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune Llama 3.1 8B Instruct using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~100-500MB for 8B models)\n",
    "\n",
    "The focus of this workshop is not the specifics of LoRA, but to actually give everyone an guide on how to carry out the process of tuning your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: NeMo Framework Setup\n",
    "\n",
    "This notebook requires the NVIDIA NeMo framework for LoRA training. We'll clone the NeMo repository to access the necessary training scripts.\n",
    "\n",
    "**NeMo Compatibility**: \n",
    "- The downloaded model uses standard NeMo format (.nemo file)\n",
    "- The training scripts work directly without any modifications\n",
    "\n",
    "**Training Experience**: In this workshop, you'll train your own LoRA adapter from scratch! This gives you hands-on experience with:\n",
    "- Setting up training data\n",
    "- Configuring LoRA parameters\n",
    "- Running the actual training\n",
    "- Testing your custom adapter\n",
    "\n",
    "The training process takes approximately 5-10 minutes for our small example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone NeMo Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the NVIDIA NeMo framework (if not already present) and verifies that the required training script are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning NeMo repository...\n",
      "Cloning into './NeMo'...\n",
      "remote: Enumerating objects: 253911, done.\u001b[K\n",
      "remote: Counting objects: 100% (612/612), done.\u001b[K\n",
      "remote: Compressing objects: 100% (277/277), done.\u001b[K\n",
      "remote: Total 253911 (delta 466), reused 337 (delta 335), pack-reused 253299 (from 4)\u001b[K\n",
      "Receiving objects: 100% (253911/253911), 461.26 MiB | 47.20 MiB/s, done.\n",
      "Resolving deltas: 100% (192134/192134), done.\n",
      "NeMo repository cloned successfully!\n",
      "\n",
      "Checking for required NeMo scripts:\n",
      "✓ Found: megatron_gpt_finetuning.py\n",
      "✓ Found: megatron_gpt_generate.py\n",
      "✓ Found: merge.py\n"
     ]
    }
   ],
   "source": [
    "# Clone NeMo repository if not already present\n",
    "import os\n",
    "\n",
    "# Use relative path for NeMo\n",
    "nemo_path = './NeMo'\n",
    "\n",
    "if not os.path.exists(nemo_path):\n",
    "    print(\"Cloning NeMo repository...\")\n",
    "    !git clone https://github.com/NVIDIA/NeMo.git {nemo_path}\n",
    "    print(\"NeMo repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"NeMo repository already exists.\")\n",
    "    \n",
    "# Verify the training scripts exist\n",
    "nemo_scripts = [\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py',\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py',\n",
    "    f'{nemo_path}/scripts/nlp_language_modeling/merge_lora_weights/merge.py'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for required NeMo scripts:\")\n",
    "for script in nemo_scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"✓ Found: {os.path.basename(script)}\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {script}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use the finetuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0a0+ebedce2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2023.12.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.3.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the required Python libraries for LoRA training and verifies that PyTorch can access the GPU, displaying the GPU name and available memory.\n",
    "\n",
    "Documentation on models and their required compute here https://docs.nvidia.com/nim/large-language-models/latest/supported-llm-specific-models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0a0+ebedce2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 42.29 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the directories needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Time to create our training data! \n",
    "\n",
    "We're creating a customer support AI, but here's the key insight: we're not teaching it WHAT customer support is - Llama already knows that. We're teaching it HOW to do customer support in YOUR specific style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimal set intended for demonstration purposes — just enough to walk through the end-to-end fine-tuning pipeline.\n",
    "In practice, you'd want a much larger dataset to achieve meaningful performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 training examples\n",
      "Created 2 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Prerequisites Before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for\n",
    "- NeMo repo\n",
    "- fine-tuning script\n",
    "- llama 3.1 8b .nemo model file\n",
    "- training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking prerequisites for training...\n",
      "\n",
      "✅ NeMo repository found\n",
      "✅ Training script found\n",
      "✅ Llama 3.1 8B model found\n",
      "\n",
      "📁 Model Information:\n",
      "   Path: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
      "   Size: 14.96 GB\n",
      "   Format: Standard NeMo checkpoint (.nemo)\n",
      "✅ Training data found\n",
      "\n",
      "🎯 Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites before training\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"🔍 Checking prerequisites for training...\\n\")\n",
    "\n",
    "# Check if NeMo is cloned - use relative path\n",
    "nemo_path = \"./NeMo\"\n",
    "if os.path.exists(nemo_path):\n",
    "    print(\"✅ NeMo repository found\")\n",
    "else:\n",
    "    print(\"❌ NeMo repository not found! Please run cell 2 to clone NeMo.\")\n",
    "\n",
    "# Check if training scripts exist\n",
    "training_script = f\"{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\"\n",
    "if os.path.exists(training_script):\n",
    "    print(\"✅ Training script found\")\n",
    "else:\n",
    "    print(\"❌ Training script not found!\")\n",
    "\n",
    "# Check if model is downloaded - look in subdirectories since NGC creates them\n",
    "model_files = glob.glob(\"lora_tutorial/models/llama-3_1-8b-instruct/**/*.nemo\", recursive=True)\n",
    "if model_files:\n",
    "    model_path = model_files[0]  # Use the first .nemo file found\n",
    "    # Check if it's a complete model (>10GB)\n",
    "    size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    if size_gb > 10:\n",
    "        print(\"✅ Llama 3.1 8B model found\")\n",
    "        print(f\"\\n📁 Model Information:\")\n",
    "        print(f\"   Path: {model_path}\")\n",
    "        print(f\"   Size: {size_gb:.2f} GB\")\n",
    "        print(f\"   Format: Standard NeMo checkpoint (.nemo)\")\n",
    "    else:\n",
    "        print(f\"⚠️  Incomplete model found ({size_gb:.1f} GB)\")\n",
    "        print(\"   Please re-run the download in 00_Workshop_Setup.ipynb\")\n",
    "else:\n",
    "    print(\"❌ Model not found! Please run notebook 00_Workshop_Setup.ipynb first\")\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists(\"lora_tutorial/data/train.jsonl\"):\n",
    "    print(\"✅ Training data found\")\n",
    "else:\n",
    "    print(\"❌ Training data not found! Please run the data preparation cells\")\n",
    "\n",
    "print(\"\\n🎯 Ready to train!\" if all([\n",
    "    os.path.exists(nemo_path),\n",
    "    os.path.exists(training_script),\n",
    "    len(model_files) > 0 and os.path.getsize(model_files[0]) / (1024**3) > 10,  # Check complete model\n",
    "    os.path.exists(\"lora_tutorial/data/train.jsonl\")\n",
    "]) else \"\\n⚠️ Please fix the issues above before training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run LoRA Training\n",
    "\n",
    "### Actually Run the Training! 🚀\n",
    "\n",
    "This is the exciting part - you'll train your own LoRA adapter! \n",
    "\n",
    "**What will happen:**\n",
    "1. The model will load from the .nemo checkpoint (takes ~30 seconds)\n",
    "2. Training will run for 50 steps (~5-10 minutes)\n",
    "3. Checkpoints will be saved every 25 steps\n",
    "4. A final LoRA adapter will be exported as a .nemo file\n",
    "\n",
    "**Note about warnings**: You'll see many warnings about missing configuration fields - these are normal and can be ignored. They appear because NeMo supports many optional features that aren't used in this training.\n",
    "\n",
    "Let's train your custom model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the LoRA fine-tuning process, training Llama 3.1 8B on the customer support dataset for 50 steps using a single GPU with LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve how well your model learns\n",
    "- Increase max_steps: Let the model see the data more times. For small datasets, 200–500 steps is a better starting point than 50.\n",
    "- Adjust learning rate: Too high can prevent convergence; too low may be too slow. Try values like 1e-4 or 5e-5.\n",
    "- Increase batch size: If memory allows, raise micro_batch_size and global_batch_size to make updates more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found Llama 3.1 8B model at lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:17:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:17:30 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-16 19:17:30 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.5\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: lora_tutorial/experiments\n",
      "      name: customer_support_lora\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 2\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.1\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - lora_tutorial/data/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - lora_tutorial/data/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:17:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:17:30 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-16 19:17:30 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2025-07-16 19:17:30 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :lora_tutorial/experiments/customer_support_lora/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:17:30 exp_manager:396] Experiments will be logged at lora_tutorial/experiments/customer_support_lora\n",
      "[NeMo I 2025-07-16 19:17:31 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:17:31 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "25-07-16 19:17:50 - PID:45124 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 2\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:17:50 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-07-16 19:17:50 megatron_init:352] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2025-07-16 19:17:50 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:17:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:17:50 megatron_base_model:584] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:498] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-16 19:17:50 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:18:11 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "Loading distributed checkpoint directly on the GPU\n",
      "[NeMo I 2025-07-16 19:18:57 nlp_overrides:1180] Model MegatronGPTSFTModel was successfully restored from /root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo.\n",
      "[NeMo I 2025-07-16 19:18:57 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2025-07-16 19:18:57 nlp_adapter_mixins:203] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-16 19:19:00 nlp_adapter_mixins:208] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:19:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:00 megatron_gpt_sft_model:811] Building GPT SFT validation datasets.\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.067944\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.053882\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:249] Loading lora_tutorial/data/val.jsonl\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001322\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2025-07-16 19:19:00 megatron_gpt_sft_model:815] Length of val dataset: 2\n",
      "[NeMo I 2025-07-16 19:19:00 megatron_gpt_sft_model:822] Building GPT SFT traing datasets.\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.055920\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.055045\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:249] Loading lora_tutorial/data/train.jsonl\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000963\n",
      "[NeMo I 2025-07-16 19:19:00 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-16 19:19:00 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2025-07-16 19:19:01 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.09 (sec)\n",
      "[NeMo I 2025-07-16 19:19:01 megatron_gpt_sft_model:824] Length of train dataset: 101\n",
      "[NeMo I 2025-07-16 19:19:01 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2025-07-16 19:19:01 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-07-16 19:19:01 megatron_base_model:1199] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2025-07-16 19:19:01 nlp_adapter_mixins:269] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-16 19:19:01 modelPT:770] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2025-07-16 19:19:01 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7b6db6769570>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n",
      "[NeMo I 2025-07-16 19:19:01 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7b6db6769ae0>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 8.0 B  | train\n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "[NeMo W 2025-07-16 19:19:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:01 nemo_logging:349] /opt/apex/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-16 19:19:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  50%|█████     | 25/50 [00:09<00:09, v_num=0, reduced_train_loss=0.00124, global_step=24.00, consumed_samples=50.00, train_step_timing in s=0.348]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.052\n",
      "Epoch 0, global step 25: 'validation_loss' reached 0.05194 (best 0.05194), saving model to '/root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0.ckpt' as top 1\n",
      "[NeMo W 2025-07-16 19:19:11 nlp_overrides:480] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 50/50 [00:19<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.345, val_loss=0.0519] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.052 >= min_delta = 0.001. New best score: 0.000\n",
      "Epoch 0, global step 50: 'validation_loss' reached 0.00010 (best 0.00010), saving model to '/root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 50/50 [00:19<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.345, val_loss=0.000104][NeMo I 2025-07-16 19:19:22 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0.ckpt\n",
      "[NeMo I 2025-07-16 19:19:22 nlp_overrides:464] Removing checkpoint: /root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.052-step=25-consumed_samples=50.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 50/50 [00:19<00:00, v_num=0, reduced_train_loss=3.75e-5, global_step=49.00, consumed_samples=100.0, train_step_timing in s=0.345, val_loss=0.000104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt\n",
      "Restored all states from the checkpoint at /root/verb-workspace/NIM-build-tune-deploy-participant/lora_tutorial/experiments/customer_support_lora/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "\n",
    "# Find the model file dynamically (NGC creates subdirectories)\n",
    "MODEL_DIR=\"lora_tutorial/models/llama-3_1-8b-instruct\"\n",
    "MODEL=$(find \"$MODEL_DIR\" -name \"*.nemo\" -type f | head -1)\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"ERROR: No .nemo model file found in $MODEL_DIR\"\n",
    "    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "TRAIN_DS=\"[lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Use relative path to NeMo\n",
    "NEMO_PATH=\"./NeMo\"\n",
    "\n",
    "echo \"✅ Found Llama 3.1 8B model at $MODEL\"\n",
    "\n",
    "# Run training with NeMo\n",
    "torchrun --nproc_per_node=1 \\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\n",
    "    exp_manager.exp_dir=lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📌 Parameter Explanations for NeMo LoRA Fine-tuning</summary>\n",
    "\n",
    "- **`exp_manager.exp_dir=lora_tutorial/experiments`**  \n",
    "  Sets the root directory for saving logs, checkpoints, and training artifacts.\n",
    "\n",
    "- **`exp_manager.name=customer_support_lora`**  \n",
    "  Defines the subfolder name under `exp_dir` for this specific run.\n",
    "\n",
    "- **`trainer.devices=1`**  \n",
    "  Specifies that we’re training on one GPU.\n",
    "\n",
    "- **`trainer.num_nodes=1`**  \n",
    "  Specifies that we’re only using one machine (no multi-node training).\n",
    "\n",
    "- **`trainer.precision=bf16-mixed`**  \n",
    "  Enables bfloat16 mixed precision training — this reduces memory usage and speeds up training while maintaining numerical stability.\n",
    "\n",
    "- **`trainer.val_check_interval=0.5`**  \n",
    "  Validation will be run every 0.5 of an epoch (i.e., halfway through).\n",
    "\n",
    "- **`trainer.max_steps=50`**  \n",
    "  Stops training after 50 optimization steps — short for demo purposes.\n",
    "\n",
    "- **`model.megatron_amp_O2=True`**  \n",
    "  Enables Megatron’s optimization level 2 for efficient memory use and fused operations.\n",
    "\n",
    "- **`++model.mcore_gpt=True`**  \n",
    "  Uses the newer modular GPT implementation (MCore) within NeMo, required for Llama models.\n",
    "\n",
    "- **`model.tensor_model_parallel_size=1`**  \n",
    "  No tensor parallelism — model layers are not split across GPUs.\n",
    "\n",
    "- **`model.pipeline_model_parallel_size=1`**  \n",
    "  No pipeline parallelism — forward/backward passes are not split across stages.\n",
    "\n",
    "- **`model.micro_batch_size=1`**  \n",
    "  Each GPU processes 1 example at a time.\n",
    "\n",
    "- **`model.global_batch_size=2`**  \n",
    "  The total batch size per optimization step is 2 — this means gradient accumulation is disabled.\n",
    "\n",
    "- **`model.restore_from_path=${MODEL}`**  \n",
    "  Loads the pretrained `.nemo` checkpoint from the specified file.\n",
    "\n",
    "- **`model.data.train_ds.file_names=${TRAIN_DS}`**  \n",
    "  Path to the training dataset file (JSONL format).\n",
    "\n",
    "- **`model.data.train_ds.concat_sampling_probabilities=[1.0]`**  \n",
    "  Since we’re using only one training file, we give it 100% sampling probability.\n",
    "\n",
    "- **`model.data.validation_ds.file_names=${VALID_DS}`**  \n",
    "  Path to the validation dataset.\n",
    "\n",
    "- **`model.peft.peft_scheme=lora`**  \n",
    "  Enables PEFT and specifies LoRA as the tuning method.\n",
    "\n",
    "- **`model.peft.lora_tuning.target_modules=[attention_qkv]`**  \n",
    "  LoRA will only be applied to the query/key/value attention weights — this is common for transformer-based models.\n",
    "\n",
    "- **`model.peft.lora_tuning.adapter_dim=32`**  \n",
    "  Sets the rank of the LoRA adapter matrices — a tradeoff between model capacity and training efficiency.\n",
    "\n",
    "- **`model.peft.lora_tuning.adapter_dropout=0.1`**  \n",
    "  Applies dropout during training of the LoRA adapters to regularize them.\n",
    "\n",
    "- **`model.optim.lr=5e-4`**  \n",
    "  Sets the learning rate for optimization — relatively high for a small batch/short run.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Look at what we've created! Let me break down these files:\n",
    "\n",
    "1. **customer_support_lora.nemo** (21MB) - This is your golden ticket! Your custom AI adapter\n",
    "2. **The .ckpt files** (147MB each) - Full training checkpoints with optimizer states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307504\n",
      "drwxr-xr-x 2 root root      4096 Jul 16 19:19  .\n",
      "drwxr-xr-x 4 root root      4096 Jul 16 19:19  ..\n",
      "-rw-r--r-- 1 root root  21012480 Jul 16 19:19  customer_support_lora.nemo\n",
      "-rw-r--r-- 1 root root 146930030 Jul 16 19:19 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0-last.ckpt'\n",
      "-rw-r--r-- 1 root root 146930030 Jul 16 19:19 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt'\n"
     ]
    }
   ],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "- ✅ Set up the NeMo training environment\n",
    "- ✅ Created training data for your custom task\n",
    "- ✅ Configured LoRA parameters for efficient training\n",
    "- ✅ Trained your own LoRA adapter on Llama 3.1 8B\n",
    "\n",
    "Your LoRA adapter is now ready to be deployed with NVIDIA NIM in the next notebook!\n",
    "\n",
    "**Next Steps**: \n",
    "- Open `04_Deploy_LoRA_with_NIM.ipynb` to deploy your custom model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
