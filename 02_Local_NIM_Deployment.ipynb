{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Local NIM Deployment\n",
        "\n",
        "This notebook will guide you through deploying NVIDIA NIMs locally on your own infrastructure.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA GPU\n",
        "- Docker installed\n",
        "- NGC API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensure GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jul 16 18:04:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             74W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell is performing a GPU availability test to verify that Docker can properly access the NVIDIA GPUs on the system. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unable to find image 'ubuntu:latest' locally\n",
            "latest: Pulling from library/ubuntu\n",
            "\n",
            "\u001b[1B12e3802c: Pull complete .72MB/29.72MBB\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:c4570d2f4665d5d118ae29fb494dee4f8db8fcfaee0e37a2e19b827f399070d3\n",
            "Status: Downloaded newer image for ubuntu:latest\n",
            "Wed Jul 16 18:04:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             50W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Verify Docker and NVIDIA runtime\n",
        "!docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache directory for NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're creating a cache directory. This is important - models are LARGE (5-100GB). The cache means:\n",
        "- Download once, use many times\n",
        "- Survive container restarts\n",
        "- Share models between containers\n",
        "- Quick model switching\n",
        "\n",
        "The cache will be at ~/.cache/nim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NIM cache directory: /root/.cache/nim\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Set cache directory\n",
        "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
        "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
        "print(f\"NIM cache directory: {LOCAL_NIM_CACHE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load API Keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load both the NGC and NVIDIA API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ NVIDIA API Key loaded successfully from .env file\n",
            "‚úÖ NGC API Key loaded successfully from .env file\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the .env file in the project root\n",
        "env_path = Path('.env')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Use override=True to ensure values are loaded even if they exist in environment\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "# Get API keys from environment\n",
        "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "ngc_api_key = os.getenv(\"NGC_API_KEY\")\n",
        "\n",
        "# Check NVIDIA API Key\n",
        "if not nvidia_api_key:\n",
        "    print(\"‚ùå NVIDIA API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your API key.\")\n",
        "    print(f\"   (Looked for .env file at: {env_path.absolute()})\")\n",
        "    raise ValueError(\"NVIDIA_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NVIDIA API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
        "\n",
        "# Check NGC API Keys\n",
        "if not ngc_api_key:\n",
        "    print(\"‚ùå NGC API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your NGC API key.\")\n",
        "    raise ValueError(\"NGC_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NGC API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NGC_API_KEY\"] = ngc_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NGC Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log into NGC, for pulling NIM Containers and other assets on nvcr.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login result: Login Succeeded\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{ngc_api_key}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "print(\"Login result:\", result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deploy Your First NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define container and image name\n",
        "\n",
        "Clean existing containers if running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define deployment parameters\n",
        "CONTAINER_NAME = \"llama3.1-8b-instruct\" \n",
        "IMG_NAME = \"nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\"\n",
        "\n",
        "# Stop existing container if running\n",
        "!docker stop {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker rm {CONTAINER_NAME} 2>/dev/null || true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Docker run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This cell deploys the NIM container with the Llama 3.1 8B Instruct model. It constructs and executes a Docker command that:\n",
        "\n",
        "**Container Configuration:**\n",
        "- Runs in detached mode (`-d`) for background operation\n",
        "- Enables GPU access with NVIDIA runtime\n",
        "- Allocates 16GB shared memory for PyTorch operations\n",
        "- Mounts the local cache directory to persist downloaded models\n",
        "- Maps port 8000 for API access\n",
        "- Runs with the current user's permissions to avoid file permission issues\n",
        "\n",
        "**Key Environment Variables:**\n",
        "- `ngc_api_key`: Authenticates with NVIDIA GPU Cloud to download the model\n",
        "- `LOCAL_NIM_CACHE`: Points to `~/.cache/nim` for model storage\n",
        "\n",
        "**Success/Failure Handling:**\n",
        "- On success: Displays the container ID and confirms deployment\n",
        "\n",
        "The first run will download the model (5-10 minutes), while subsequent runs use the cached model for faster startup (30-60 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting NIM container...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Container started successfully!\n",
            "Container ID: 56287a20a0092ce5c9ab7ef10cf1e8ca07425c3d84939b742688346787de3ce1\n"
          ]
        }
      ],
      "source": [
        "# Start NIM container\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d --name={CONTAINER_NAME} \\\n",
        "    --runtime=nvidia \\\n",
        "    --gpus all \\\n",
        "    --shm-size=16GB \\\n",
        "    -e NGC_API_KEY={ngc_api_key} \\\n",
        "    -v {LOCAL_NIM_CACHE}:/opt/nim/.cache \\\n",
        "    -u $(id -u) \\\n",
        "    -p 8000:8000 \\\n",
        "    {IMG_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting NIM container...\")\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# Check if the command succeeded\n",
        "if result.returncode == 0 and result.stdout.strip():\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"‚úÖ Container started successfully!\")\n",
        "    print(f\"Container ID: {container_id}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to start container!\")\n",
        "    print(f\"Return code: {result.returncode}\")\n",
        "    if result.stderr:\n",
        "        print(f\"Error message: {result.stderr}\")\n",
        "    if result.stdout:\n",
        "        print(f\"Output: {result.stdout}\")\n",
        "    \n",
        "    # Common issues and solutions\n",
        "    print(\"\\nTroubleshooting tips:\")\n",
        "    print(\"1. Check if Docker is running: docker info\")\n",
        "    print(\"2. Check if image exists: docker images | grep llama\")\n",
        "    print(\"3. Check if port 8000 is already in use: docker ps -a\")\n",
        "    print(f\"4. Check Docker logs: docker logs {CONTAINER_NAME}\")\n",
        "    print(\"5. Verify NGC authentication: echo $NGC_API_KEY\")\n",
        "    print(\"6. Check available disk space: df -h\")\n",
        "    print(\"7. Verify GPU is accessible: nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Waiting for container to set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function polls the health endpoint until the NIM is ready. This may take a few minutes\n",
        "\n",
        "[When ready appears]: The NIM is now ready to serve requests through the familiar OpenAI API format\n",
        "\n",
        "If cell fails on timeoout, try running the cell again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for NIM to start (this may take a few minutes on first run)...\n",
            "...........................................\n",
            "‚úÖ NIM is ready!\n",
            "NIM is ready to serve requests!\n"
          ]
        }
      ],
      "source": [
        "def wait_for_nim_ready(max_attempts=60, sleep_time=5):\n",
        "    \"\"\"Wait for NIM to be ready to serve requests\"\"\"\n",
        "    print(\"Waiting for NIM to start (this may take a few minutes on first run)...\")\n",
        "    \n",
        "    # Get container IP\n",
        "    import subprocess\n",
        "    import json\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', CONTAINER_NAME], \n",
        "                              capture_output=True, text=True)\n",
        "        container_info = json.loads(result.stdout)\n",
        "        container_ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "        health_url = f\"http://{container_ip}:8000/v1/health/ready\"\n",
        "    except:\n",
        "        health_url = \"http://localhost:8000/v1/health/ready\"  # fallback\n",
        "    \n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = requests.get(health_url)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n‚úÖ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(sleep_time)\n",
        "    \n",
        "    print(\"\\n‚ùå NIM failed to start\")\n",
        "    return False\n",
        "\n",
        "# Wait for container to be ready\n",
        "if wait_for_nim_ready():\n",
        "    print(\"NIM is ready to serve requests!\")\n",
        "else:\n",
        "    print(\"Check logs with: docker logs\", CONTAINER_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get container's IP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the container's IP address\n",
        "- localhost might not work on GPU cloud instances\n",
        "- we will directly connect to the container's network via the IP addr\n",
        "- The cell then verifies the NIM is working by requesting the available models list, confirming the API is ready to serve requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Container IP: 172.17.0.3\n",
            "‚úÖ NIM is accessible via container IP!\n",
            "Available models: {'object': 'list', 'data': [{'id': 'meta/llama-3.1-8b-instruct', 'object': 'model', 'created': 1752690836, 'owned_by': 'system', 'root': 'meta/llama-3.1-8b-instruct', 'parent': None, 'max_model_len': 131072, 'permission': [{'id': 'modelperm-162147f1ab4e4f9e81686c4e70c4e153', 'object': 'model_permission', 'created': 1752690836, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Get container IP address\n",
        "def get_container_ip(container_name):\n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', container_name], \n",
        "                              capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            container_info = json.loads(result.stdout)\n",
        "            ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "            print(f\"Container IP: {ip}\")\n",
        "            return ip\n",
        "        else:\n",
        "            print(f\"Failed to get container info for '{container_name}'\")\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting container IP: {e}\")\n",
        "        return None\n",
        "\n",
        "container_ip = get_container_ip(CONTAINER_NAME)\n",
        "\n",
        "# If we have the IP, try connecting to it directly\n",
        "if container_ip:\n",
        "    try:\n",
        "        response = requests.get(f\"http://{container_ip}:8000/v1/models\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ NIM is accessible via container IP!\")\n",
        "            print(\"Available models:\", response.json())\n",
        "        else:\n",
        "            print(f\"‚ùå Got status code {response.status_code} from container IP\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error connecting to container IP: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Local NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run our NIM locally\n",
        "- points to local container instead of the cloud\n",
        "\n",
        "Try switching between cloud and local NIMs:\n",
        "- by simply changing 'base_url'\n",
        "\n",
        "This demonstrates the power of NIMs: same API, same code, but now running on your own hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "Here is a short poem about AI:\n",
            "\n",
            "Metal minds with circuits bright,\n",
            "Think and learn through endless night.\n",
            "Synthetic intelligence, a wondrous thing,\n",
            "Processing data, with computations that sing.\n",
            "\n",
            "With logic guiding, and codes unseen,\n",
            "AI navigates the digital scene.\n",
            "From silicon dreams to futuristic depth,\n",
            "A new era unfolds, a world to keep.\n",
            "\n",
            "Yet as it rises, with power and might,\n",
            "We question ethics, and what's in sight.\n",
            "Will it aid humanity, or pave its own way?\n",
            "Only time will tell, in a technological day."
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Create OpenAI client pointing to your local NIM\n",
        "client = OpenAI(\n",
        "    base_url=f\"http://{container_ip}:8000/v1\",\n",
        "    api_key=\"not-needed-for-local\",  # Local NIM doesn't require autu\n",
        ")\n",
        "\n",
        "# You can reference how we called this model via the API\n",
        "# client = OpenAI(\n",
        "#     base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "#     api_key=nvidia_api_key\n",
        "# )\n",
        "\n",
        "# Example: Streaming response\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"meta/llama-3.1-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note on output: Your poem may be different from the example shown because of the different parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models availale in local NIM deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call the `/v1/models` endpoint to list available models.\n",
        "\n",
        "This confirms the model is loaded and ready.\n",
        "\n",
        "Helps verify the correct model name for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": \"meta/llama-3.1-8b-instruct\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1752691199,\n",
            "      \"owned_by\": \"system\",\n",
            "      \"root\": \"meta/llama-3.1-8b-instruct\",\n",
            "      \"parent\": null,\n",
            "      \"max_model_len\": 131072,\n",
            "      \"permission\": [\n",
            "        {\n",
            "          \"id\": \"modelperm-ab62b432d07f4ac7bebaa2d18db627c8\",\n",
            "          \"object\": \"model_permission\",\n",
            "          \"created\": 1752691199,\n",
            "          \"allow_create_engine\": false,\n",
            "          \"allow_sampling\": true,\n",
            "          \"allow_logprobs\": true,\n",
            "          \"allow_search_indices\": false,\n",
            "          \"allow_view\": true,\n",
            "          \"allow_fine_tuning\": false,\n",
            "          \"organization\": \"*\",\n",
            "          \"group\": null,\n",
            "          \"is_blocking\": false\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Check available models\n",
        "response = requests.get(f\"http://{container_ip}:8000/v1/models\")\n",
        "models = response.json()\n",
        "print(\"Available models:\")\n",
        "print(json.dumps(models, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Clean Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we move on to LoRA fine-tuning, let's properly clean up our deployment. This is important for a few reasons:\n",
        "\n",
        "1. Frees up GPU memory for our next activities\n",
        "2. Prevents port conflicts if we redeploy\n",
        "\n",
        "Don't worry - the model remains cached, so if you want to restart this NIM later, it'll start up in seconds, not minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama3.1-8b-instruct\n",
            "llama3.1-8b-instruct\n"
          ]
        }
      ],
      "source": [
        "# Stop and remove container\n",
        "!docker stop {CONTAINER_NAME}\n",
        "!docker rm {CONTAINER_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've learned how to:\n",
        "- Deploy NIMs locally with Docker\n",
        "- Test deployments\n",
        "\n",
        "Next: Let's explore LoRA fine-tuning with NeMo!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
