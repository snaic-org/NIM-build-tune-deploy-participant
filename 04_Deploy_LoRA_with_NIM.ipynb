{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Deploying LoRA Adapters with NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to deploy your trained LoRA adapters using NVIDIA NIM. We'll cover:\n",
        "- Understanding NIM's LoRA deployment architecture\n",
        "- Using Docker volumes for reliable LoRA mounting\n",
        "- Testing your deployed LoRA adapter\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "1. Completed notebook 03 (LoRA training) - you should have a `.nemo` file\n",
        "2. Docker installed with GPU support\n",
        "3. Your NGC API key ready\n",
        "4. At least 20GB of free disk space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding NIM LoRA Deployment\n",
        "\n",
        "### How NIM Handles LoRA Adapters\n",
        "\n",
        "NVIDIA NIM supports dynamic LoRA loading through:\n",
        "- **NIM_PEFT_SOURCE**: Environment variable pointing to your LoRA directory\n",
        "- **Automatic Discovery**: NIM scans for `.nemo` files in subdirectories\n",
        "- **Hot Reloading**: With `NIM_PEFT_REFRESH_INTERVAL`, NIM checks for new adapters\n",
        "\n",
        "### Expected Directory Structure\n",
        "\n",
        "```\n",
        "NIM_PEFT_SOURCE/\n",
        "‚îú‚îÄ‚îÄ adapter1/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter1.nemo\n",
        "‚îú‚îÄ‚îÄ adapter2/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter2.nemo\n",
        "‚îî‚îÄ‚îÄ adapter3/\n",
        "    ‚îî‚îÄ‚îÄ adapter3.nemo\n",
        "```\n",
        "\n",
        "Each adapter must be in its own subdirectory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load NGC Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGC API Key configured: ‚úì\n",
            "Working directory: /root/verb-workspace/NIM-build-tune-deploy-participant\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables from .env file\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    # If python-dotenv is not installed, try to read .env manually\n",
        "    if os.path.exists('.env'):\n",
        "        with open('.env', 'r') as f:\n",
        "            for line in f:\n",
        "                if '=' in line:\n",
        "                    key, value = line.strip().split('=', 1)\n",
        "                    os.environ[key] = value\n",
        "\n",
        "# Set up environment\n",
        "NGC_API_KEY = os.getenv('NGC_API_KEY')\n",
        "if not NGC_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  NGC_API_KEY not found in environment or .env file!\")\n",
        "    print(\"Please run the Workshop Setup notebook (00_Workshop_Setup.ipynb) first.\")\n",
        "else:\n",
        "    os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "print(f\"NGC API Key configured: {'‚úì' if NGC_API_KEY else '‚úó'}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log into NGC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Successfully logged in to NGC\n"
          ]
        }
      ],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{NGC_API_KEY}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "if \"Login Succeeded\" in result.stdout:\n",
        "    print(\"‚úì Successfully logged in to NGC\")\n",
        "else:\n",
        "    print(\"‚úó Login failed!\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Your LoRA Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's check that your LoRA adapter is ready for deployment.\n",
        "\n",
        "Now let's make sure your LoRA adapter is ready. We're looking for the .nemo file from notebook 03."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Found LoRA adapter: lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\n",
            "  Size: 20.04 MB\n"
          ]
        }
      ],
      "source": [
        "# Check for LoRA files\n",
        "lora_paths = [\n",
        "    \"lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\",\n",
        "    \"loras/customer_support_lora/customer_support_lora.nemo\"\n",
        "]\n",
        "\n",
        "lora_file = None\n",
        "for path in lora_paths:\n",
        "    if os.path.exists(path):\n",
        "        lora_file = path\n",
        "        print(f\"‚úì Found LoRA adapter: {path}\")\n",
        "        print(f\"  Size: {os.path.getsize(path) / 1024 / 1024:.2f} MB\")\n",
        "        break\n",
        "\n",
        "if not lora_file:\n",
        "    print(\"‚úó No LoRA adapter found!\")\n",
        "    print(\"\\nPlease ensure you've completed notebook 03 and have a .nemo file.\")\n",
        "    print(\"Expected locations:\")\n",
        "    for path in lora_paths:\n",
        "        print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell prepares the LoRA adapter for deployment by creating the required directory structure (`loras/customer_support_lora`) and copying the trained LoRA file into it, which NIM expects for loading custom adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Copied LoRA adapter to deployment directory\n",
            "LoRA deployment structure:\n",
            "loras/customer_support_lora/customer_support_lora.nemo\n"
          ]
        }
      ],
      "source": [
        "# Create proper directory structure for NIM\n",
        "!mkdir -p loras/customer_support_lora\n",
        "\n",
        "# Copy LoRA file if needed\n",
        "if lora_file and not os.path.exists(\"loras/customer_support_lora/customer_support_lora.nemo\"):\n",
        "    !cp {lora_file} loras/customer_support_lora/\n",
        "    print(\"‚úì Copied LoRA adapter to deployment directory\")\n",
        "\n",
        "# Verify structure\n",
        "!echo \"LoRA deployment structure:\"\n",
        "!tree loras/ 2>/dev/null || find loras/ -type f -name \"*.nemo\" | head -10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean Up Existing Resources\n",
        "\n",
        "Before deploying, let's ensure we have a clean slate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning up existing resources...\n",
            "\n",
            "‚úì Cleanup complete\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CONTAINER_NAME = \"llama3.1-lora-nim-volume\"\n",
        "VOLUME_NAME = \"nim-lora-adapters\"\n",
        "IMAGE_NAME = \"nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\"\n",
        "\n",
        "# Clean up any existing resources\n",
        "print(\"üßπ Cleaning up existing resources...\")\n",
        "!docker rm -f {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker volume rm {VOLUME_NAME} 2>/dev/null || true\n",
        "\n",
        "print(\"\\n‚úì Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Docker Volume and Copy LoRA Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell creates a Docker volume and copies the LoRA adapter file into it using two methods:\n",
        "\n",
        "1. **First attempt**: Uses a bind mount to copy files directly\n",
        "2. **Fallback method**: If that fails (common on cloud), uses `docker cp` with a temporary container\n",
        "\n",
        "The volume is needed because cloud GPU instances often have issues with direct file mounting, so Docker volumes provide a reliable way to make the LoRA file available to the NIM container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "\n",
        "#### Important: Cloud GPU Deployment Challenges\n",
        "\n",
        "#### The Bind Mount Problem\n",
        "\n",
        "On cloud GPU instances, Docker bind mounts often fail due to:\n",
        "- Storage driver incompatibilities\n",
        "- Security policies\n",
        "- Network file systems\n",
        "\n",
        "**Symptoms:**\n",
        "- Mounted directories appear empty inside containers\n",
        "- Files exist on host but not visible in container\n",
        "- No error messages, just empty directories\n",
        "\n",
        "#### The Solution: Docker Volumes\n",
        "\n",
        "Docker named volumes work reliably where bind mounts fail. We'll use this approach throughout the notebook.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Creating Docker volume for LoRA adapters...\n",
            "nim-lora-adapters\n",
            "\n",
            "üìã Copying LoRA files to Docker volume...\n",
            "No files to copy\n",
            "total 8\n",
            "drwxr-xr-x    2 root     root          4096 Jul 16 19:23 .\n",
            "drwxr-xr-x    3 root     root          4096 Jul 16 19:23 ..\n",
            "\n",
            "\n",
            "üìã Ensuring LoRA files are in the volume (using docker cp)...\n",
            "a7f937c67a85b52aa5465cb0266e1a93b666a6c4819aaa16dc318bbb623e7294\n",
            "Successfully copied 21MB to temp-container:/data/customer_support_lora/\n",
            "total 20528\n",
            "drwxr-xr-x    2 root     root          4096 Jul 16 19:23 .\n",
            "drwxr-xr-x    3 root     root          4096 Jul 16 19:23 ..\n",
            "-rw-r--r--    1 root     root      21012480 Jul 16 19:20 customer_support_lora.nemo\n",
            "temp-container\n",
            "\n",
            "‚úì LoRA files copied to volume\n"
          ]
        }
      ],
      "source": [
        "# Create Docker volume\n",
        "print(\"üì¶ Creating Docker volume for LoRA adapters...\")\n",
        "!docker volume create {VOLUME_NAME}\n",
        "\n",
        "# Copy LoRA files to the volume using a temporary container\n",
        "print(\"\\nüìã Copying LoRA files to Docker volume...\")\n",
        "# Method 1: Try with bind mount first\n",
        "copy_result = subprocess.run(\n",
        "    f'docker run --rm -v {VOLUME_NAME}:/data -v $(pwd)/loras:/source alpine sh -c '\n",
        "    f'\"mkdir -p /data/customer_support_lora && '\n",
        "    f'cp -r /source/customer_support_lora/* /data/customer_support_lora/ 2>/dev/null || echo \\'No files to copy\\' && '\n",
        "    f'ls -la /data/customer_support_lora/\"',\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(copy_result.stdout)\n",
        "\n",
        "# Method 2: Use docker cp as fallback (more reliable on cloud)\n",
        "print(\"\\nüìã Ensuring LoRA files are in the volume (using docker cp)...\")\n",
        "!docker run -d --name temp-container -v {VOLUME_NAME}:/data alpine sleep 3600\n",
        "!docker cp loras/customer_support_lora/customer_support_lora.nemo temp-container:/data/customer_support_lora/\n",
        "!docker exec temp-container ls -la /data/customer_support_lora/\n",
        "!docker rm -f temp-container\n",
        "\n",
        "print(\"\\n‚úì LoRA files copied to volume\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output shows the two-step copying process:\n",
        "\n",
        "**First attempt failed**: \"No files to copy\" - the bind mount method didn't work (common on cloud GPUs)\n",
        "\n",
        "**Second attempt succeeded**: \n",
        "- \"Successfully copied 21MB\" - the `docker cp` method worked\n",
        "- The LoRA file (`customer_support_lora.nemo`, 21MB) is now in the Docker volume\n",
        "- Ready for NIM to use\n",
        "\n",
        "This is why the cell uses two methods - the first one often fails on cloud, but the second one (docker cp) is more reliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start NIM Container with LoRA Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell starts the NIM container with LoRA support enabled.\n",
        "\n",
        "**Key configuration:**\n",
        "- `NIM_PEFT_SOURCE=/lora-store` - tells NIM where to find LoRA adapters\n",
        "- `NIM_PEFT_REFRESH_INTERVAL=300` - checks for new LoRAs every 5 minutes\n",
        "- `-v {VOLUME_NAME}:/lora-store` - mounts the Docker volume containing your LoRA file\n",
        "\n",
        "The container runs in the background with GPU access and will take 2-3 minutes to initialize on first run as it loads the base model and discovers the LoRA adapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting NIM container with LoRA support...\n",
            "\n",
            "Command: \n",
            "docker run -d \\\n",
            "    --name=llama3.1-lora-nim-volume \\\n",
            "    --runtime=nvidia \\\n",
            "    --gpus all \\\n",
            "    --shm-size=16GB \\\n",
            "    -e NGC_API_KEY=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 \\\n",
            "    -e NIM_PEFT_SOURCE=/lora-store \\\n",
            "    -e NIM_PEFT_REFRESH_INTERVAL=300 \\\n",
            "    -v nim-lora-adapters:/lora-store \\\n",
            "    -p 8000:8000 \\\n",
            "    nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\n",
            "\n",
            "\n",
            "‚úì Container started: 3af54eec9038\n",
            "\n",
            "‚è≥ Container is initializing. This may take 2-3 minutes on first run...\n"
          ]
        }
      ],
      "source": [
        "# Start NIM container with LoRA support\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d \\\\\n",
        "    --name={CONTAINER_NAME} \\\\\n",
        "    --runtime=nvidia \\\\\n",
        "    --gpus all \\\\\n",
        "    --shm-size=16GB \\\\\n",
        "    -e NGC_API_KEY={NGC_API_KEY} \\\\\n",
        "    -e NIM_PEFT_SOURCE=/lora-store \\\\\n",
        "    -e NIM_PEFT_REFRESH_INTERVAL=300 \\\\\n",
        "    -v {VOLUME_NAME}:/lora-store \\\\\n",
        "    -p 8000:8000 \\\\\n",
        "    {IMAGE_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"üöÄ Starting NIM container with LoRA support...\")\n",
        "print(f\"\\nCommand: {docker_cmd}\")\n",
        "\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"\\n‚úì Container started: {container_id[:12]}\")\n",
        "    print(\"\\n‚è≥ Container is initializing. This may take 2-3 minutes on first run...\")\n",
        "else:\n",
        "    print(\"\\n‚úó Failed to start container\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Get Container Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's find our container's IP address. On cloud instances, 'localhost' might not work, so we get the actual container IP.\n",
        "\n",
        "You'll see something like 172.17.0.3 - that's Docker's internal network.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìç Container IP: 172.17.0.3\n",
            "\n",
            "üîç Verifying LoRA files in container...\n",
            "total 20528\n",
            "drwxr-xr-x 2 root root     4096 Jul 16 19:23 .\n",
            "drwxr-xr-x 3 root root     4096 Jul 16 19:23 ..\n",
            "-rw-r--r-- 1 root root 21012480 Jul 16 19:20 customer_support_lora.nemo\n"
          ]
        }
      ],
      "source": [
        "# Get container IP address\n",
        "def get_container_ip():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            f\"docker inspect -f '{{{{range.NetworkSettings.Networks}}}}{{{{.IPAddress}}}}{{{{end}}}}' {CONTAINER_NAME}\",\n",
        "            shell=True, capture_output=True, text=True\n",
        "        )\n",
        "        ip = result.stdout.strip()\n",
        "        return ip if ip else \"localhost\"\n",
        "    except:\n",
        "        return \"localhost\"\n",
        "\n",
        "container_ip = get_container_ip()\n",
        "print(f\"üìç Container IP: {container_ip}\")\n",
        "base_url = f\"http://{container_ip}:8000\"\n",
        "\n",
        "# Verify LoRA files are visible inside container\n",
        "print(\"\\nüîç Verifying LoRA files in container...\")\n",
        "!docker exec {CONTAINER_NAME} ls -la /lora-store/customer_support_lora/ || echo \"Container still starting...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Wait for NIM to Initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NIM needs time to initialize:\n",
        "- Loading the base model\n",
        "- Scanning for LoRA adapters\n",
        "- Optimizing for your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Waiting for NIM to initialize...\n",
            "..................\n",
            "‚úÖ NIM is ready!\n",
            "\n",
            "üìã Checking LoRA synchronization logs...\n",
            "INFO 2025-07-16 19:25:39.137 ngc_profile.py:333] Running NIM with LoRA enabled. Only looking for compatible profiles that support LoRA.\n",
            "INFO 2025-07-16 19:25:39.137 ngc_injector.py:159] Valid profile: 7b8458eb682edb0d2a48b4019b098ba0bfbc4377aadeeaa11b346c63c7adf724 (tensorrt_llm-trtllm_buildable-bf16-tp1-pp1-lora) on GPUs [0]\n",
            "INFO 2025-07-16 19:25:39.137 ngc_injector.py:159] Valid profile: f749ba07aade1d9e1c36ca1b4d0b67949122bd825e8aa6a52909115888a34b95 (vllm-bf16-tp1-pp1-lora) on GPUs [0]\n",
            "INFO 2025-07-16 19:25:39.137 ngc_injector.py:315] Selected profile: 7b8458eb682edb0d2a48b4019b098ba0bfbc4377aadeeaa11b346c63c7adf724 (tensorrt_llm-trtllm_buildable-bf16-tp1-pp1-lora)\n",
            "INFO 2025-07-16 19:25:39.138 ngc_injector.py:323] Profile metadata: feat_lora: true\n",
            "INFO 2025-07-16 19:25:39.138 ngc_injector.py:323] Profile metadata: feat_lora_max_rank: 32\n",
            "INFO 2025-07-16 19:26:54.917 launch.py:304] running command ['/opt/nim/llm/.venv/bin/python3', '-m', 'nim_llm_sdk.entrypoints.openai.api_server', '--served-model-name', 'meta/llama-3.1-8b-instruct', '--async-engine-args', '{\"model\": \"/tmp/LLM-der5fqbs\", \"served_model_name\": [\"meta/llama-3.1-8b-instruct\"], \"tokenizer\": \"/tmp/LLM-der5fqbs\", \"skip_tokenizer_init\": false, \"tokenizer_mode\": \"auto\", \"trust_remote_code\": false, \"download_dir\": null, \"load_format\": \"auto\", \"config_format\": \"auto\", \"dtype\": \"bfloat16\", \"kv_cache_dtype\": \"auto\", \"quantization_param_path\": null, \"seed\": 0, \"max_model_len\": null, \"worker_use_ray\": false, \"distributed_executor_backend\": \"mp\", \"pipeline_parallel_size\": 1, \"tensor_parallel_size\": 1, \"max_parallel_loading_workers\": null, \"block_size\": 16, \"enable_prefix_caching\": false, \"disable_sliding_window\": false, \"use_v2_block_manager\": true, \"swap_space\": 4, \"cpu_offload_gb\": 0, \"gpu_memory_utilization\": 0.9, \"max_num_batched_tokens\": null, \"max_num_seqs\": 256, \"max_logprobs\": 20, \"disable_log_stats\": false, \"revision\": null, \"code_revision\": null, \"rope_scaling\": null, \"rope_theta\": null, \"tokenizer_revision\": null, \"quantization\": null, \"enforce_eager\": false, \"max_context_len_to_capture\": null, \"max_seq_len_to_capture\": 8192, \"disable_custom_all_reduce\": false, \"tokenizer_pool_size\": 0, \"tokenizer_pool_type\": \"ray\", \"tokenizer_pool_extra_config\": null, \"limit_mm_per_prompt\": null, \"enable_lora\": true, \"enable_dora\": false, \"max_loras\": 8, \"max_lora_rank\": 32, \"enable_prompt_adapter\": false, \"max_prompt_adapters\": 1, \"max_prompt_adapter_token\": 0, \"fully_sharded_loras\": false, \"lora_extra_vocab_size\": 256, \"long_lora_scaling_factors\": null, \"lora_dtype\": \"auto\", \"max_cpu_loras\": 16, \"peft_source\": \"/lora-store\", \"peft_refresh_interval\": 300, \"device\": \"auto\", \"num_scheduler_steps\": 1, \"multi_step_stream_outputs\": true, \"ray_workers_use_nsight\": false, \"num_gpu_blocks_override\": null, \"num_lookahead_slots\": 0, \"model_loader_extra_config\": null, \"ignore_patterns\": [], \"preemption_mode\": null, \"scheduler_delay_factor\": 0.0, \"enable_chunked_prefill\": null, \"guided_decoding_backend\": \"outlines\", \"speculative_model\": null, \"speculative_model_quantization\": null, \"speculative_draft_tensor_parallel_size\": null, \"num_speculative_tokens\": null, \"speculative_disable_mqa_scorer\": false, \"speculative_max_model_len\": null, \"speculative_disable_by_batch_size\": null, \"ngram_prompt_lookup_max\": null, \"ngram_prompt_lookup_min\": null, \"spec_decoding_acceptance_method\": \"rejection_sampler\", \"typical_acceptance_sampler_posterior_threshold\": null, \"typical_acceptance_sampler_posterior_alpha\": null, \"qlora_adapter_name_or_path\": null, \"disable_logprobs_during_spec_decoding\": null, \"otlp_traces_endpoint\": null, \"collect_detailed_traces\": null, \"disable_async_output_proc\": false, \"override_neuron_config\": null, \"mm_processor_kwargs\": null, \"scheduling_policy\": \"fcfs\", \"disable_log_requests\": true, \"selected_gpus\": [{\"name\": \"NVIDIA A100-SXM4-40GB\", \"device_index\": 0, \"device_id\": \"20b0:10de\", \"total_memory\": 42285268992, \"free_memory\": 42285268992, \"used_memory\": 0, \"reserved_memory\": 0, \"compute_capability\": [8, 0]}], \"tllm_buildable\": true, \"tllm_config_json_str\": null, \"profile_id\": \"7b8458eb682edb0d2a48b4019b098ba0bfbc4377aadeeaa11b346c63c7adf724\", \"quantization_algo\": null, \"hf_config_json_str\": \"{\\\\n  \\\\\"architectures\\\\\": [\\\\n    \\\\\"LlamaForCausalLM\\\\\"\\\\n  ],\\\\n  \\\\\"attention_bias\\\\\": false,\\\\n  \\\\\"attention_dropout\\\\\": 0.0,\\\\n  \\\\\"bos_token_id\\\\\": 128000,\\\\n  \\\\\"eos_token_id\\\\\": [\\\\n    128001,\\\\n    128008,\\\\n    128009\\\\n  ],\\\\n  \\\\\"hidden_act\\\\\": \\\\\"silu\\\\\",\\\\n  \\\\\"hidden_size\\\\\": 4096,\\\\n  \\\\\"initializer_range\\\\\": 0.02,\\\\n  \\\\\"intermediate_size\\\\\": 14336,\\\\n  \\\\\"max_position_embeddings\\\\\": 131072,\\\\n  \\\\\"mlp_bias\\\\\": false,\\\\n  \\\\\"model_type\\\\\": \\\\\"llama\\\\\",\\\\n  \\\\\"num_attention_heads\\\\\": 32,\\\\n  \\\\\"num_hidden_layers\\\\\": 32,\\\\n  \\\\\"num_key_value_heads\\\\\": 8,\\\\n  \\\\\"pretraining_tp\\\\\": 1,\\\\n  \\\\\"rms_norm_eps\\\\\": 1e-05,\\\\n  \\\\\"rope_scaling\\\\\": {\\\\n    \\\\\"factor\\\\\": 8.0,\\\\n    \\\\\"low_freq_factor\\\\\": 1.0,\\\\n    \\\\\"high_freq_factor\\\\\": 4.0,\\\\n    \\\\\"original_max_position_embeddings\\\\\": 8192,\\\\n    \\\\\"rope_type\\\\\": \\\\\"llama3\\\\\"\\\\n  },\\\\n  \\\\\"rope_theta\\\\\": 500000.0,\\\\n  \\\\\"tie_word_embeddings\\\\\": false,\\\\n  \\\\\"torch_dtype\\\\\": \\\\\"bfloat16\\\\\",\\\\n  \\\\\"transformers_version\\\\\": \\\\\"4.42.3\\\\\",\\\\n  \\\\\"use_cache\\\\\": true,\\\\n  \\\\\"vocab_size\\\\\": 128256\\\\n}\\\\n\"}']\n",
            "INFO 2025-07-16 19:27:31.484 utils.py:257] Building rank 0 with config ConstrainedTrtllmConfig(dtype='bfloat16', pp_size=1, tp_size=1, max_batch_size=64, max_seq_len=131072, max_num_tokens=8192, lora=ConstrainedTrtllmLoraConfig(enable=True, modules=['attn_q', 'attn_k', 'attn_v', 'attn_dense', 'mlp_h_to_4h', 'mlp_4h_to_h', 'mlp_gate', 'moe_h_to_4h', 'moe_4h_to_h', 'moe_gate'], max_rank=32), quant_config=None, gpus_per_node=1, hf_config={'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'attention_dropout': 0.0, 'bos_token_id': 128000, 'eos_token_id': [128001, 128008, 128009], 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 14336, 'max_position_embeddings': 131072, 'mlp_bias': False, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 8, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'rope_theta': 500000.0, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.42.3', 'use_cache': True, 'vocab_size': 128256}, max_prompt_embedding_table_size=None, max_encoder_input_len=None)\n",
            "INFO 2025-07-16 19:27:31.485 utils.py:270] Building engine from non-NeMo checkpoint rank 0 with config ConstrainedTrtllmConfig(dtype='bfloat16', pp_size=1, tp_size=1, max_batch_size=64, max_seq_len=131072, max_num_tokens=8192, lora=ConstrainedTrtllmLoraConfig(enable=True, modules=['attn_q', 'attn_k', 'attn_v', 'attn_dense', 'mlp_h_to_4h', 'mlp_4h_to_h', 'mlp_gate', 'moe_h_to_4h', 'moe_4h_to_h', 'moe_gate'], max_rank=32), quant_config=None, gpus_per_node=1, hf_config={'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'attention_dropout': 0.0, 'bos_token_id': 128000, 'eos_token_id': [128001, 128008, 128009], 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 14336, 'max_position_embeddings': 131072, 'mlp_bias': False, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 8, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'rope_theta': 500000.0, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.42.3', 'use_cache': True, 'vocab_size': 128256}, max_prompt_embedding_table_size=None, max_encoder_input_len=None)\n",
            "INFO 2025-07-16 19:28:50.314 utils.py:380] Using 3019898880 bytes of gpu memory for PEFT cache\n",
            "INFO 2025-07-16 19:28:59.107 synchronizer_peft.py:41] Initializing the models synchronizer ...\n",
            "INFO 2025-07-16 19:28:59.117 synchronizer_lora.py:37] LoRA synchronizer successfully initialized!\n",
            "INFO 2025-07-16 19:28:59.117 synchronizer_custom_model.py:56] CustomModel synchronizer successfully initialized!\n",
            "INFO 2025-07-16 19:28:59.117 synchronizer_lora.py:74] Synchronizing LoRA models with local LoRA directory ...\n",
            "INFO 2025-07-16 19:28:59.124 synchronizer_lora.py:86] Done synchronizing LoRA models with local LoRA directory\n",
            "INFO 2025-07-16 19:28:59.125 synchronizer_custom_model.py:66] Synchronizing CustomModels models with local directory ...\n",
            "INFO 2025-07-16 19:28:59.125 synchronizer_custom_model.py:82] Done synchronizing CustomModels models with local directory\n",
            "INFO 2025-07-16 19:28:59.129 base.py:895] Added job \"LoRAModelSynchronizer.synchronize\" to job store \"default\"\n",
            "INFO 2025-07-16 19:28:59.130 base.py:895] Added job \"CustomModelSynchronizer.synchronize\" to job store \"default\"\n"
          ]
        }
      ],
      "source": [
        "# Wait for NIM to be ready\n",
        "def wait_for_nim(base_url, timeout=300):\n",
        "    print(\"‚è≥ Waiting for NIM to initialize...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            response = requests.get(f\"{base_url}/v1/health/ready\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n‚úÖ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(5)\n",
        "    \n",
        "    print(\"\\n‚úó Timeout waiting for NIM\")\n",
        "    return False\n",
        "\n",
        "if wait_for_nim(base_url):\n",
        "    # Check logs for LoRA loading\n",
        "    print(\"\\nüìã Checking LoRA synchronization logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | grep -i \"lora\\|peft\\|adapter\\|synchroniz\" | tail -20\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  NIM is taking longer than expected. Checking logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | tail -30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Available Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The moment of truth! Let's see what models are available.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "You should see TWO models:\n",
        "1. meta/llama3-8b-instruct - the base model\n",
        "2. customer_support_lora - your fine-tuned adapter\n",
        "\n",
        "If you only see the base model, give it another minute and run again. NIM might still be scanning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Available models:\n",
            "==================================================\n",
            "\n",
            "Found 2 model(s):\n",
            "\n",
            "  ‚Ä¢ meta/llama-3.1-8b-instruct\n",
            "  ‚Ä¢ customer_support_lora\n",
            "    Type: LoRA adapter\n",
            "    ‚ú® Your custom model is ready!\n",
            "\n",
            "‚úÖ Both base model and LoRA adapter are available!\n"
          ]
        }
      ],
      "source": [
        "# Check available models\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/v1/models\")\n",
        "    if response.status_code == 200:\n",
        "        models = response.json()\n",
        "        print(\"üìã Available models:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        model_count = len(models.get('data', []))\n",
        "        print(f\"\\nFound {model_count} model(s):\\n\")\n",
        "        \n",
        "        for model in models.get('data', []):\n",
        "            model_id = model.get('id', 'unknown')\n",
        "            print(f\"  ‚Ä¢ {model_id}\")\n",
        "            if model_id == \"meta/llama3-8b-instruct\":\n",
        "                print(\"    Type: Base model\")\n",
        "            elif \"lora\" in model_id.lower():\n",
        "                print(\"    Type: LoRA adapter\")\n",
        "                print(\"    ‚ú® Your custom model is ready!\")\n",
        "        \n",
        "        if model_count == 1:\n",
        "            print(\"\\n‚ö†Ô∏è  Only base model found. LoRA may still be loading...\")\n",
        "            print(\"Wait 30 seconds and run this cell again.\")\n",
        "        elif model_count > 1:\n",
        "            print(\"\\n‚úÖ Both base model and LoRA adapter are available!\")\n",
        "    else:\n",
        "        print(f\"Error: Status code {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to NIM: {e}\")\n",
        "    print(\"\\nMake sure the container is running and healthy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test Your LoRA Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the exciting part - let's test both models with the same query!\n",
        "\n",
        "Watch the difference:\n",
        "- Base model: Generic, helpful but not specific\n",
        "- LoRA model: Uses your training data, knows your policies\n",
        "\n",
        "This is the power of fine-tuning - domain-specific responses without training a whole new model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Test Query: My order hasn't arrived yet. Order number is 12345.\n",
            "======================================================================\n",
            "\n",
            "ü§ñ BASE MODEL RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "I'd be happy to help you track down your order. To assist you further, I'll need to know a few more details. Here are a few questions:\n",
            "\n",
            "1. Who did you place the order with? Was it an online retailer, a physical store, or a marketplace like Amazon or eBay?\n",
            "2. When did you place the order? Was it yesterday, last week, or a few days ago?\n",
            "3. Have you checked the estimated delivery date and the tracking status on the order confirmation email or the retailer's website?\n",
            "\n",
            "Please let me know the answers to these questions, and I'll do my best to help you locate your order and find out what's going on!\n",
            "\n",
            "üéØ LORA MODEL RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "I'd be happy to help you with your order #12345. Can you please tell me a little more about the order, such as the date you placed it, the items you ordered, and the shipping method you chose? This will help me look into the status of your order right away.\n",
            "\n",
            "======================================================================\n",
            "üí° Notice the difference? The LoRA model provides more specific,\n",
            "   policy-aware responses based on your training data!\n"
          ]
        }
      ],
      "source": [
        "# Test function\n",
        "def test_model(model_name, query):\n",
        "    \"\"\"Test a model with a query\"\"\"\n",
        "    url = f\"{base_url}/v1/chat/completions\"\n",
        "    \n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": [\n",
        "            # {\n",
        "            #     \"role\": \"system\",\n",
        "            #     \"content\": \"You are a helpful customer support assistant.\"\n",
        "            # },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        else:\n",
        "            return f\"Error: {response.status_code} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Test query\n",
        "test_query = \"My order hasn't arrived yet. Order number is 12345.\"\n",
        "\n",
        "print(\"üß™ Test Query:\", test_query)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test base model\n",
        "print(\"\\nü§ñ BASE MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "base_response = test_model(\"meta/llama-3.1-8b-instruct\", test_query)\n",
        "print(base_response)\n",
        "\n",
        "# Test LoRA model\n",
        "print(\"\\nüéØ LORA MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "lora_response = test_model(\"customer_support_lora\", test_query)\n",
        "print(lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üí° Notice the difference? The LoRA model provides more specific,\")\n",
        "print(\"   policy-aware responses based on your training data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Multiple Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test a few more scenarios, though not all would work because we had so little training data and training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Multiple Scenarios\n",
            "================================================================================\n",
            "\n",
            "üìå Scenario 1: How do I reset my password?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Base Model:\n",
            "Please check the help center for that type of information: https://www.meta.com/help/\n",
            "\n",
            "LoRA Model:\n",
            "I'd be happy to help you reset your password. To get started, could you please tell me a little more about your account? What is your username or email address associated with your account?\n",
            "\n",
            "üìå Scenario 2: How can I return my item?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "To get the most accurate return information for your item, could you please provide me with more details, such as:\n",
            "\n",
            "* The item you'd like to return\n",
            "* Where you purchased the item\n",
            "* Your location (coun...\n",
            "\n",
            "LoRA Model:\n",
            "To return an item, you'll need to follow the return policy of the store or retailer. Here's a general steps to return an item:\n",
            "\n",
            "1. **Check the return policy**: Look for the return policy on the store'...\n",
            "\n",
            "üìå Scenario 3: I received a damaged product. What should I do?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "Receiving a damaged product can be frustrating. Here's a step-by-step guide to help you handle the situation:\n",
            "\n",
            "**Immediate Action**\n",
            "\n",
            "1. **Take photos**: Document the damage by taking clear, high-quali...\n",
            "\n",
            "LoRA Model:\n",
            "Sorry to hear that you received a damaged product. Here's a step-by-step guide to help you resolve the issue:\n",
            "\n",
            "1. **Document the damage**: Take photos or videos of the damage from multiple angles. Thi...\n",
            "\n",
            "üìå Scenario 4: Does your shop offer international shipping?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "I'm a large language model, I don't have a physical shop. I exist solely as a digital entity, so I don't offer shipping or physical products. I'm here to provide information, answer questions, and hel...\n",
            "\n",
            "LoRA Model:\n",
            "I'm happy to help, but I don't have a shop to offer shipping from. I'm an AI assistant, and I don't have a physical presence or products to sell. I exist to provide information and answer questions to...\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Testing complete! Your LoRA adapter is working perfectly!\n"
          ]
        }
      ],
      "source": [
        "# Test multiple scenarios\n",
        "test_scenarios = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"How can I return my item?\",\n",
        "    \"I received a damaged product. What should I do?\",\n",
        "    \"Does your shop offer international shipping?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Multiple Scenarios\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(test_scenarios, 1):\n",
        "    print(f\"\\nüìå Scenario {i}: {scenario}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Test both models\n",
        "    base_response = test_model(\"meta/llama-3.1-8b-instruct\", scenario)\n",
        "    lora_response = test_model(\"customer_support_lora\", scenario)\n",
        "    \n",
        "    print(\"\\nBase Model:\")\n",
        "    print(base_response[:200] + \"...\" if len(base_response) > 200 else base_response)\n",
        "    \n",
        "    print(\"\\nLoRA Model:\")\n",
        "    print(lora_response[:200] + \"...\" if len(lora_response) > 200 else lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Testing complete! Your LoRA adapter is working perfectly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully deployed a LoRA adapter with NVIDIA NIM! Key takeaways:\n",
        "\n",
        "‚úÖ **Use Docker volumes** for reliable deployment on cloud GPUs  \n",
        "‚úÖ **Follow the directory structure** - each LoRA in its own subdirectory  \n",
        "‚úÖ **NIM handles the complexity** - automatic discovery, optimized serving  \n",
        "\n",
        "### Resources\n",
        "\n",
        "- [NVIDIA NIM Documentation](https://docs.nvidia.com/nim/)\n",
        "- [NeMo Framework](https://github.com/NVIDIA/NeMo)\n",
        "- [NGC Catalog](https://catalog.ngc.nvidia.com)\n",
        "\n",
        "Happy NIM-ing! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
