{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ NIM Workshop Setup Guide\n",
        "\n",
        "Welcome to the NVIDIA NIM Workshop! This notebook will help you set up everything needed for working with **Llama 3.1 8B Instruct**.\n",
        "\n",
        "## üìã Quick Start\n",
        "\n",
        "1. **Run cell 1**: Set up your API keys\n",
        "2. **Run cell 2**: Check prerequisites  \n",
        "3. **Run cell 3**: Download Llama 3.1 8B model\n",
        "4. **Run cell 4**: Verify setup\n",
        "\n",
        "## ü§ñ Model Information\n",
        "\n",
        "**Llama 3.1 8B Instruct** (~15GB)\n",
        "- Perfect size for demos and LoRA fine-tuning\n",
        "- Standard NeMo format compatible with training scripts\n",
        "- Works with standard NGC access\n",
        "\n",
        "## üõ†Ô∏è Prerequisites\n",
        "\n",
        "- **NGC Account**: Free account at [ngc.nvidia.com](https://ngc.nvidia.com)\n",
        "- **NGC API Key**: Generate at [ngc.nvidia.com/setup/api-key](https://ngc.nvidia.com/setup/api-key)\n",
        "- **NVIDIA API Key**: For cloud NIMs from [build.nvidia.com](https://build.nvidia.com)\n",
        "- **Docker**: For local NIM deployment\n",
        "- **25GB+ disk space**: For model and container\n",
        "\n",
        "Let's get started! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Set Up Your API Keys\n",
        "\n",
        "You'll need two API keys for this workshop:\n",
        "\n",
        "1. **NGC API Key** - To download the model\n",
        "2. **NVIDIA API Key** - To use cloud-hosted NIMs\n",
        "\n",
        "Run the cell below and enter your keys when prompted:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê API Key Setup\n",
            "\n",
            "Enter your NGC API Key (for model downloads):\n",
            "Get one at: https://ngc.nvidia.com/setup/api-key\n",
            "\n",
            "Enter your NVIDIA API Key (for cloud NIMs):\n",
            "Get one at: https://build.nvidia.com\n",
            "\n",
            "‚úÖ API keys configured!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"üîê API Key Setup\\n\")\n",
        "\n",
        "# Get NGC API Key\n",
        "print(\"Enter your NGC API Key (for model downloads):\")\n",
        "print(\"Get one at: https://ngc.nvidia.com/setup/api-key\")\n",
        "ngc_key = getpass.getpass(\"NGC API Key: \")\n",
        "\n",
        "# Get NVIDIA API Key for cloud NIMs\n",
        "print(\"\\nEnter your NVIDIA API Key (for cloud NIMs):\")\n",
        "print(\"Get one at: https://build.nvidia.com\")\n",
        "nvidia_key = getpass.getpass(\"NVIDIA API Key: \")\n",
        "\n",
        "# Save to environment\n",
        "os.environ['NGC_API_KEY'] = ngc_key\n",
        "os.environ['NGC_CLI_API_KEY'] = ngc_key  # New environment variable name\n",
        "os.environ['NVIDIA_API_KEY'] = nvidia_key\n",
        "\n",
        "# Save to .env file\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"NGC_API_KEY={ngc_key}\\n\")\n",
        "    f.write(f\"NGC_CLI_API_KEY={ngc_key}\\n\")\n",
        "    f.write(f\"NVIDIA_API_KEY={nvidia_key}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ API keys configured!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Check Prerequisites\n",
        "\n",
        "Let's verify all required tools are installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "print(\"üîç Checking prerequisites...\\n\")\n",
        "\n",
        "# 1. Check Docker\n",
        "try:\n",
        "    docker_version = subprocess.check_output(['docker', '--version'], text=True).strip()\n",
        "    print(f\"‚úÖ Docker: {docker_version}\")\n",
        "except:\n",
        "    print(\"‚ùå Docker: Not installed - get it from https://docs.docker.com/get-docker/\")\n",
        "\n",
        "# 2. Check/Install NGC CLI\n",
        "if os.path.exists('ngc-cli/ngc'):\n",
        "    result = subprocess.run(['./ngc-cli/ngc', '--version'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ NGC CLI: {result.stdout.strip()}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  NGC CLI found but not working\")\n",
        "else:\n",
        "    print(\"üì• Installing NGC CLI...\")\n",
        "    os.system('wget -q https://ngc.nvidia.com/downloads/ngccli_linux.zip')\n",
        "    os.system('unzip -q ngccli_linux.zip')\n",
        "    os.system('chmod +x ngc-cli/ngc')\n",
        "    os.system('rm ngccli_linux.zip')\n",
        "    print(\"‚úÖ NGC CLI installed\")\n",
        "\n",
        "# 3. Check GPU (optional)\n",
        "try:\n",
        "    gpu = subprocess.check_output(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], text=True).strip()\n",
        "    print(f\"‚úÖ GPU: {gpu}\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è  No GPU detected (you can still use cloud NIMs)\")\n",
        "\n",
        "# 4. Check disk space\n",
        "free_gb = shutil.disk_usage(\"/\").free // (2**30)\n",
        "print(f\"‚úÖ Disk space: {free_gb} GB free\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Download Llama 3.1 8B Model and NIM Container\n",
        "\n",
        "This will download:\n",
        "- **Llama 3.1 8B Instruct** (~15 GB) - The model for LoRA fine-tuning\n",
        "- **NIM Docker Container** - For local deployment\n",
        "\n",
        "‚è±Ô∏è Takes 10-30 minutes depending on internet speed\n",
        "\n",
        "### üìù Note about NGC CLI Output\n",
        "The NGC CLI shows detailed progress information. Don't worry about all the progress bars and symbols - just look for:\n",
        "- `Download status: Completed` - This means success!\n",
        "- The download summary at the bottom shows total files and size transferred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading workshop assets...\n",
            "\n",
            "üì• Downloading Llama 3.1 8B model (~15 GB)...\n",
            "   This will take 10-30 minutes depending on your connection\n",
            "   Download starting...\n",
            "\n",
            "‚úÖ Model download completed successfully!                    \n",
            "\n",
            "   Total files downloaded: 1\n",
            "   Total transferred: 14.96 GB\n",
            "\n",
            "üìÇ Model location: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
            "üíæ Model size: 15.0 GB\n",
            "\n",
            "üì• Checking Docker container...\n",
            "‚úÖ Docker container already downloaded\n",
            "\n",
            "‚úÖ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "\n",
        "print(\"üì• Downloading workshop assets...\\n\")\n",
        "\n",
        "# Check prerequisites\n",
        "if not os.environ.get('NGC_API_KEY'):\n",
        "    print(\"‚ùå Please run the API key setup cell first\")\n",
        "else:\n",
        "    # 1. Download Llama 3.1 8B model\n",
        "    model_dir = \"lora_tutorial/models/llama-3_1-8b-instruct\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Check if already downloaded\n",
        "    nemo_files = glob.glob(f\"{model_dir}/**/*.nemo\", recursive=True)\n",
        "    complete_model = None\n",
        "    \n",
        "    for nf in nemo_files:\n",
        "        size_gb = os.path.getsize(nf) / (1024**3)\n",
        "        if size_gb > 10:  # Full model should be ~15GB\n",
        "            complete_model = nf\n",
        "            break\n",
        "    \n",
        "    if complete_model:\n",
        "        print(\"‚úÖ Model already downloaded!\")\n",
        "        print(f\"üìÇ Location: {complete_model}\")\n",
        "        print(f\"üíæ Size: {os.path.getsize(complete_model) / (1024**3):.1f} GB\")\n",
        "    else:\n",
        "        print(\"üì• Downloading Llama 3.1 8B model (~15 GB)...\")\n",
        "        print(\"   This will take 10-30 minutes depending on your connection\")\n",
        "        print(\"   Download starting...\\n\")\n",
        "        \n",
        "        # Set environment variable and run download\n",
        "        os.environ['NGC_CLI_API_KEY'] = os.environ['NGC_API_KEY']\n",
        "        \n",
        "        # Build download command\n",
        "        cmd = f'cd {model_dir} && ../../../ngc-cli/ngc registry model download-version \"nvidia/nemo/llama-3_1-8b-nemo:1.0\" --org nvidia'\n",
        "        \n",
        "        # Run with subprocess to capture but simplify output\n",
        "        # Function to show simple progress indicator\n",
        "        download_complete = False\n",
        "        def show_progress():\n",
        "            symbols = [\"‚†ã\", \"‚†ô\", \"‚†π\", \"‚†∏\", \"‚†º\", \"‚†¥\", \"‚†¶\", \"‚†ß\", \"‚†á\", \"‚†è\"]\n",
        "            i = 0\n",
        "            while not download_complete:\n",
        "                print(f\"\\r{symbols[i % len(symbols)]} Downloading... (this may take 10-30 minutes)\", end=\"\", flush=True)\n",
        "                time.sleep(0.5)\n",
        "                i += 1\n",
        "        \n",
        "        # Start progress indicator in background\n",
        "        progress_thread = threading.Thread(target=show_progress)\n",
        "        progress_thread.start()\n",
        "        \n",
        "        # Run download\n",
        "        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        stdout, stderr = process.communicate()\n",
        "        result = process.returncode\n",
        "        \n",
        "        # Stop progress indicator\n",
        "        download_complete = True\n",
        "        progress_thread.join()\n",
        "        print(\"\\r\" + \" \" * 60 + \"\\r\", end=\"\")  # Clear the progress line\n",
        "        \n",
        "        if result == 0:\n",
        "            # Extract summary info from stdout\n",
        "            if \"Download status: Completed\" in stdout:\n",
        "                print(\"‚úÖ Model download completed successfully!\\n\")\n",
        "                # Try to extract useful info\n",
        "                for line in stdout.split('\\n'):\n",
        "                    if \"Total files downloaded:\" in line or \"Total transferred:\" in line:\n",
        "                        print(f\"   {line.strip()}\")\n",
        "            else:\n",
        "                print(\"‚úÖ Model download complete!\")\n",
        "            \n",
        "            # Find the downloaded file\n",
        "            nemo_files = glob.glob(f\"{model_dir}/**/*.nemo\", recursive=True)\n",
        "            if nemo_files:\n",
        "                print(f\"\\nüìÇ Model location: {nemo_files[0]}\")\n",
        "                print(f\"üíæ Model size: {os.path.getsize(nemo_files[0]) / (1024**3):.1f} GB\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Download failed. Please check:\")\n",
        "            print(\"   - Your NGC API key is valid\")\n",
        "            print(\"   - You have internet connectivity\")\n",
        "            print(\"   - You have enough disk space (need ~15GB)\")\n",
        "    \n",
        "    # 2. Download Docker container\n",
        "    print(\"\\nüì• Checking Docker container...\")\n",
        "    image = \"nvcr.io/nim/meta/llama3-8b-instruct:latest\"\n",
        "    \n",
        "    # Simple check using docker images\n",
        "    result = subprocess.run(f\"docker images {image} --format '{{{{.Repository}}}}:{{{{.Tag}}}}'\", \n",
        "                          shell=True, capture_output=True, text=True)\n",
        "    \n",
        "    if image in result.stdout:\n",
        "        print(\"‚úÖ Docker container already downloaded\")\n",
        "    else:\n",
        "        print(\"üì• Pulling NIM Docker container...\")\n",
        "        # Docker login\n",
        "        os.system(f\"echo {os.environ['NGC_API_KEY']} | docker login nvcr.io -u \\\\$oauthtoken --password-stdin >/dev/null 2>&1\")\n",
        "        \n",
        "        # Pull container\n",
        "        result = os.system(f\"docker pull {image}\")\n",
        "        if result == 0:\n",
        "            print(\"‚úÖ Docker container downloaded\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Container download failed - check Docker and NGC access\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Verify Setup\n",
        "\n",
        "Let's make sure everything is ready for the workshop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Verifying setup...\n",
            "\n",
            "‚úÖ Model downloaded\n",
            "‚úÖ Docker container\n",
            "‚úÖ NGC API Key\n",
            "‚úÖ NVIDIA API Key\n",
            "\n",
            "üì° Cloud API: ‚úÖ Connected\n",
            "\n",
            "üéâ All set! You're ready for the NIM workshop!\n",
            "\n",
            "üìÇ Model location: lora_tutorial/models/llama-3_1-8b-instruct/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\n",
            "üíæ Model size: 15.0 GB\n",
            "üê≥ Container: nvcr.io/nim/meta/llama3-8b-instruct:latest\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç Verifying setup...\\n\")\n",
        "\n",
        "# Quick checks - check if any .nemo file exists in the model directory or subdirectories\n",
        "import glob\n",
        "nemo_files = glob.glob(\"lora_tutorial/models/llama-3_1-8b-instruct/**/*.nemo\", recursive=True)\n",
        "# Check if we have a complete model (>10GB)\n",
        "model_downloaded = False\n",
        "for nf in nemo_files:\n",
        "    if os.path.getsize(nf) / (1024**3) > 10:\n",
        "        model_downloaded = True\n",
        "        break\n",
        "\n",
        "checks = {\n",
        "    \"Model downloaded\": model_downloaded,\n",
        "    \"Docker container\": bool(subprocess.run(['docker', 'images', '-q', 'nvcr.io/nim/meta/llama3-8b-instruct:latest'],\n",
        "                                       capture_output=True, text=True).stdout.strip()),\n",
        "    \"NGC API Key\": bool(os.environ.get('NGC_API_KEY')),\n",
        "    \"NVIDIA API Key\": bool(os.environ.get('NVIDIA_API_KEY'))\n",
        "}\n",
        "\n",
        "# Print results\n",
        "for item, status in checks.items():\n",
        "    print(f\"{'‚úÖ' if status else '‚ùå'} {item}\")\n",
        "\n",
        "# Test cloud API connection\n",
        "try:\n",
        "    import requests\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY', '')}\"}\n",
        "    response = requests.get(\"https://integrate.api.nvidia.com/v1/models\", headers=headers, timeout=5)\n",
        "    print(f\"\\nüì° Cloud API: {'‚úÖ Connected' if response.status_code == 200 else f'‚ö†Ô∏è  Status {response.status_code}'}\")\n",
        "except:\n",
        "    print(\"\\nüì° Cloud API: ‚ö†Ô∏è  Could not test connection\")\n",
        "\n",
        "# Summary\n",
        "if all(checks.values()):\n",
        "    print(\"\\nüéâ All set! You're ready for the NIM workshop!\")\n",
        "    # Find the actual model file (look in subdirectories)\n",
        "    complete_models = [nf for nf in nemo_files if os.path.getsize(nf) / (1024**3) > 10]\n",
        "    if complete_models:\n",
        "        print(f\"\\nüìÇ Model location: {complete_models[0]}\")\n",
        "        print(f\"üíæ Model size: {os.path.getsize(complete_models[0]) / (1024**3):.1f} GB\")\n",
        "    else:\n",
        "        print(\"\\nüìÇ Model location: Not found - please run the download cell\")\n",
        "    print(\"üê≥ Container: nvcr.io/nim/meta/llama3-8b-instruct:latest\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some components missing - please check above\")\n",
        "    \n",
        "# Create data directory for later use\n",
        "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "### Model Information\n",
        "- **Model**: Llama 3.1 8B Instruct \n",
        "- **Format**: Standard NeMo checkpoint (`.nemo` file)\n",
        "- **Location**: `lora_tutorial/models/llama-3_1-8b-instruct/*.nemo` (exact filename depends on download)\n",
        "\n",
        "### What's Next?\n",
        "1. **01_NIM_API_Tutorial.ipynb** - Learn to use cloud-hosted NIMs\n",
        "2. **02_Local_NIM_Deployment.ipynb** - Deploy NIMs locally with Docker\n",
        "3. **03_LoRA_Training.ipynb** - Fine-tune the model with LoRA\n",
        "4. **04_Deploy_LoRA_with_NIM.ipynb** - Deploy your fine-tuned model\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "**If download fails:**\n",
        "- Verify your NGC API key is correct\n",
        "- Check your internet connection\n",
        "- Try running the download cell again (downloads can be resumed)\n",
        "\n",
        "**Docker issues:**\n",
        "- Make sure Docker daemon is running\n",
        "- On Linux: `sudo systemctl start docker`\n",
        "- Test with: `docker run hello-world`\n",
        "\n",
        "**Understanding the download output:**\n",
        "The NGC CLI shows detailed progress with many symbols and progress bars. This is normal! \n",
        "The key indicators of success are:\n",
        "- `Download status: Completed`\n",
        "- Summary showing total files and GB transferred\n",
        "- Exit code 0 (success)\n",
        "\n",
        "**Model format:**\n",
        "The Llama 3.1 model uses standard NeMo format:\n",
        "- Single `.nemo` file containing all weights and configuration\n",
        "- Compatible with NeMo training scripts without modifications\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to start?** Open `01_NIM_API_Tutorial.ipynb` to begin the workshop! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
