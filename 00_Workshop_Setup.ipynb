{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ NIM Workshop Setup Guide\n",
        "\n",
        "Welcome to the NVIDIA NIM Workshop! This notebook will help you set up everything needed for working with **Llama 3.2 1B Instruct**.\n",
        "\n",
        "## üìã Quick Start\n",
        "\n",
        "1. **Run cell 1**: Set up your API keys\n",
        "2. **Run cell 2**: Check prerequisites  \n",
        "3. **Run cell 3**: Download Llama 3.2 1B model\n",
        "4. **Run cell 4**: Verify setup\n",
        "\n",
        "## ü§ñ Model Information\n",
        "\n",
        "**Llama 3.2 1B Instruct** (2.3GB)\n",
        "- Perfect size for demos and LoRA fine-tuning\n",
        "- NeMo 2 format with distributed checkpoints\n",
        "- Works with standard NGC access\n",
        "\n",
        "## üõ†Ô∏è Prerequisites\n",
        "\n",
        "- **NGC Account**: Free account at [ngc.nvidia.com](https://ngc.nvidia.com)\n",
        "- **NGC API Key**: Generate at [ngc.nvidia.com/setup/api-key](https://ngc.nvidia.com/setup/api-key)\n",
        "- **NVIDIA API Key**: For cloud NIMs from [build.nvidia.com](https://build.nvidia.com)\n",
        "- **Docker**: For local NIM deployment\n",
        "- **15GB+ disk space**: For model and container\n",
        "\n",
        "Let's get started! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Set Up Your API Keys\n",
        "\n",
        "You'll need two API keys for this workshop:\n",
        "\n",
        "1. **NGC API Key** - To download the model\n",
        "2. **NVIDIA API Key** - To use cloud-hosted NIMs\n",
        "\n",
        "Run the cell below and enter your keys when prompted:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê API Key Setup\n",
            "\n",
            "Enter your NGC API Key (for model downloads):\n",
            "Get one at: https://ngc.nvidia.com/setup/api-key\n",
            "\n",
            "Enter your NVIDIA API Key (for cloud NIMs):\n",
            "Get one at: https://build.nvidia.com\n",
            "\n",
            "‚úÖ API keys configured!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"üîê API Key Setup\\n\")\n",
        "\n",
        "# Get NGC API Key\n",
        "print(\"Enter your NGC API Key (for model downloads):\")\n",
        "print(\"Get one at: https://ngc.nvidia.com/setup/api-key\")\n",
        "ngc_key = getpass.getpass(\"NGC API Key: \")\n",
        "\n",
        "# Get NVIDIA API Key for cloud NIMs\n",
        "print(\"\\nEnter your NVIDIA API Key (for cloud NIMs):\")\n",
        "print(\"Get one at: https://build.nvidia.com\")\n",
        "nvidia_key = getpass.getpass(\"NVIDIA API Key: \")\n",
        "\n",
        "# Save to environment\n",
        "os.environ['NGC_API_KEY'] = ngc_key\n",
        "os.environ['NGC_CLI_API_KEY'] = ngc_key  # New environment variable name\n",
        "os.environ['NVIDIA_API_KEY'] = nvidia_key\n",
        "\n",
        "# Save to .env file\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(f\"NGC_API_KEY={ngc_key}\\n\")\n",
        "    f.write(f\"NGC_CLI_API_KEY={ngc_key}\\n\")\n",
        "    f.write(f\"NVIDIA_API_KEY={nvidia_key}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ API keys configured!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Check Prerequisites\n",
        "\n",
        "Let's verify all required tools are installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking prerequisites...\n",
            "\n",
            "‚úÖ Docker: Docker version 27.3.1, build ce12230\n",
            "‚úÖ NGC CLI: NGC CLI 3.160.1\n",
            "‚úÖ GPU: NVIDIA A100-SXM4-80GB\n",
            "‚úÖ Disk space: 307 GB free\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "print(\"üîç Checking prerequisites...\\n\")\n",
        "\n",
        "# 1. Check Docker\n",
        "try:\n",
        "    docker_version = subprocess.check_output(['docker', '--version'], text=True).strip()\n",
        "    print(f\"‚úÖ Docker: {docker_version}\")\n",
        "except:\n",
        "    print(\"‚ùå Docker: Not installed - get it from https://docs.docker.com/get-docker/\")\n",
        "\n",
        "# 2. Check/Install NGC CLI\n",
        "if os.path.exists('ngc-cli/ngc'):\n",
        "    result = subprocess.run(['./ngc-cli/ngc', '--version'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ NGC CLI: {result.stdout.strip()}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  NGC CLI found but not working\")\n",
        "else:\n",
        "    print(\"üì• Installing NGC CLI...\")\n",
        "    os.system('wget -q https://ngc.nvidia.com/downloads/ngccli_linux.zip')\n",
        "    os.system('unzip -q ngccli_linux.zip')\n",
        "    os.system('chmod +x ngc-cli/ngc')\n",
        "    os.system('rm ngccli_linux.zip')\n",
        "    print(\"‚úÖ NGC CLI installed\")\n",
        "\n",
        "# 3. Check GPU (optional)\n",
        "try:\n",
        "    gpu = subprocess.check_output(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], text=True).strip()\n",
        "    print(f\"‚úÖ GPU: {gpu}\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è  No GPU detected (you can still use cloud NIMs)\")\n",
        "\n",
        "# 4. Check disk space\n",
        "free_gb = shutil.disk_usage(\"/\").free // (2**30)\n",
        "print(f\"‚úÖ Disk space: {free_gb} GB free\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Download Llama 3.2 1B Model and NIM Container\n",
        "\n",
        "This will download:\n",
        "- **Llama 3.2 1B Instruct** (2.3 GB) - The model for LoRA fine-tuning\n",
        "- **NIM Docker Container** - For local deployment\n",
        "\n",
        "‚è±Ô∏è Takes 5-15 minutes depending on internet speed\n",
        "\n",
        "### üìù Note about NGC CLI Output\n",
        "The NGC CLI shows detailed progress information. Don't worry about all the progress bars and symbols - just look for:\n",
        "- `Download status: Completed` - This means success!\n",
        "- The download summary at the bottom shows total files and size transferred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading workshop assets...\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading Llama 3.2 1B Instruct model (2.3 GB)...\n",
            "   This may take a few minutes depending on your connection speed...\n",
            "   Please wait...\n",
            "\n",
            "‚è≥ Download in progress...\n",
            "   Download status: Completed\n",
            "   Total files downloaded: 10\n",
            "   Total transferred: 2.32 GB\n",
            "   Duration taken: 5s\n",
            "\n",
            "============================================================\n",
            "‚úÖ MODEL DOWNLOADED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "üìÇ Model location: lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\n",
            "\n",
            "üìÅ Downloaded files:\n",
            "llama-3_2-1b-instruct_v2.0/\n",
            "  weights/\n",
            "    .metadata\n",
            "    __0_0.distcp\n",
            "    __0_1.distcp\n",
            "    metadata.json\n",
            "    common.pt\n",
            "  context/\n",
            "    io.json\n",
            "    model.yaml\n",
            "    nemo_tokenizer/\n",
            "      tokenizer_config.json\n",
            "      tokenizer.json\n",
            "      special_tokens_map.json\n",
            "\n",
            "üì• Checking Docker container...\n",
            "‚úÖ Docker container already downloaded\n",
            "\n",
            "‚úÖ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"üì• Downloading workshop assets...\\n\")\n",
        "\n",
        "# Check prerequisites\n",
        "if not os.environ.get('NGC_API_KEY'):\n",
        "    print(\"‚ùå Please run the API key setup cell first\")\n",
        "else:\n",
        "    # Configure NGC CLI\n",
        "    os.system(f\"./ngc-cli/ngc config set --api-key {os.environ['NGC_API_KEY']} >/dev/null 2>&1\")\n",
        "    \n",
        "    # 1. Download Llama 3.2 1B model\n",
        "    model_dir = \"lora_tutorial/models/llama-3_2-1b-instruct\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Check if already downloaded\n",
        "    model_subdir = f\"{model_dir}/llama-3_2-1b-instruct_v2.0\"\n",
        "    if os.path.exists(model_subdir):\n",
        "        print(\"‚úÖ Model already downloaded!\")\n",
        "        print(f\"üìÇ Location: {model_subdir}\")\n",
        "        # Check size\n",
        "        total_size = 0\n",
        "        for root, dirs, files in os.walk(model_subdir):\n",
        "            for file in files:\n",
        "                total_size += os.path.getsize(os.path.join(root, file))\n",
        "        print(f\"üíæ Size: {total_size/(1024**3):.1f} GB\")\n",
        "    else:\n",
        "        print(\"üì• Downloading Llama 3.2 1B Instruct model (2.3 GB)...\")\n",
        "        print(\"   This may take a few minutes depending on your connection speed...\")\n",
        "        print(\"   Please wait...\\n\")\n",
        "        \n",
        "        # Download with correct model name, capturing output to reduce verbosity\n",
        "        cmd = f\"cd {model_dir} && ../../../ngc-cli/ngc registry model download-version nvidia/nemo/llama-3_2-1b-instruct:2.0\"\n",
        "        \n",
        "        # Run command and capture output\n",
        "        import subprocess\n",
        "        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        \n",
        "        # Show simplified progress\n",
        "        print(\"‚è≥ Download in progress...\")\n",
        "        stdout, stderr = process.communicate()\n",
        "        result = process.returncode\n",
        "        \n",
        "        # Check if successful and show summary\n",
        "        if result == 0:\n",
        "            # Extract key information from output\n",
        "            if \"Download status: Completed\" in stdout:\n",
        "                # Parse the summary from stdout\n",
        "                lines = stdout.split('\\n')\n",
        "                summary_started = False\n",
        "                for line in lines:\n",
        "                    if \"----\" in line and not summary_started:\n",
        "                        summary_started = True\n",
        "                    elif summary_started and (\"Download status:\" in line or \n",
        "                                            \"Total files downloaded:\" in line or \n",
        "                                            \"Total transferred:\" in line or\n",
        "                                            \"Duration taken:\" in line):\n",
        "                        print(f\"   {line.strip()}\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"‚úÖ MODEL DOWNLOADED SUCCESSFULLY!\")\n",
        "            print(\"=\"*60)\n",
        "            # The model will be in a subdirectory\n",
        "            model_subdir = f\"{model_dir}/llama-3_2-1b-instruct_v2.0\"\n",
        "            if os.path.exists(model_subdir):\n",
        "                print(f\"\\nüìÇ Model location: {model_subdir}\")\n",
        "                # List what's in the model directory\n",
        "                print(\"\\nüìÅ Downloaded files:\")\n",
        "                for root, dirs, files in os.walk(model_subdir):\n",
        "                    level = root.replace(model_subdir, '').count(os.sep)\n",
        "                    indent = ' ' * 2 * level\n",
        "                    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                    subindent = ' ' * 2 * (level + 1)\n",
        "                    for file in files[:5]:  # Show first 5 files\n",
        "                        print(f\"{subindent}{file}\")\n",
        "                    if len(files) > 5:\n",
        "                        print(f\"{subindent}... and {len(files)-5} more files\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Download failed - please check your NGC API key\")\n",
        "    \n",
        "    # 2. Download Docker container\n",
        "    print(\"\\nüì• Checking Docker container...\")\n",
        "    image = \"nvcr.io/nim/meta/llama-3.2-1b-instruct:latest\"\n",
        "    \n",
        "    # Check if image exists\n",
        "    if os.system(f\"docker images -q {image} 2>/dev/null | grep -q .\") == 0:\n",
        "        print(\"‚úÖ Docker container already downloaded\")\n",
        "    else:\n",
        "        print(\"üì• Downloading NIM Docker container...\")\n",
        "        # Docker login\n",
        "        os.system(f\"echo {os.environ['NGC_API_KEY']} | docker login nvcr.io -u \\\\$oauthtoken --password-stdin >/dev/null 2>&1\")\n",
        "        \n",
        "        # Pull container with cleaner output\n",
        "        print(\"‚è≥ Pulling Docker container...\")\n",
        "        pull_result = subprocess.run(f\"docker pull {image}\", shell=True, capture_output=True, text=True)\n",
        "        if pull_result.returncode == 0:\n",
        "            print(\"‚úÖ Docker container downloaded\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Container download failed - check Docker and NGC access\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Verify Setup\n",
        "\n",
        "Let's make sure everything is ready for the workshop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Verifying setup...\n",
            "\n",
            "‚úÖ Model downloaded\n",
            "‚úÖ Docker container\n",
            "‚úÖ NGC API Key\n",
            "‚úÖ NVIDIA API Key\n",
            "\n",
            "üì° Cloud API: ‚úÖ Connected\n",
            "\n",
            "üéâ All set! You're ready for the NIM workshop!\n",
            "\n",
            "üìÇ Model location: lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0/\n",
            "üê≥ Container: nvcr.io/nim/meta/llama-3.2-1b-instruct:latest\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç Verifying setup...\\n\")\n",
        "\n",
        "# Quick checks\n",
        "checks = {\n",
        "    \"Model downloaded\": os.path.exists(\"lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0\"),\n",
        "    \"Docker container\": bool(subprocess.run(['docker', 'images', '-q', 'nvcr.io/nim/meta/llama-3.2-1b-instruct:latest'],\n",
        "                                       capture_output=True, text=True).stdout.strip()),\n",
        "    \"NGC API Key\": bool(os.environ.get('NGC_API_KEY')),\n",
        "    \"NVIDIA API Key\": bool(os.environ.get('NVIDIA_API_KEY'))\n",
        "}\n",
        "\n",
        "# Print results\n",
        "for item, status in checks.items():\n",
        "    print(f\"{'‚úÖ' if status else '‚ùå'} {item}\")\n",
        "\n",
        "# Test cloud API connection\n",
        "try:\n",
        "    import requests\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY', '')}\"}\n",
        "    response = requests.get(\"https://integrate.api.nvidia.com/v1/models\", headers=headers, timeout=5)\n",
        "    print(f\"\\nüì° Cloud API: {'‚úÖ Connected' if response.status_code == 200 else f'‚ö†Ô∏è  Status {response.status_code}'}\")\n",
        "except:\n",
        "    print(\"\\nüì° Cloud API: ‚ö†Ô∏è  Could not test connection\")\n",
        "\n",
        "# Summary\n",
        "if all(checks.values()):\n",
        "    print(\"\\nüéâ All set! You're ready for the NIM workshop!\")\n",
        "    print(\"\\nüìÇ Model location: lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0/\")\n",
        "    print(\"üê≥ Container: nvcr.io/nim/meta/llama-3.2-1b-instruct:latest\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some components missing - please check above\")\n",
        "    \n",
        "# Create data directory for later use\n",
        "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "### Model Information\n",
        "- **Model**: Llama 3.2 1B Instruct \n",
        "- **Format**: NeMo 2 distributed checkpoint (`.distcp` files)\n",
        "- **Location**: `lora_tutorial/models/llama-3_2-1b-instruct/llama-3_2-1b-instruct_v2.0/`\n",
        "\n",
        "### What's Next?\n",
        "1. **01_NIM_API_Tutorial.ipynb** - Learn to use cloud-hosted NIMs\n",
        "2. **02_Local_NIM_Deployment.ipynb** - Deploy NIMs locally with Docker\n",
        "3. **03_LoRA_Training.ipynb** - Fine-tune the model with LoRA\n",
        "4. **04_Deploy_LoRA_with_NIM.ipynb** - Deploy your fine-tuned model\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "**If download fails:**\n",
        "- Verify your NGC API key is correct\n",
        "- Check your internet connection\n",
        "- Try running the download cell again (downloads can be resumed)\n",
        "\n",
        "**Docker issues:**\n",
        "- Make sure Docker daemon is running\n",
        "- On Linux: `sudo systemctl start docker`\n",
        "- Test with: `docker run hello-world`\n",
        "\n",
        "**Understanding the download output:**\n",
        "The NGC CLI shows detailed progress with many symbols and progress bars. This is normal! \n",
        "The key indicators of success are:\n",
        "- `Download status: Completed`\n",
        "- Summary showing total files and GB transferred\n",
        "- Exit code 0 (success)\n",
        "\n",
        "**Model format:**\n",
        "The Llama 3.2 model uses NeMo 2 format with distributed checkpoints:\n",
        "- `weights/` - Model weights in `.distcp` format\n",
        "- `context/` - Configuration files\n",
        "- This is different from the older single `.nemo` file format\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to start?** Open `01_NIM_API_Tutorial.ipynb` to begin the workshop! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
