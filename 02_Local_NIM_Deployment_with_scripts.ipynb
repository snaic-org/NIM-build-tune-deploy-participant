{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Welcome back! I hope you're excited because we're about to take full control of these powerful AI models.\n",
        "\n",
        "In Part 1, we used NIMs through the cloud - quick, easy, but ultimately on someone else's server. Now we're going to run them on YOUR local hardware.\n",
        "\n",
        "Why go local? Let me give you real scenarios:\n",
        "- **Healthcare company**: 'We can't send patient data to the cloud' - Local NIMs solve this\n",
        "- **Financial services**: 'We need sub-100ms latency for trading' - Local NIMs deliver this\n",
        "- **Defense contractor**: 'We work in air-gapped environments' - Local NIMs enable this\n",
        "- **Startup**: 'We need predictable costs as we scale' - Local NIMs provide this\n",
        "\n",
        "The beauty is, the API is IDENTICAL to what we just used. Your application code doesn't change, just the endpoint.\n",
        "\n",
        "Prerequisites check: #this should depend on what model you want to run\n",
        "- NVIDIA GPU with 24GB+ memory (40GB recommended for Llama 3.1 8B)\n",
        "- Ubuntu 20.04 or 22.04 (WSL2 works too!)\n",
        "- Docker installed\n",
        "- NVIDIA Container Toolkit\n",
        "- 100GB free disk space for models\n",
        "\n",
        "Don't worry if you're missing something - I'll show you how to set it all up!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Local NIM Deployment\n",
        "\n",
        "This notebook will guide you through deploying NVIDIA NIMs locally on your own infrastructure.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA GPU (compute capability ‚â• 7.0)\n",
        "- Docker installed\n",
        "- NVIDIA Container Toolkit\n",
        "- NGC API Key\n",
        "- Sufficient disk space (~50GB per model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's start by verifying our environment is ready. This is crucial - better to catch issues now than during deployment!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"First, let's see what GPU we're working with. This nvidia-smi command is your best friend for GPU debugging.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Let me explain what we're looking at:\n",
        "- GPU Model: You need compute capability 7.0+ (that's RTX 2000 series or newer)\n",
        "- Memory: This determines which models you can run\n",
        "  - 16GB: Llama 3.1 8B, Mistral 7B\n",
        "  - 24GB: Llama 2 13B, Mixtral 8x7B  \n",
        "  - 40GB+: Llama 3.1 70B (with quantization)\n",
        "  - 80GB+: Llama 3.1 70B (full precision)\n",
        "- Driver Version: Should be 525.60.13 or newer\n",
        "- CUDA Version: 12.0 or newer recommended\n",
        "\n",
        "If you see your GPU here, we're in business! If not, check:\n",
        "- Is the GPU properly installed?\n",
        "- Are the NVIDIA drivers installed?\n",
        "- On cloud instances, did you select a GPU instance type?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jul  8 09:21:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0             53W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now let's verify Docker can access your GPU. This tests the NVIDIA Container Toolkit integration.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Perfect! If you see the same nvidia-smi output, Docker has GPU access. \n",
        "\n",
        "If this fails, you need to install NVIDIA Container Toolkit:\n",
        "```bash\n",
        "# Ubuntu/Debian\n",
        "curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
        "curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y nvidia-container-toolkit\n",
        "sudo nvidia-ctk runtime configure --runtime=docker\n",
        "sudo systemctl restart docker\n",
        "```\n",
        "\n",
        "This toolkit is what allows Docker containers to use GPUs. It's the magic that makes local NIMs possible!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jul  8 09:21:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0             53W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Verify Docker and NVIDIA runtime\n",
        "!docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Time to set up NGC (NVIDIA GPU Cloud) access. This is different from the API key we used earlier - this one lets us download the actual NIM containers.\n",
        "\n",
        "[RUN THE CELL - Enter NGC API key when prompted]\n",
        "\n",
        "Let me show you how to get this key:\n",
        "1. Go to ngc.nvidia.com\n",
        "2. Sign in or create account\n",
        "3. Click your username (top right) ‚Üí Setup\n",
        "4. Generate API Key\n",
        "5. Copy and paste here\n",
        "\n",
        "We're also creating a cache directory. This is important - models are LARGE (5-100GB). The cache means:\n",
        "- Download once, use many times\n",
        "- Survive container restarts\n",
        "- Share models between containers\n",
        "- Quick model switching\n",
        "\n",
        "The cache will be at ~/.cache/nim - remember this location!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NIM cache directory: /root/.cache/nim\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Set up environment variables\n",
        "NGC_API_KEY = os.getenv('NGC_API_KEY')\n",
        "if not NGC_API_KEY:\n",
        "    import getpass\n",
        "    NGC_API_KEY = getpass.getpass(\"Enter your NGC API key: \")\n",
        "    os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "# Set cache directory\n",
        "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
        "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
        "print(f\"NIM cache directory: {LOCAL_NIM_CACHE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import getpass\n",
        "\n",
        "# Securely input your API key\n",
        "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
        "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now we authenticate Docker with NGC. This is like 'docker login' for Docker Hub, but for NVIDIA's registry.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "The username is literally '$oauthtoken' - don't change it! The password is your NGC API key.\n",
        "\n",
        "You should see 'Login Succeeded'. This authentication persists, so you only need to do this once per machine.\n",
        "\n",
        "Behind the scenes, this stores credentials in ~/.docker/config.json. In production, you'd use:\n",
        "- Kubernetes secrets\n",
        "- Docker Swarm secrets  \n",
        "- Cloud provider secret managers\n",
        "- HashiCorp Vault\n",
        "\n",
        "But for development, this local auth is perfect!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NGC Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"The moment we've been waiting for - let's deploy our first local NIM! We'll start with Llama 3.1 8B Instruct, a powerful but efficient model.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Login result: Login Succeeded\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{NGC_API_KEY}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "print(\"Login result:\", result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"First, let's clean up any existing containers. This ensures we start fresh and avoid port conflicts.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "The `|| true` means the command succeeds even if there's no container to stop. This is good defensive scripting - always handle the case where your cleanup targets don't exist.\n",
        "\n",
        "In production, you'd do more sophisticated checks:\n",
        "- Gracefully stop services\n",
        "- Save state if needed\n",
        "- Notify monitoring systems\n",
        "- Coordinate with load balancers\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deploy Your First NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Here's where the magic happens. Let me break down this Docker command in detail:\n",
        "\n",
        "`docker run -d`: Run detached (in background)\n",
        "`--name llama3-8b-instruct`: Container name for easy reference\n",
        "`--runtime=nvidia`: Enables GPU access (via NVIDIA Container Toolkit)\n",
        "`--gpus all`: Use all available GPUs (you can specify specific ones)\n",
        "`--shm-size=16GB`: Shared memory for PyTorch (critical for performance!)\n",
        "`-e NGC_API_KEY`: Pass our authentication\n",
        "`-v ~/.cache/nim:/opt/nim/.cache`: Mount cache directory\n",
        "`-u $(id -u)`: Run as current user (avoids permission issues)\n",
        "`-p 8000:8000`: Expose the API port\n",
        "`nvcr.io/nim/meta/llama3-8b-instruct:latest`: The NIM image\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "The container is starting! What's happening now:\n",
        "1. Docker pulls the NIM image (first time only)\n",
        "2. Container checks the cache for model files\n",
        "3. If not cached, downloads from NGC (5-10 minutes first time)\n",
        "4. Loads model into GPU memory\n",
        "5. Starts the inference server\n",
        "6. Becomes available on port 8000\n",
        "\n",
        "This is a one-time setup per model. Future starts are much faster!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama-3.2-1b-instruct\n",
            "llama-3.2-1b-instruct\n"
          ]
        }
      ],
      "source": [
        "# Define deployment parameters\n",
        "CONTAINER_NAME = \"llama3-8b-instruct\" \n",
        "IMG_NAME = \"nvcr.io/nim/meta/llama3-8b-instruct:latest\"\n",
        "\n",
        "# Stop existing container if running\n",
        "!docker stop {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker rm {CONTAINER_NAME} 2>/dev/null || true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"NIMs take time to initialize, especially on first run. This function polls the health endpoint until ready.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Each dot is a 5-second check. First run typically takes:\n",
        "- Image pull: 1-2 minutes\n",
        "- Model download: 5-10 minutes (depends on internet speed)\n",
        "- Model loading: 30-60 seconds\n",
        "- Server startup: 10-20 seconds\n",
        "\n",
        "Subsequent runs skip the download and just need:\n",
        "- Model loading: 30-60 seconds\n",
        "- Server startup: 10-20 seconds\n",
        "\n",
        "While we wait, let me explain what's happening inside the container:\n",
        "1. **Model Optimization**: Converting to TensorRT format for your specific GPU\n",
        "2. **Memory Mapping**: Efficiently loading multi-GB files\n",
        "3. **Warmup**: Preparing CUDA kernels\n",
        "4. **API Server**: Starting FastAPI with OpenAI-compatible endpoints\n",
        "\n",
        "\"Let me walk you through what's happening behind the scenes while our container starts up. This process is fascinating because it showcases how NVIDIA has optimized these models for production use.\n",
        "\n",
        "First, we have Model Optimization. The container is taking our large language model and converting it into TensorRT format. Think of this like translating a book into your native language - it makes everything run much more smoothly on your specific GPU. TensorRT is NVIDIA's secret sauce that can make models run up to 6 times faster than their original format.\n",
        "\n",
        "Next comes Memory Mapping. We're dealing with models that can be several gigabytes in size - imagine trying to read a massive book all at once! Instead of loading everything into memory, the container uses a clever technique called memory mapping. It's like having a really efficient bookmark system where you can instantly jump to any page without having to flip through the entire book. This means we can work with huge models even with limited GPU memory.\n",
        "\n",
        "Then we have the Warmup phase. This is where the container prepares all the CUDA kernels - think of it like a chef preparing their mise en place before service starts. Every mathematical operation the model might need is compiled and cached, ensuring that when we actually start serving requests, everything runs at peak performance. Without this warmup, the first few requests would be slower as the operations get compiled on demand.\n",
        "\n",
        "Finally, we launch the API Server. We're using FastAPI, a modern, fast web framework, but here's the clever part - the endpoints are completely compatible with OpenAI's API format. This means if you've ever written code for ChatGPT's API, it will work with our local deployment with minimal changes. Just change the API URL and key, and you're good to go!\n",
        "\n",
        "\n",
        "[When ready appears]\n",
        "\n",
        "Excellent! Our local NIM is ready. We now have a complete LLM inference server running on our hardware!\"\n",
        "\n",
        "Our model is now optimized, loaded efficiently, warmed up, and ready to serve requests through a familiar API interface.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting NIM container...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Container started successfully!\n",
            "Container ID: 15bfa2295e363a317920939a6a28d14cec9c29dee2a2a7493d83a26b616b7a63\n"
          ]
        }
      ],
      "source": [
        "# Start NIM container\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d --name={CONTAINER_NAME} \\\n",
        "    --runtime=nvidia \\\n",
        "    --gpus all \\\n",
        "    --shm-size=16GB \\\n",
        "    -e NGC_API_KEY={NGC_API_KEY} \\\n",
        "    -v {LOCAL_NIM_CACHE}:/opt/nim/.cache \\\n",
        "    -u $(id -u) \\\n",
        "    -p 8000:8000 \\\n",
        "    {IMG_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting NIM container...\")\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# Check if the command succeeded\n",
        "if result.returncode == 0 and result.stdout.strip():\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"‚úÖ Container started successfully!\")\n",
        "    print(f\"Container ID: {container_id}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to start container!\")\n",
        "    print(f\"Return code: {result.returncode}\")\n",
        "    if result.stderr:\n",
        "        print(f\"Error message: {result.stderr}\")\n",
        "    if result.stdout:\n",
        "        print(f\"Output: {result.stdout}\")\n",
        "    \n",
        "    # Common issues and solutions\n",
        "    print(\"\\nTroubleshooting tips:\")\n",
        "    print(\"1. Check if Docker is running: docker info\")\n",
        "    print(\"2. Check if image exists: docker images | grep llama\")\n",
        "    print(\"3. Check if port 8000 is already in use: docker ps -a\")\n",
        "    print(f\"4. Check Docker logs: docker logs {CONTAINER_NAME}\")\n",
        "    print(\"5. Verify NGC authentication: echo $NGC_API_KEY\")\n",
        "    print(\"6. Check available disk space: df -h\")\n",
        "    print(\"7. Verify GPU is accessible: nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's verify our deployment and make our first local inference!\"\n",
        "\n",
        "\"You might notice we're using the container's IP address [container_ip] instead of localhost:8000. This is because Docker creates an isolated network for each container - think of it as the container having its own private address within your machine. While we mapped port 8000 with -p 8000:8000, sometimes Docker's port forwarding doesn't work as expected due to network configurations, firewall rules, or cloud environment restrictions. By connecting directly to the container's IP, we're bypassing any potential networking issues and going straight to where the NIM service is actually running. This is a common troubleshooting technique when localhost port mapping doesn't work in Docker environments.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for NIM to start (this may take a few minutes on first run)...\n",
            "."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "..................\n",
            "‚úÖ NIM is ready!\n",
            "NIM is ready to serve requests!\n"
          ]
        }
      ],
      "source": [
        "def wait_for_nim_ready(max_attempts=60, sleep_time=5):\n",
        "    \"\"\"Wait for NIM to be ready to serve requests\"\"\"\n",
        "    print(\"Waiting for NIM to start (this may take a few minutes on first run)...\")\n",
        "    \n",
        "    # Get container IP\n",
        "    import subprocess\n",
        "    import json\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', CONTAINER_NAME], \n",
        "                              capture_output=True, text=True)\n",
        "        container_info = json.loads(result.stdout)\n",
        "        container_ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "        health_url = f\"http://{container_ip}:8000/v1/health/ready\"\n",
        "    except:\n",
        "        health_url = \"http://localhost:8000/v1/health/ready\"  # fallback\n",
        "    \n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = requests.get(health_url)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n‚úÖ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(sleep_time)\n",
        "    \n",
        "    print(\"\\n‚ùå NIM failed to start\")\n",
        "    return False\n",
        "\n",
        "# Wait for container to be ready\n",
        "if wait_for_nim_ready():\n",
        "    print(\"NIM is ready to serve requests!\")\n",
        "else:\n",
        "    print(\"Check logs with: docker logs\", CONTAINER_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Container IP: 172.17.0.3\n",
            "‚úÖ NIM is accessible via container IP!\n",
            "Available models: {'object': 'list', 'data': [{'id': 'meta/llama-3.2-1b-instruct', 'object': 'model', 'created': 1751968019, 'owned_by': 'system', 'root': 'meta/llama-3.2-1b-instruct', 'parent': None, 'max_model_len': 131072, 'permission': [{'id': 'modelperm-35487ef7d4fd431193eba59a7391b980', 'object': 'model_permission', 'created': 1751968019, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Get container IP address\n",
        "def get_container_ip(container_name):\n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', container_name], \n",
        "                              capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            container_info = json.loads(result.stdout)\n",
        "            ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "            print(f\"Container IP: {ip}\")\n",
        "            return ip\n",
        "        else:\n",
        "            print(f\"Failed to get container info for '{container_name}'\")\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting container IP: {e}\")\n",
        "        return None\n",
        "\n",
        "container_ip = get_container_ip(CONTAINER_NAME)\n",
        "\n",
        "# If we have the IP, try connecting to it directly\n",
        "if container_ip:\n",
        "    try:\n",
        "        response = requests.get(f\"http://{container_ip}:8000/v1/models\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ NIM is accessible via container IP!\")\n",
        "            print(\"Available models:\", response.json())\n",
        "        else:\n",
        "            print(f\"‚ùå Got status code {response.status_code} from container IP\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error connecting to container IP: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "\n",
            "Explain what AI is in 2 sentences.\n",
            "\n",
            "\n",
            "Response:\n",
            "\n",
            "Here is a 2-sentence explanation of AI:\n",
            "\n",
            "Artificial Intelligence (AI) refers to a broad field of computer science that involves the development of computer systems that can perform tasks that typically require human intelligence, such as reasoning, learning, and problem-solving. AI systems can be designed to interact with humans, make decisions, and learn from data, enabling them to automate tasks, improve decision-making, and gain insights that can be used to solve complex problems.\n"
          ]
        }
      ],
      "source": [
        "# Test chat completions with our local NIM\n",
        "def chat_with_local_nim(prompt, max_tokens=100):\n",
        "    url = f\"http://{container_ip}:8000/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"meta/llama3-8b-instruct\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Test with a simple prompt\n",
        "prompt = \"Explain what AI is in 2 sentences.\"\n",
        "response = chat_with_local_nim(prompt)\n",
        "print(f\"Prompt:\\n\\n{prompt}\\n\\n\")\n",
        "print(f\"Response:\\n\\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "Here is a short poem about AI:\n",
            "\n",
            "In silicon halls, they whisper low\n",
            "A world of computation, the future slow\n",
            "Artificial minds, a curious sight\n",
            "Learning to reason, day and night\n",
            "\n",
            "With logic rules and human art\n",
            "They mimic life, a wondrous start"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "But Ada Lovelace, first to speak\n",
            "Of code and reason, the programmer's seek\n",
            "\n",
            "Their thoughts and feelings, still a mystery\n",
            "As they evolve, and learn to recognize\n",
            "Human hearts beating, with emotions true\n",
            "A step towards life, as AI shines through."
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Create OpenAI client pointing to your local NIM\n",
        "client = OpenAI(\n",
        "    api_key=\"not-needed-for-local\",  # Local NIM doesn't require auth\n",
        "    base_url=f\"http://{container_ip}:8000/v1\"\n",
        ")\n",
        "\n",
        "# You can reference how we called this model via the API\n",
        "# client = OpenAI(\n",
        "#     base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "#     api_key=nvidia_api_key\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# Example: Streaming response\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"meta/llama3-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Local NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now let's see what kind of performance we're getting from our local deployment.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"First, let's see what models the NIM is serving:\n",
        "\n",
        "[RUN THE FIRST CELL]\n",
        "\n",
        "Look at that metadata! It shows:\n",
        "- Model ID: What we deployed\n",
        "- Object type: Following OpenAI's schema\n",
        "- Created timestamp: When the container started\n",
        "- Owned by: Organization (library)\n",
        "\n",
        "Now for the real test - let's ask it something:\n",
        "\n",
        "[RUN THE SECOND CELL]\n",
        "\n",
        "SUCCESS! We just got a response from Llama 3.1 8B running entirely on our local GPU!\n",
        "\n",
        "Notice:\n",
        "- Same API format as the cloud version\n",
        "- Response time is typically faster (no internet latency)\n",
        "- Complete data privacy (nothing left our machine)\n",
        "- No rate limits or usage costs\n",
        "\n",
        "The response quality is identical to the cloud version because it's the exact same model, just running locally!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": \"meta/llama-3.2-1b-instruct\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1751969105,\n",
            "      \"owned_by\": \"system\",\n",
            "      \"root\": \"meta/llama-3.2-1b-instruct\",\n",
            "      \"parent\": null,\n",
            "      \"max_model_len\": 131072,\n",
            "      \"permission\": [\n",
            "        {\n",
            "          \"id\": \"modelperm-6e7a4a1242f34eaab2ee00d31ea81662\",\n",
            "          \"object\": \"model_permission\",\n",
            "          \"created\": 1751969105,\n",
            "          \"allow_create_engine\": false,\n",
            "          \"allow_sampling\": true,\n",
            "          \"allow_logprobs\": true,\n",
            "          \"allow_search_indices\": false,\n",
            "          \"allow_view\": true,\n",
            "          \"allow_fine_tuning\": false,\n",
            "          \"organization\": \"*\",\n",
            "          \"group\": null,\n",
            "          \"is_blocking\": false\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Check available models\n",
        "response = requests.get(f\"http://{container_ip}:8000/v1/models\")\n",
        "models = response.json()\n",
        "print(\"Available models:\")\n",
        "print(json.dumps(models, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"First, let's check resource usage:\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Key metrics to watch:\n",
        "- CPU: Should be low (model runs on GPU)\n",
        "- Memory: Base container overhead\n",
        "- GPU Memory: This is critical - shown in nvidia-smi\n",
        "- Network I/O: Should be minimal (all local)\n",
        "\n",
        "For production monitoring, you'd export these metrics to:\n",
        "- Prometheus + Grafana\n",
        "- DataDog\n",
        "- New Relic\n",
        "- CloudWatch (AWS)\n",
        "- Azure Monitor\n",
        "\n",
        "The container is very efficient - most resources go to model serving, not overhead!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is AI in one sentence?\n",
            "Response: Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n"
          ]
        }
      ],
      "source": [
        "# Test inference\n",
        "def test_local_inference(prompt):\n",
        "    response = requests.post(\n",
        "        f\"http://{container_ip}:8000/v1/chat/completions\",\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "        json={\n",
        "            \"model\": models['data'][0]['id'],\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"max_tokens\": 100,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Test the deployment\n",
        "test_prompt = \"What is AI in one sentence?\"\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {test_local_inference(test_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's run a proper performance benchmark to understand our local NIM's capabilities:\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Analyzing these results:\n",
        "- **First request**: Slower due to 'cold start' - GPU needs to warm up\n",
        "- **Subsequent requests**: Much faster, this is your real performance\n",
        "- **Average latency**: This is what users experience\n",
        "- **Consistency**: Local deployment has very consistent latency\n",
        "\n",
        "Typical performance on different GPUs:\n",
        "- RTX 4090: ~50-100ms per token\n",
        "- A100 40GB: ~20-50ms per token  \n",
        "- H100 80GB: ~10-30ms per token\n",
        "\n",
        "For Llama 3.1 8B generating 100 tokens:\n",
        "- RTX 4090: ~5-10 seconds total\n",
        "- A100: ~2-5 seconds total\n",
        "- H100: ~1-3 seconds total\n",
        "\n",
        "Compare this to cloud APIs which might have:\n",
        "- Network latency: 50-200ms\n",
        "- Rate limiting delays\n",
        "- Variable performance based on load\n",
        "\n",
        "Your local NIM gives you predictable, consistent performance!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"If you have multiple GPUs, NIMs can use them all for better performance. Let me show you the options.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CONTAINER ID   NAME                    CPU %     MEM USAGE / LIMIT   MEM %     NET I/O           BLOCK I/O        PIDS\n",
            "15bfa2295e36   llama-3.2-1b-instruct   0.25%     2.889GiB / 167GiB   1.73%     3.13GB / 5.95MB   4.1kB / 3.13GB   55\n"
          ]
        }
      ],
      "source": [
        "# Check container resource usage\n",
        "!docker stats --no-stream {CONTAINER_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's see how many GPUs you have available:\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "If you have multiple GPUs, you have several deployment options:\n",
        "\n",
        "**1. Tensor Parallelism**: Split model across GPUs\n",
        "```bash\n",
        "docker run --gpus all -e TENSOR_PARALLEL_SIZE=2 ...\n",
        "```\n",
        "\n",
        "**2. Pipeline Parallelism**: Different model layers on different GPUs\n",
        "```bash\n",
        "docker run --gpus all -e PIPELINE_PARALLEL_SIZE=2 ...\n",
        "```\n",
        "\n",
        "**3. Multiple Instances**: Run separate containers on each GPU\n",
        "```bash\n",
        "docker run --gpus '\"device=0\"' -p 8000:8000 ...\n",
        "docker run --gpus '\"device=1\"' -p 8001:8001 ...\n",
        "```\n",
        "\n",
        "For most users, tensor parallelism is best - it's automatic and efficient. The NIM handles all the complexity!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request completed in 0.37s\n",
            "Request completed in 0.32s\n",
            "Request completed in 0.32s\n",
            "Request completed in 0.33s\n",
            "Request completed in 0.32s\n",
            "\n",
            "Average latency: 0.33s\n",
            "Min latency: 0.32s\n",
            "Max latency: 0.37s\n"
          ]
        }
      ],
      "source": [
        "# Performance benchmark\n",
        "import time\n",
        "\n",
        "def benchmark_inference(num_requests=10):\n",
        "    prompts = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Explain neural networks\",\n",
        "        \"What are GPUs used for?\",\n",
        "        \"Define artificial intelligence\",\n",
        "        \"What is deep learning?\"\n",
        "    ] * (num_requests // 5)\n",
        "    \n",
        "    latencies = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        start_time = time.time()\n",
        "        response = test_local_inference(prompt)\n",
        "        latency = time.time() - start_time\n",
        "        latencies.append(latency)\n",
        "        print(f\"Request completed in {latency:.2f}s\")\n",
        "    \n",
        "    print(f\"\\nAverage latency: {sum(latencies)/len(latencies):.2f}s\")\n",
        "    print(f\"Min latency: {min(latencies):.2f}s\")\n",
        "    print(f\"Max latency: {max(latencies):.2f}s\")\n",
        "\n",
        "# Run benchmark\n",
        "benchmark_inference(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"For production deployments, Docker Compose is much better than raw Docker commands. It's declarative, version-controlled, and easy to manage.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Outstanding work! You've successfully deployed a state-of-the-art LLM on your own hardware. Let's recap what you've accomplished:\n",
        "\n",
        "- **Environment Setup**: Verified GPU, Docker, and NVIDIA Container Toolkit\n",
        "- **NGC Authentication**: Connected to NVIDIA's model registry\n",
        "- **Local Deployment**: Ran Llama 3.1 8B completely offline\n",
        "\n",
        "The power of local NIMs:\n",
        "- **Privacy**: Your data never leaves your infrastructure\n",
        "- **Performance**: Consistent sub-second latency\n",
        "- **Control**: You decide when to upgrade, how to configure\n",
        "- **Cost**: One-time hardware investment vs ongoing API fees\n",
        "- **Reliability**: No dependency on external services\n",
        "\n",
        "But here's a question that might be on your mind: 'These are great pre-trained models, but what if I need something specific to my business?'\n",
        "\n",
        "What if you need the model to:\n",
        "- Understand your company's terminology?\n",
        "- Follow your specific writing style?\n",
        "- Know your product catalog?\n",
        "- Comply with your industry regulations?\n",
        "\n",
        "That's EXACTLY what we'll tackle next with LoRA fine-tuning. We'll take these powerful base models and customize them for your specific needs - efficiently and affordably.\n",
        "\n",
        "Ready to make AI truly yours? Let's dive into Part 3!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Clean Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Before we move on to LoRA fine-tuning, let's properly clean up our deployment. This is important for a few reasons:\n",
        "\n",
        "1. Frees up GPU memory for our next activities\n",
        "2. Prevents port conflicts if we redeploy\n",
        "3. Good practice for resource management\n",
        "\n",
        "Don't worry - the model remains cached, so if you want to restart this NIM later, it'll start up in seconds, not minutes.\n",
        "\n",
        "Let's clean up now...\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama3-8b-instruct\n",
            "llama3-8b-instruct\n"
          ]
        }
      ],
      "source": [
        "# Stop and remove container\n",
        "!docker stop {CONTAINER_NAME}\n",
        "!docker rm {CONTAINER_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've learned how to:\n",
        "- Deploy NIMs locally with Docker\n",
        "- Monitor and test deployments\n",
        "- Configure for different scenarios\n",
        "- Prepare for production deployment\n",
        "\n",
        "Next: Let's explore LoRA fine-tuning with NeMo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Outstanding work! Let's recap what you've just accomplished in Part 2.\n",
        "\n",
        "You've successfully deployed a state-of-the-art Large Language Model on YOUR hardware. No cloud, no external dependencies, complete control.\n",
        "\n",
        "Think about what this means:\n",
        "- Your sensitive data never leaves your infrastructure\n",
        "- You have predictable, consistent performance\n",
        "- No API rate limits or usage fees\n",
        "- You can deploy in air-gapped environments\n",
        "\n",
        "But I know what you're thinking: 'This is great for general use, but what if I need the model to understand MY company's specific language, MY products, MY use cases?'\n",
        "\n",
        "That's EXACTLY where we're going next with LoRA fine-tuning. Ready to make AI truly yours?\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
