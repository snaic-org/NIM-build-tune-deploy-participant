{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Deploying LoRA Adapters with NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to deploy your trained LoRA adapters using NVIDIA NIM. We'll cover:\n",
        "- Understanding NIM's LoRA deployment architecture\n",
        "- Using Docker volumes for reliable LoRA mounting\n",
        "- Testing your deployed LoRA adapter\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "1. Completed notebook 03 (LoRA training) - you should have a `.nemo` file\n",
        "2. Docker installed with GPU support\n",
        "3. Your NGC API key ready\n",
        "4. At least 20GB of free disk space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding NIM LoRA Deployment\n",
        "\n",
        "### How NIM Handles LoRA Adapters\n",
        "\n",
        "NVIDIA NIM supports dynamic LoRA loading through:\n",
        "- **NIM_PEFT_SOURCE**: Environment variable pointing to your LoRA directory\n",
        "- **Automatic Discovery**: NIM scans for `.nemo` files in subdirectories\n",
        "- **Hot Reloading**: With `NIM_PEFT_REFRESH_INTERVAL`, NIM checks for new adapters\n",
        "\n",
        "### Expected Directory Structure\n",
        "\n",
        "```\n",
        "NIM_PEFT_SOURCE/\n",
        "‚îú‚îÄ‚îÄ adapter1/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter1.nemo\n",
        "‚îú‚îÄ‚îÄ adapter2/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter2.nemo\n",
        "‚îî‚îÄ‚îÄ adapter3/\n",
        "    ‚îî‚îÄ‚îÄ adapter3.nemo\n",
        "```\n",
        "\n",
        "Each adapter must be in its own subdirectory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important: Cloud GPU Deployment Challenges\n",
        "\n",
        "### The Bind Mount Problem\n",
        "\n",
        "On cloud GPU instances (AWS, GCP, Azure), Docker bind mounts often fail due to:\n",
        "- Storage driver incompatibilities\n",
        "- Security policies\n",
        "- Network file systems\n",
        "\n",
        "**Symptoms:**\n",
        "- Mounted directories appear empty inside containers\n",
        "- Files exist on host but not visible in container\n",
        "- No error messages, just empty directories\n",
        "\n",
        "### The Solution: Docker Volumes\n",
        "\n",
        "Docker named volumes work reliably where bind mounts fail. We'll use this approach throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load NGC Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables from .env file\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    # If python-dotenv is not installed, try to read .env manually\n",
        "    if os.path.exists('.env'):\n",
        "        with open('.env', 'r') as f:\n",
        "            for line in f:\n",
        "                if '=' in line:\n",
        "                    key, value = line.strip().split('=', 1)\n",
        "                    os.environ[key] = value\n",
        "\n",
        "# Set up environment\n",
        "NGC_API_KEY = os.getenv('NGC_API_KEY')\n",
        "if not NGC_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  NGC_API_KEY not found in environment or .env file!\")\n",
        "    print(\"Please run the Workshop Setup notebook (00_Workshop_Setup.ipynb) first.\")\n",
        "else:\n",
        "    os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "print(f\"NGC API Key configured: {'‚úì' if NGC_API_KEY else '‚úó'}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log into NGC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{NGC_API_KEY}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "if \"Login Succeeded\" in result.stdout:\n",
        "    print(\"‚úì Successfully logged in to NGC\")\n",
        "else:\n",
        "    print(\"‚úó Login failed!\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Your LoRA Adapter\n",
        "\n",
        "First, let's check that your LoRA adapter is ready for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's make sure your LoRA adapter is ready. We're looking for the .nemo file from notebook 03."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for LoRA files\n",
        "lora_paths = [\n",
        "    \"lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\",\n",
        "    \"loras/customer_support_lora/customer_support_lora.nemo\"\n",
        "]\n",
        "\n",
        "lora_file = None\n",
        "for path in lora_paths:\n",
        "    if os.path.exists(path):\n",
        "        lora_file = path\n",
        "        print(f\"‚úì Found LoRA adapter: {path}\")\n",
        "        print(f\"  Size: {os.path.getsize(path) / 1024 / 1024:.2f} MB\")\n",
        "        break\n",
        "\n",
        "if not lora_file:\n",
        "    print(\"‚úó No LoRA adapter found!\")\n",
        "    print(\"\\nPlease ensure you've completed notebook 03 and have a .nemo file.\")\n",
        "    print(\"Expected locations:\")\n",
        "    for path in lora_paths:\n",
        "        print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell prepares the LoRA adapter for deployment by creating the required directory structure (`loras/customer_support_lora`) and copying the trained LoRA file into it, which NIM expects for loading custom adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create proper directory structure for NIM\n",
        "!mkdir -p loras/customer_support_lora\n",
        "\n",
        "# Copy LoRA file if needed\n",
        "if lora_file and not os.path.exists(\"loras/customer_support_lora/customer_support_lora.nemo\"):\n",
        "    !cp {lora_file} loras/customer_support_lora/\n",
        "    print(\"‚úì Copied LoRA adapter to deployment directory\")\n",
        "\n",
        "# Verify structure\n",
        "!echo \"LoRA deployment structure:\"\n",
        "!tree loras/ 2>/dev/null || find loras/ -type f -name \"*.nemo\" | head -10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean Up Existing Resources\n",
        "\n",
        "Before deploying, let's ensure we have a clean slate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONTAINER_NAME = \"llama3.1-lora-nim-volume\"\n",
        "VOLUME_NAME = \"nim-lora-adapters\"\n",
        "IMAGE_NAME = \"nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\"\n",
        "\n",
        "# Clean up any existing resources\n",
        "print(\"üßπ Cleaning up existing resources...\")\n",
        "!docker rm -f {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker volume rm {VOLUME_NAME} 2>/dev/null || true\n",
        "\n",
        "print(\"\\n‚úì Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Docker Volume and Copy LoRA Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell creates a Docker volume and copies the LoRA adapter file into it using two methods:\n",
        "\n",
        "1. **First attempt**: Uses a bind mount to copy files directly\n",
        "2. **Fallback method**: If that fails (common on cloud), uses `docker cp` with a temporary container\n",
        "\n",
        "The volume is needed because cloud GPU instances often have issues with direct file mounting, so Docker volumes provide a reliable way to make the LoRA file available to the NIM container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Docker volume\n",
        "print(\"üì¶ Creating Docker volume for LoRA adapters...\")\n",
        "!docker volume create {VOLUME_NAME}\n",
        "\n",
        "# Copy LoRA files to the volume using a temporary container\n",
        "print(\"\\nüìã Copying LoRA files to Docker volume...\")\n",
        "# Method 1: Try with bind mount first\n",
        "copy_result = subprocess.run(\n",
        "    f'docker run --rm -v {VOLUME_NAME}:/data -v $(pwd)/loras:/source alpine sh -c '\n",
        "    f'\"mkdir -p /data/customer_support_lora && '\n",
        "    f'cp -r /source/customer_support_lora/* /data/customer_support_lora/ 2>/dev/null || echo \\'No files to copy\\' && '\n",
        "    f'ls -la /data/customer_support_lora/\"',\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(copy_result.stdout)\n",
        "\n",
        "# Method 2: Use docker cp as fallback (more reliable on cloud)\n",
        "print(\"\\nüìã Ensuring LoRA files are in the volume (using docker cp)...\")\n",
        "!docker run -d --name temp-container -v {VOLUME_NAME}:/data alpine sleep 3600\n",
        "!docker cp loras/customer_support_lora/customer_support_lora.nemo temp-container:/data/customer_support_lora/\n",
        "!docker exec temp-container ls -la /data/customer_support_lora/\n",
        "!docker rm -f temp-container\n",
        "\n",
        "print(\"\\n‚úì LoRA files copied to volume\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output shows the two-step copying process:\n",
        "\n",
        "**First attempt failed**: \"No files to copy\" - the bind mount method didn't work (common on cloud GPUs)\n",
        "\n",
        "**Second attempt succeeded**: \n",
        "- \"Successfully copied 21MB\" - the `docker cp` method worked\n",
        "- The LoRA file (`customer_support_lora.nemo`, 21MB) is now in the Docker volume\n",
        "- Ready for NIM to use\n",
        "\n",
        "This is why the cell uses two methods - the first one often fails on cloud, but the second one (docker cp) is more reliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start NIM Container with LoRA Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell starts the NIM container with LoRA support enabled.\n",
        "\n",
        "**Key configuration:**\n",
        "- `NIM_PEFT_SOURCE=/lora-store` - tells NIM where to find LoRA adapters\n",
        "- `NIM_PEFT_REFRESH_INTERVAL=300` - checks for new LoRAs every 5 minutes\n",
        "- `-v {VOLUME_NAME}:/lora-store` - mounts the Docker volume containing your LoRA file\n",
        "\n",
        "The container runs in the background with GPU access and will take 2-3 minutes to initialize on first run as it loads the base model and discovers the LoRA adapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start NIM container with LoRA support\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d \\\\\n",
        "    --name={CONTAINER_NAME} \\\\\n",
        "    --runtime=nvidia \\\\\n",
        "    --gpus all \\\\\n",
        "    --shm-size=16GB \\\\\n",
        "    -e NGC_API_KEY={NGC_API_KEY} \\\\\n",
        "    -e NIM_PEFT_SOURCE=/lora-store \\\\\n",
        "    -e NIM_PEFT_REFRESH_INTERVAL=300 \\\\\n",
        "    -v {VOLUME_NAME}:/lora-store \\\\\n",
        "    -p 8000:8000 \\\\\n",
        "    {IMAGE_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"üöÄ Starting NIM container with LoRA support...\")\n",
        "print(f\"\\nCommand: {docker_cmd}\")\n",
        "\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"\\n‚úì Container started: {container_id[:12]}\")\n",
        "    print(\"\\n‚è≥ Container is initializing. This may take 2-3 minutes on first run...\")\n",
        "else:\n",
        "    print(\"\\n‚úó Failed to start container\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Get Container Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's find our container's IP address. On cloud instances, 'localhost' might not work, so we get the actual container IP.\n",
        "\n",
        "You'll see something like 172.17.0.3 - that's Docker's internal network.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get container IP address\n",
        "def get_container_ip():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            f\"docker inspect -f '{{{{range.NetworkSettings.Networks}}}}{{{{.IPAddress}}}}{{{{end}}}}' {CONTAINER_NAME}\",\n",
        "            shell=True, capture_output=True, text=True\n",
        "        )\n",
        "        ip = result.stdout.strip()\n",
        "        return ip if ip else \"localhost\"\n",
        "    except:\n",
        "        return \"localhost\"\n",
        "\n",
        "container_ip = get_container_ip()\n",
        "print(f\"üìç Container IP: {container_ip}\")\n",
        "base_url = f\"http://{container_ip}:8000\"\n",
        "\n",
        "# Verify LoRA files are visible inside container\n",
        "print(\"\\nüîç Verifying LoRA files in container...\")\n",
        "!docker exec {CONTAINER_NAME} ls -la /lora-store/customer_support_lora/ || echo \"Container still starting...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Wait for NIM to Initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NIM needs time to initialize:\n",
        "- Loading the base model\n",
        "- Scanning for LoRA adapters\n",
        "- Optimizing for your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for NIM to be ready\n",
        "def wait_for_nim(base_url, timeout=300):\n",
        "    print(\"‚è≥ Waiting for NIM to initialize...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            response = requests.get(f\"{base_url}/v1/health/ready\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n‚úÖ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(5)\n",
        "    \n",
        "    print(\"\\n‚úó Timeout waiting for NIM\")\n",
        "    return False\n",
        "\n",
        "if wait_for_nim(base_url):\n",
        "    # Check logs for LoRA loading\n",
        "    print(\"\\nüìã Checking LoRA synchronization logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | grep -i \"lora\\|peft\\|adapter\\|synchroniz\" | tail -20\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  NIM is taking longer than expected. Checking logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | tail -30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Available Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The moment of truth! Let's see what models are available.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "You should see TWO models:\n",
        "1. meta/llama3-8b-instruct - the base model\n",
        "2. customer_support_lora - your fine-tuned adapter\n",
        "\n",
        "If you only see the base model, give it another minute and run again. NIM might still be scanning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available models\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/v1/models\")\n",
        "    if response.status_code == 200:\n",
        "        models = response.json()\n",
        "        print(\"üìã Available models:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        model_count = len(models.get('data', []))\n",
        "        print(f\"\\nFound {model_count} model(s):\\n\")\n",
        "        \n",
        "        for model in models.get('data', []):\n",
        "            model_id = model.get('id', 'unknown')\n",
        "            print(f\"  ‚Ä¢ {model_id}\")\n",
        "            if model_id == \"meta/llama3-8b-instruct\":\n",
        "                print(\"    Type: Base model\")\n",
        "            elif \"lora\" in model_id.lower():\n",
        "                print(\"    Type: LoRA adapter\")\n",
        "                print(\"    ‚ú® Your custom model is ready!\")\n",
        "        \n",
        "        if model_count == 1:\n",
        "            print(\"\\n‚ö†Ô∏è  Only base model found. LoRA may still be loading...\")\n",
        "            print(\"Wait 30 seconds and run this cell again.\")\n",
        "        elif model_count > 1:\n",
        "            print(\"\\n‚úÖ Both base model and LoRA adapter are available!\")\n",
        "    else:\n",
        "        print(f\"Error: Status code {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to NIM: {e}\")\n",
        "    print(\"\\nMake sure the container is running and healthy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test Your LoRA Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the exciting part - let's test both models with the same query!\n",
        "\n",
        "Watch the difference:\n",
        "- Base model: Generic, helpful but not specific\n",
        "- LoRA model: Uses your training data, knows your policies\n",
        "\n",
        "This is the power of fine-tuning - domain-specific responses without training a whole new model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Test Query: My order hasn't arrived yet. Order number is 12345.\n",
            "======================================================================\n",
            "\n",
            "ü§ñ BASE MODEL RESPONSE:\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'd be happy to help you track down your order. To assist you further, I'll need to know a few more details. Here are a few questions:\n",
            "\n",
            "1. Who did you place the order with? Was it an online retailer, a physical store, or a marketplace like Amazon or eBay?\n",
            "2. When did you place the order? Was it yesterday, last week, or a few days ago?\n",
            "3. Have you checked the estimated delivery date and the tracking status on the order confirmation email or the retailer's website?\n",
            "\n",
            "Please let me know the answers to these questions, and I'll do my best to help you locate your order and find out what's going on!\n",
            "\n",
            "üéØ LORA MODEL RESPONSE:\n",
            "----------------------------------------------------------------------\n",
            "I'd be happy to help you with your order #12345. Can you please tell me a little more about the order, such as the date you placed it, the items you ordered, and the shipping method you chose? This will help me look into the status of your order right away.\n",
            "\n",
            "======================================================================\n",
            "üí° Notice the difference? The LoRA model provides more specific,\n",
            "   policy-aware responses based on your training data!\n"
          ]
        }
      ],
      "source": [
        "# Test function\n",
        "def test_model(model_name, query):\n",
        "    \"\"\"Test a model with a query\"\"\"\n",
        "    url = f\"{base_url}/v1/chat/completions\"\n",
        "    \n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": [\n",
        "            # {\n",
        "            #     \"role\": \"system\",\n",
        "            #     \"content\": \"You are a helpful customer support assistant.\"\n",
        "            # },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        else:\n",
        "            return f\"Error: {response.status_code} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Test query\n",
        "test_query = \"My order hasn't arrived yet. Order number is 12345.\"\n",
        "\n",
        "print(\"üß™ Test Query:\", test_query)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test base model\n",
        "print(\"\\nü§ñ BASE MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "base_response = test_model(\"meta/llama-3.1-8b-instruct\", test_query)\n",
        "print(base_response)\n",
        "\n",
        "# Test LoRA model\n",
        "print(\"\\nüéØ LORA MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "lora_response = test_model(\"customer_support_lora\", test_query)\n",
        "print(lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üí° Notice the difference? The LoRA model provides more specific,\")\n",
        "print(\"   policy-aware responses based on your training data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Multiple Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's test a few more scenarios to really see the difference.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Each scenario shows how the LoRA has learned your specific policies and tone. The base model is helpful but generic - the LoRA knows YOUR business!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Multiple Scenarios\n",
            "================================================================================\n",
            "\n",
            "üìå Scenario 1: How do I reset my password?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Base Model:\n",
            "Please check the help center for that type of information: https://www.meta.com/help/\n",
            "\n",
            "LoRA Model:\n",
            "I'd be happy to help you reset your password. To get started, could you please tell me a little more about your account? What is your username or email address associated with your account?\n",
            "\n",
            "üìå Scenario 2: What is your return policy?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "I don't have a physical product to return. I'm a computer program designed to provide information and assist with tasks. I don't have a return policy in the classical sense.\n",
            "\n",
            "However, if you're not sa...\n",
            "\n",
            "LoRA Model:\n",
            "I don't have a physical store or products to sell, so I don't have a return policy in the classical sense. I exist solely as a digital assistant, and our interactions are through text-based conversati...\n",
            "\n",
            "üìå Scenario 3: I received a damaged product. What should I do?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "Receiving a damaged product can be frustrating. Here's a step-by-step guide to help you handle the situation:\n",
            "\n",
            "**Immediate Action**\n",
            "\n",
            "1. **Take photos**: Document the damage by taking clear, high-quali...\n",
            "\n",
            "LoRA Model:\n",
            "Sorry to hear that you received a damaged product. Here's a step-by-step guide to help you resolve the issue:\n",
            "\n",
            "1. **Document the damage**: Take photos or videos of the damage from multiple angles. Thi...\n",
            "\n",
            "üìå Scenario 4: Do you offer international shipping?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base Model:\n",
            "I'm just a large language model, I don't have have a physical presence or a shipping capability. I exist solely as a digital entity, so I don't have products to ship. However, I can provide informatio...\n",
            "\n",
            "LoRA Model:\n",
            "I'm happy to help, but I need to clarify that I'm a large language model, I don't have a physical store or products to ship. I exist solely as a digital entity, and my purpose is to provide informatio...\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Testing complete! Your LoRA adapter is working perfectly!\n"
          ]
        }
      ],
      "source": [
        "# Test multiple scenarios\n",
        "test_scenarios = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"What is your return policy?\",\n",
        "    \"I received a damaged product. What should I do?\",\n",
        "    \"Do you offer international shipping?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Multiple Scenarios\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(test_scenarios, 1):\n",
        "    print(f\"\\nüìå Scenario {i}: {scenario}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Test both models\n",
        "    base_response = test_model(\"meta/llama-3.1-8b-instruct\", scenario)\n",
        "    lora_response = test_model(\"customer_support_lora\", scenario)\n",
        "    \n",
        "    print(\"\\nBase Model:\")\n",
        "    print(base_response[:200] + \"...\" if len(base_response) > 200 else base_response)\n",
        "    \n",
        "    print(\"\\nLoRA Model:\")\n",
        "    print(lora_response[:200] + \"...\" if len(lora_response) > 200 else lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Testing complete! Your LoRA adapter is working perfectly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Chatbot\n",
        "\n",
        "Now let's have a conversation with your LoRA model! This simple chatbot interface lets you:\n",
        "- Switch between the base model and your LoRA model to compare responses\n",
        "- Have a continuous conversation\n",
        "- See how your fine-tuned model handles customer support queries\n",
        "\n",
        "Try asking customer support questions like:\n",
        "- \"I need help with my order\"\n",
        "- \"How do I reset my password?\"\n",
        "- \"What's your return policy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_564391/3210259877.py:103: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
            "  input_box.on_submit(handle_enter)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>üí¨ Chat with your LoRA Model</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<p style='color: #666; font-size: 0.9em;'>Tip: Press Enter to send your message quickly!</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d81fdf670f504f85a943a9078d0f2821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "RadioButtons(description='Model:', options=('customer_support_lora', 'meta/llama-3.1-8b-instruct'), value='cus‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62b71530b86e49568953d2f7382f34a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Text(value='', continuous_update=False, description='You:', layout=Layout(width='80%'), placeho‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fe4d290b3e24ad0ae693819fd1e6e6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Simple Interactive Chatbot\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "import threading\n",
        "\n",
        "# Create chat interface\n",
        "chat_history = []\n",
        "output_area = widgets.Output()\n",
        "send_lock = threading.Lock()  # Prevent concurrent sends\n",
        "\n",
        "# Use Text widget for input\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your message and press Enter...',\n",
        "    description='You:',\n",
        "    layout=widgets.Layout(width='80%'),\n",
        "    continuous_update=False  # Important: only trigger on Enter/blur\n",
        ")\n",
        "\n",
        "send_button = widgets.Button(\n",
        "    description='Send',\n",
        "    button_style='primary',\n",
        "    icon='paper-plane'\n",
        ")\n",
        "\n",
        "clear_button = widgets.Button(\n",
        "    description='Clear Chat',\n",
        "    button_style='warning',\n",
        "    icon='trash'\n",
        ")\n",
        "\n",
        "# Model selector\n",
        "model_selector = widgets.RadioButtons(\n",
        "    options=['customer_support_lora', 'meta/llama-3.1-8b-instruct'],\n",
        "    value='customer_support_lora',\n",
        "    description='Model:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "def update_chat_display():\n",
        "    \"\"\"Update the chat display with current history\"\"\"\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        # Display all messages as one HTML block\n",
        "        html_content = \"\"\n",
        "        for msg in chat_history:\n",
        "            html_content += f\"<p style='margin: 5px 0;'>{msg}</p>\"\n",
        "        if html_content:\n",
        "            display(HTML(html_content))\n",
        "\n",
        "def send_message(b=None):\n",
        "    \"\"\"Send message - with lock to prevent double sends\"\"\"\n",
        "    # Use lock to prevent concurrent execution\n",
        "    if not send_lock.acquire(blocking=False):\n",
        "        return  # Already sending, ignore this call\n",
        "    \n",
        "    try:\n",
        "        user_msg = input_box.value.strip()\n",
        "        if not user_msg:\n",
        "            return\n",
        "        \n",
        "        # Store message and clear input immediately\n",
        "        msg_to_send = user_msg\n",
        "        input_box.value = ''\n",
        "        \n",
        "        # Add user message to history\n",
        "        chat_history.append(f\"<b>You:</b> {msg_to_send}\")\n",
        "        \n",
        "        # Show message and typing indicator\n",
        "        chat_history.append(\"<i>Assistant is typing...</i>\")\n",
        "        update_chat_display()\n",
        "        \n",
        "        # Get model response\n",
        "        try:\n",
        "            response = test_model(model_selector.value, msg_to_send)\n",
        "            # Replace typing indicator with actual response\n",
        "            chat_history[-1] = f\"<b>Assistant ({model_selector.value.split('/')[-1]}):</b> {response}\"\n",
        "        except Exception as e:\n",
        "            chat_history[-1] = f\"<b>Error:</b> {str(e)}\"\n",
        "        \n",
        "        # Update display with final response\n",
        "        update_chat_display()\n",
        "    finally:\n",
        "        send_lock.release()\n",
        "\n",
        "def clear_chat(b):\n",
        "    \"\"\"Clear chat history\"\"\"\n",
        "    global chat_history\n",
        "    chat_history = []\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        display(HTML(\"<p><i>Chat cleared. Ready for a new conversation!</i></p>\"))\n",
        "    input_box.value = ''\n",
        "\n",
        "def handle_enter(widget):\n",
        "    \"\"\"Handle Enter key press\"\"\"\n",
        "    send_message()\n",
        "\n",
        "# Connect event handlers\n",
        "send_button.on_click(send_message)\n",
        "clear_button.on_click(clear_chat)\n",
        "\n",
        "# Enable Enter key submission with proper handling\n",
        "input_box.on_submit(handle_enter)\n",
        "\n",
        "# Display interface\n",
        "display(HTML(\"<h3>üí¨ Chat with your LoRA Model</h3>\"))\n",
        "display(HTML(\"<p style='color: #666; font-size: 0.9em;'>Press Enter or click Send to send your message.</p>\"))\n",
        "display(model_selector)\n",
        "display(widgets.HBox([input_box, send_button, clear_button]))\n",
        "display(output_area)\n",
        "\n",
        "# Initial message\n",
        "with output_area:\n",
        "    display(HTML(\"<p><i>Ready to chat! Select a model and type your message.</i></p>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully deployed a LoRA adapter with NVIDIA NIM! Key takeaways:\n",
        "\n",
        "‚úÖ **Use Docker volumes** for reliable deployment on cloud GPUs  \n",
        "‚úÖ **Follow the directory structure** - each LoRA in its own subdirectory  \n",
        "‚úÖ **NIM handles the complexity** - automatic discovery, optimized serving  \n",
        "\n",
        "### Resources\n",
        "\n",
        "- [NVIDIA NIM Documentation](https://docs.nvidia.com/nim/)\n",
        "- [NeMo Framework](https://github.com/NVIDIA/NeMo)\n",
        "- [NGC Catalog](https://catalog.ngc.nvidia.com)\n",
        "\n",
        "Happy NIM-ing! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
